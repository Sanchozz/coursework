{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "from bo_parameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificators = [\n",
    "    (\"LGBMClassifier\", LGBMClassifier, SEARCH_SPACE_LGBM, LIGHTGBM_PARAMS),\n",
    "    (\"XGBClassifier\", XGBClassifier, SEARCH_SPACE_XGB, XGBOOST_PARAMS),\n",
    "    (\"TABNETClassifier\", TabNetClassifier, SEARCH_SPACE_TABNET, TABNET_PARAMS),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesianOptimization(clf, params, search_range, model_name):\n",
    "    def func_gb(values):\n",
    "        for i, param in enumerate(search_range):\n",
    "            params[param.name] = values[i]\n",
    "            if param.name == \"n_a\":\n",
    "                params[\"n_d\"] = values[i]\n",
    "        print('\\nTesting next set of paramaters...', params)\n",
    "\n",
    "        model = clf(**params)\n",
    "        model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], eval_metric=[\"auc\"], early_stopping_rounds=10, verbose=0)\n",
    "        neg_auc = round(-roc_auc_score(valid_y, model.predict_proba(valid_x)[:, 1]), 8)\n",
    "\n",
    "        print('AUC: ', -neg_auc, ' of boosting iteration ')\n",
    "        return neg_auc\n",
    "    \n",
    "    def func_tabnet(values):\n",
    "        for i, param in enumerate(search_range):\n",
    "            params[param.name] = values[i]\n",
    "            if param.name == \"n_a\":\n",
    "                params[\"n_d\"] = values[i]\n",
    "        print('\\nTesting next set of paramaters...', params)\n",
    "\n",
    "        model = clf(**params)\n",
    "        model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], eval_metric=[\"auc\"], max_epochs=40)\n",
    "        neg_auc = round(-roc_auc_score(valid_y, model.predict_proba(valid_x)[:, 1]), 8)\n",
    "\n",
    "        print('AUC: ', -neg_auc, ' of boosting iteration ')\n",
    "        return neg_auc\n",
    "    \n",
    "    return func_gb if model_name != \"TABNETClassifier\" else func_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"syn\" + str(i + 1) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTED: syn1\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.12084853580348885, 'max_depth': 14, 'n_estimators': 630}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.0464\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.34145281732771193, 'max_depth': 5, 'n_estimators': 434}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.0389\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18067812223820384, 'max_depth': 4, 'n_estimators': 787}\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.0408\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3551549238018522, 'max_depth': 6, 'n_estimators': 757}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.0431\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49666929835141815, 'max_depth': 6, 'n_estimators': 289}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.0458\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2311182427782875, 'max_depth': 9, 'n_estimators': 910}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.0396\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14072737652391287, 'max_depth': 8, 'n_estimators': 838}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.0542\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31246356220852756, 'max_depth': 3, 'n_estimators': 871}\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.0461\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2044953157381911, 'max_depth': 8, 'n_estimators': 141}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.0532\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2457713688588792, 'max_depth': 3, 'n_estimators': 266}\n",
      "AUC:  0.59347697  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.0301\n",
      "Function value obtained: -0.5935\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3212361085197305, 'max_depth': 10, 'n_estimators': 331}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0386\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31622124767487747, 'max_depth': 13, 'n_estimators': 656}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.0399\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.12007096260563653, 'max_depth': 6, 'n_estimators': 548}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.0479\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.22257389087410692, 'max_depth': 9, 'n_estimators': 717}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0380\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4017972298730733, 'max_depth': 11, 'n_estimators': 337}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0407\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.26074959708622025, 'max_depth': 8, 'n_estimators': 591}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0550\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.23586825850415408, 'max_depth': 13, 'n_estimators': 276}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.0381\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4889757321919298, 'max_depth': 7, 'n_estimators': 390}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.0415\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4787112301559654, 'max_depth': 12, 'n_estimators': 835}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.0450\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2832805429127948, 'max_depth': 11, 'n_estimators': 874}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.0431\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49391274137167374, 'max_depth': 10, 'n_estimators': 684}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.0407\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4475197826417706, 'max_depth': 8, 'n_estimators': 510}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.0499\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3836434887869429, 'max_depth': 12, 'n_estimators': 420}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.0392\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4898004528567461, 'max_depth': 13, 'n_estimators': 531}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0432\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31467894505827154, 'max_depth': 14, 'n_estimators': 762}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.0396\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18905940911790714, 'max_depth': 6, 'n_estimators': 535}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.0532\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.336006827862922, 'max_depth': 10, 'n_estimators': 177}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.0389\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.41036757505504684, 'max_depth': 4, 'n_estimators': 123}\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.0343\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49395672000367996, 'max_depth': 12, 'n_estimators': 534}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.0425\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4302144373025426, 'max_depth': 15, 'n_estimators': 982}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 2.6349\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2998228194339197, 'max_depth': 12, 'n_estimators': 783}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99999467  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.1983\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.34850575442384213, 'max_depth': 11, 'n_estimators': 199}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.9998364  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.1344\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5017996767697259, 'max_depth': 7, 'n_estimators': 876}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99999467  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.1377\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6394450279970071, 'max_depth': 4, 'n_estimators': 627}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99974214  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.1387\n",
      "Function value obtained: -0.9997\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6407388649081306, 'max_depth': 11, 'n_estimators': 145}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.1479\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9529419512873549, 'max_depth': 3, 'n_estimators': 256}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99991464  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.0910\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5101869944450673, 'max_depth': 3, 'n_estimators': 340}\n",
      "[01:10:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.1057\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4797853231270728, 'max_depth': 7, 'n_estimators': 391}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.1552\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3050652650602069, 'max_depth': 10, 'n_estimators': 468}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99977415  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.1500\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8678227780345608, 'max_depth': 11, 'n_estimators': 778}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99999822  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.1309\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4718224161404099, 'max_depth': 4, 'n_estimators': 108}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.1128\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4817383987106695, 'max_depth': 8, 'n_estimators': 921}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99983462  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1322\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5320563605136461, 'max_depth': 7, 'n_estimators': 366}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99998755  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.1080\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49127775700776555, 'max_depth': 10, 'n_estimators': 909}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99993954  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.1201\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7798304044196362, 'max_depth': 12, 'n_estimators': 733}\n",
      "[01:10:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99972081  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.1042\n",
      "Function value obtained: -0.9997\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7279743302983198, 'max_depth': 6, 'n_estimators': 477}\n",
      "[01:10:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99998933  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0942\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.25240476115716765, 'max_depth': 10, 'n_estimators': 401}\n",
      "[01:10:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99997155  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.2251\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.15868281021378744, 'max_depth': 14, 'n_estimators': 291}\n",
      "[01:10:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99981328  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.2753\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.45207641657894415, 'max_depth': 10, 'n_estimators': 932}\n",
      "[01:10:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99993776  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.1105\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.13037677135789952, 'max_depth': 14, 'n_estimators': 129}\n",
      "[01:10:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99968524  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.2625\n",
      "Function value obtained: -0.9997\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.24849547796029128, 'max_depth': 6, 'n_estimators': 797}\n",
      "[01:10:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99913218  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.1616\n",
      "Function value obtained: -0.9991\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.21912847251717726, 'max_depth': 15, 'n_estimators': 437}\n",
      "[01:10:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99991642  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.2467\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4947955597957918, 'max_depth': 10, 'n_estimators': 765}\n",
      "[01:10:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99999822  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.1535\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9341643297424782, 'max_depth': 10, 'n_estimators': 591}\n",
      "[01:10:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  1.0  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0899\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18177356452368387, 'max_depth': 15, 'n_estimators': 138}\n",
      "[01:10:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.999623  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.1841\n",
      "Function value obtained: -0.9996\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18523745229435157, 'max_depth': 7, 'n_estimators': 262}\n",
      "[01:10:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99979816  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.1548\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14048775656067247, 'max_depth': 12, 'n_estimators': 683}\n",
      "[01:10:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99967635  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.1246\n",
      "Function value obtained: -0.9997\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4996594587460905, 'max_depth': 5, 'n_estimators': 547}\n",
      "[01:10:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99998933  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.1151\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5904366014037258, 'max_depth': 13, 'n_estimators': 833}\n",
      "[01:10:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99999111  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.1380\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31215930542588105, 'max_depth': 3, 'n_estimators': 704}\n",
      "[01:10:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.50442355  of boosting iteration \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 1.0966\n",
      "Function value obtained: -0.5044\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8294789242800777, 'lambda_sparse': 0.024825767831183742, 'n_steps': 3, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.81801 | val_0_auc: 0.623   |  0:00:00s\n",
      "epoch 1  | loss: 0.65475 | val_0_auc: 0.75301 |  0:00:01s\n",
      "epoch 2  | loss: 0.54664 | val_0_auc: 0.87652 |  0:00:01s\n",
      "epoch 3  | loss: 0.4144  | val_0_auc: 0.93603 |  0:00:02s\n",
      "epoch 4  | loss: 0.29924 | val_0_auc: 0.96227 |  0:00:03s\n",
      "epoch 5  | loss: 0.23451 | val_0_auc: 0.9838  |  0:00:03s\n",
      "epoch 6  | loss: 0.19945 | val_0_auc: 0.99206 |  0:00:04s\n",
      "epoch 7  | loss: 0.15644 | val_0_auc: 0.99531 |  0:00:05s\n",
      "epoch 8  | loss: 0.13924 | val_0_auc: 0.99343 |  0:00:05s\n",
      "epoch 9  | loss: 0.1202  | val_0_auc: 0.99637 |  0:00:06s\n",
      "epoch 10 | loss: 0.11401 | val_0_auc: 0.99529 |  0:00:06s\n",
      "epoch 11 | loss: 0.11397 | val_0_auc: 0.99828 |  0:00:07s\n",
      "epoch 12 | loss: 0.10589 | val_0_auc: 0.99834 |  0:00:08s\n",
      "epoch 13 | loss: 0.10228 | val_0_auc: 0.99801 |  0:00:08s\n",
      "epoch 14 | loss: 0.0923  | val_0_auc: 0.99782 |  0:00:09s\n",
      "epoch 15 | loss: 0.08558 | val_0_auc: 0.99811 |  0:00:09s\n",
      "epoch 16 | loss: 0.09284 | val_0_auc: 0.99678 |  0:00:10s\n",
      "epoch 17 | loss: 0.10201 | val_0_auc: 0.99851 |  0:00:11s\n",
      "epoch 18 | loss: 0.09551 | val_0_auc: 0.99592 |  0:00:11s\n",
      "epoch 19 | loss: 0.09423 | val_0_auc: 0.99706 |  0:00:12s\n",
      "epoch 20 | loss: 0.09497 | val_0_auc: 0.99909 |  0:00:12s\n",
      "epoch 21 | loss: 0.08739 | val_0_auc: 0.99915 |  0:00:13s\n",
      "epoch 22 | loss: 0.08433 | val_0_auc: 0.99928 |  0:00:14s\n",
      "epoch 23 | loss: 0.10322 | val_0_auc: 0.99932 |  0:00:14s\n",
      "epoch 24 | loss: 0.09811 | val_0_auc: 0.9995  |  0:00:15s\n",
      "epoch 25 | loss: 0.0916  | val_0_auc: 0.99669 |  0:00:15s\n",
      "epoch 26 | loss: 0.0875  | val_0_auc: 0.99956 |  0:00:16s\n",
      "epoch 27 | loss: 0.08784 | val_0_auc: 0.9995  |  0:00:17s\n",
      "epoch 28 | loss: 0.07781 | val_0_auc: 0.99962 |  0:00:17s\n",
      "epoch 29 | loss: 0.08133 | val_0_auc: 0.99914 |  0:00:18s\n",
      "epoch 30 | loss: 0.07504 | val_0_auc: 0.99907 |  0:00:18s\n",
      "epoch 31 | loss: 0.08466 | val_0_auc: 0.9994  |  0:00:19s\n",
      "epoch 32 | loss: 0.09554 | val_0_auc: 0.99972 |  0:00:20s\n",
      "epoch 33 | loss: 0.06541 | val_0_auc: 0.99951 |  0:00:20s\n",
      "epoch 34 | loss: 0.08979 | val_0_auc: 0.99532 |  0:00:21s\n",
      "epoch 35 | loss: 0.06951 | val_0_auc: 0.9993  |  0:00:22s\n",
      "epoch 36 | loss: 0.07642 | val_0_auc: 0.99771 |  0:00:22s\n",
      "epoch 37 | loss: 0.08912 | val_0_auc: 0.99929 |  0:00:23s\n",
      "epoch 38 | loss: 0.06849 | val_0_auc: 0.99887 |  0:00:23s\n",
      "epoch 39 | loss: 0.07206 | val_0_auc: 0.99492 |  0:00:24s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 32 and best_val_0_auc = 0.99972\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99971547  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 24.7319\n",
      "Function value obtained: -0.9997\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2621898591379357, 'lambda_sparse': 0.06827404765949346, 'n_steps': 4, 'n_a': 64, 'n_d': 64, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.11818 | val_0_auc: 0.62496 |  0:00:01s\n",
      "epoch 1  | loss: 0.76422 | val_0_auc: 0.67983 |  0:00:02s\n",
      "epoch 2  | loss: 0.62112 | val_0_auc: 0.79915 |  0:00:04s\n",
      "epoch 3  | loss: 0.49563 | val_0_auc: 0.89313 |  0:00:05s\n",
      "epoch 4  | loss: 0.38838 | val_0_auc: 0.92499 |  0:00:07s\n",
      "epoch 5  | loss: 0.30752 | val_0_auc: 0.95795 |  0:00:08s\n",
      "epoch 6  | loss: 0.23386 | val_0_auc: 0.98806 |  0:00:09s\n",
      "epoch 7  | loss: 0.17485 | val_0_auc: 0.98713 |  0:00:11s\n",
      "epoch 8  | loss: 0.14845 | val_0_auc: 0.99603 |  0:00:12s\n",
      "epoch 9  | loss: 0.14468 | val_0_auc: 0.9957  |  0:00:14s\n",
      "epoch 10 | loss: 0.13548 | val_0_auc: 0.99476 |  0:00:15s\n",
      "epoch 11 | loss: 0.12542 | val_0_auc: 0.99711 |  0:00:17s\n",
      "epoch 12 | loss: 0.12047 | val_0_auc: 0.99726 |  0:00:18s\n",
      "epoch 13 | loss: 0.11948 | val_0_auc: 0.9975  |  0:00:19s\n",
      "epoch 14 | loss: 0.10747 | val_0_auc: 0.99727 |  0:00:21s\n",
      "epoch 15 | loss: 0.10441 | val_0_auc: 0.99794 |  0:00:22s\n",
      "epoch 16 | loss: 0.09944 | val_0_auc: 0.99805 |  0:00:24s\n",
      "epoch 17 | loss: 0.09562 | val_0_auc: 0.99744 |  0:00:25s\n",
      "epoch 18 | loss: 0.11618 | val_0_auc: 0.99802 |  0:00:26s\n",
      "epoch 19 | loss: 0.09842 | val_0_auc: 0.9979  |  0:00:28s\n",
      "epoch 20 | loss: 0.09098 | val_0_auc: 0.99764 |  0:00:29s\n",
      "epoch 21 | loss: 0.07874 | val_0_auc: 0.99677 |  0:00:31s\n",
      "epoch 22 | loss: 0.09532 | val_0_auc: 0.99543 |  0:00:32s\n",
      "epoch 23 | loss: 0.10258 | val_0_auc: 0.99202 |  0:00:34s\n",
      "epoch 24 | loss: 0.0905  | val_0_auc: 0.99861 |  0:00:35s\n",
      "epoch 25 | loss: 0.10774 | val_0_auc: 0.99713 |  0:00:36s\n",
      "epoch 26 | loss: 0.10152 | val_0_auc: 0.99213 |  0:00:38s\n",
      "epoch 27 | loss: 0.10065 | val_0_auc: 0.99584 |  0:00:39s\n",
      "epoch 28 | loss: 0.11384 | val_0_auc: 0.99442 |  0:00:41s\n",
      "epoch 29 | loss: 0.1043  | val_0_auc: 0.99798 |  0:00:42s\n",
      "epoch 30 | loss: 0.08559 | val_0_auc: 0.99732 |  0:00:43s\n",
      "epoch 31 | loss: 0.09966 | val_0_auc: 0.99701 |  0:00:45s\n",
      "epoch 32 | loss: 0.0915  | val_0_auc: 0.99299 |  0:00:46s\n",
      "epoch 33 | loss: 0.09137 | val_0_auc: 0.99835 |  0:00:48s\n",
      "epoch 34 | loss: 0.12532 | val_0_auc: 0.99718 |  0:00:49s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.99861\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9986058  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 50.0003\n",
      "Function value obtained: -0.9986\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1427004248602988, 'lambda_sparse': 0.023953745562627717, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.09699 | val_0_auc: 0.60363 |  0:00:01s\n",
      "epoch 1  | loss: 0.76249 | val_0_auc: 0.69569 |  0:00:02s\n",
      "epoch 2  | loss: 0.63076 | val_0_auc: 0.79579 |  0:00:04s\n",
      "epoch 3  | loss: 0.51787 | val_0_auc: 0.87679 |  0:00:05s\n",
      "epoch 4  | loss: 0.45296 | val_0_auc: 0.88575 |  0:00:06s\n",
      "epoch 5  | loss: 0.35854 | val_0_auc: 0.9536  |  0:00:08s\n",
      "epoch 6  | loss: 0.28677 | val_0_auc: 0.97779 |  0:00:09s\n",
      "epoch 7  | loss: 0.26541 | val_0_auc: 0.97505 |  0:00:11s\n",
      "epoch 8  | loss: 0.24158 | val_0_auc: 0.98489 |  0:00:12s\n",
      "epoch 9  | loss: 0.20243 | val_0_auc: 0.98207 |  0:00:13s\n",
      "epoch 10 | loss: 0.19906 | val_0_auc: 0.98632 |  0:00:15s\n",
      "epoch 11 | loss: 0.2019  | val_0_auc: 0.98206 |  0:00:16s\n",
      "epoch 12 | loss: 0.19206 | val_0_auc: 0.98231 |  0:00:18s\n",
      "epoch 13 | loss: 0.18607 | val_0_auc: 0.98912 |  0:00:19s\n",
      "epoch 14 | loss: 0.16913 | val_0_auc: 0.98859 |  0:00:20s\n",
      "epoch 15 | loss: 0.15276 | val_0_auc: 0.99219 |  0:00:22s\n",
      "epoch 16 | loss: 0.15205 | val_0_auc: 0.99706 |  0:00:23s\n",
      "epoch 17 | loss: 0.14811 | val_0_auc: 0.99632 |  0:00:25s\n",
      "epoch 18 | loss: 0.15179 | val_0_auc: 0.99451 |  0:00:26s\n",
      "epoch 19 | loss: 0.15251 | val_0_auc: 0.99484 |  0:00:27s\n",
      "epoch 20 | loss: 0.14924 | val_0_auc: 0.99181 |  0:00:29s\n",
      "epoch 21 | loss: 0.13289 | val_0_auc: 0.99261 |  0:00:30s\n",
      "epoch 22 | loss: 0.14462 | val_0_auc: 0.99336 |  0:00:31s\n",
      "epoch 23 | loss: 0.12581 | val_0_auc: 0.99678 |  0:00:33s\n",
      "epoch 24 | loss: 0.12284 | val_0_auc: 0.99612 |  0:00:34s\n",
      "epoch 25 | loss: 0.13985 | val_0_auc: 0.99417 |  0:00:36s\n",
      "epoch 26 | loss: 0.1347  | val_0_auc: 0.99663 |  0:00:37s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.99706\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99705689  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 37.9518\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3098114359993993, 'lambda_sparse': 0.024845666569320884, 'n_steps': 7, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.3812  | val_0_auc: 0.54005 |  0:00:01s\n",
      "epoch 1  | loss: 0.81553 | val_0_auc: 0.64185 |  0:00:03s\n",
      "epoch 2  | loss: 0.71071 | val_0_auc: 0.68447 |  0:00:04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.63322 | val_0_auc: 0.74555 |  0:00:06s\n",
      "epoch 4  | loss: 0.58806 | val_0_auc: 0.78697 |  0:00:07s\n",
      "epoch 5  | loss: 0.51697 | val_0_auc: 0.87022 |  0:00:09s\n",
      "epoch 6  | loss: 0.46509 | val_0_auc: 0.86876 |  0:00:11s\n",
      "epoch 7  | loss: 0.41835 | val_0_auc: 0.90618 |  0:00:12s\n",
      "epoch 8  | loss: 0.37901 | val_0_auc: 0.9444  |  0:00:14s\n",
      "epoch 9  | loss: 0.32252 | val_0_auc: 0.95653 |  0:00:15s\n",
      "epoch 10 | loss: 0.31135 | val_0_auc: 0.9418  |  0:00:17s\n",
      "epoch 11 | loss: 0.26837 | val_0_auc: 0.96663 |  0:00:18s\n",
      "epoch 12 | loss: 0.28221 | val_0_auc: 0.93919 |  0:00:20s\n",
      "epoch 13 | loss: 0.244   | val_0_auc: 0.97492 |  0:00:22s\n",
      "epoch 14 | loss: 0.24392 | val_0_auc: 0.94894 |  0:00:23s\n",
      "epoch 15 | loss: 0.22398 | val_0_auc: 0.97064 |  0:00:25s\n",
      "epoch 16 | loss: 0.21416 | val_0_auc: 0.97901 |  0:00:26s\n",
      "epoch 17 | loss: 0.19601 | val_0_auc: 0.98393 |  0:00:28s\n",
      "epoch 18 | loss: 0.22033 | val_0_auc: 0.94839 |  0:00:30s\n",
      "epoch 19 | loss: 0.23098 | val_0_auc: 0.9675  |  0:00:31s\n",
      "epoch 20 | loss: 0.19832 | val_0_auc: 0.99057 |  0:00:33s\n",
      "epoch 21 | loss: 0.18145 | val_0_auc: 0.98116 |  0:00:34s\n",
      "epoch 22 | loss: 0.17843 | val_0_auc: 0.99142 |  0:00:36s\n",
      "epoch 23 | loss: 0.18751 | val_0_auc: 0.97939 |  0:00:37s\n",
      "epoch 24 | loss: 0.17642 | val_0_auc: 0.97919 |  0:00:39s\n",
      "epoch 25 | loss: 0.17024 | val_0_auc: 0.96786 |  0:00:40s\n",
      "epoch 26 | loss: 0.18245 | val_0_auc: 0.98599 |  0:00:42s\n",
      "epoch 27 | loss: 0.16795 | val_0_auc: 0.99448 |  0:00:44s\n",
      "epoch 28 | loss: 0.17344 | val_0_auc: 0.99329 |  0:00:45s\n",
      "epoch 29 | loss: 0.1921  | val_0_auc: 0.98832 |  0:00:47s\n",
      "epoch 30 | loss: 0.16664 | val_0_auc: 0.98355 |  0:00:48s\n",
      "epoch 31 | loss: 0.13796 | val_0_auc: 0.98915 |  0:00:50s\n",
      "epoch 32 | loss: 0.14949 | val_0_auc: 0.99164 |  0:00:51s\n",
      "epoch 33 | loss: 0.14126 | val_0_auc: 0.9915  |  0:00:53s\n",
      "epoch 34 | loss: 0.14264 | val_0_auc: 0.98674 |  0:00:55s\n",
      "epoch 35 | loss: 0.13932 | val_0_auc: 0.99272 |  0:00:56s\n",
      "epoch 36 | loss: 0.1297  | val_0_auc: 0.9872  |  0:00:58s\n",
      "epoch 37 | loss: 0.13352 | val_0_auc: 0.98809 |  0:00:59s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.99448\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99447656  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 60.3638\n",
      "Function value obtained: -0.9945\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3195237672998774, 'lambda_sparse': 0.08482418591451325, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.09721 | val_0_auc: 0.56984 |  0:00:01s\n",
      "epoch 1  | loss: 0.83275 | val_0_auc: 0.62753 |  0:00:02s\n",
      "epoch 2  | loss: 0.74678 | val_0_auc: 0.69732 |  0:00:04s\n",
      "epoch 3  | loss: 0.66137 | val_0_auc: 0.7368  |  0:00:05s\n",
      "epoch 4  | loss: 0.60656 | val_0_auc: 0.76665 |  0:00:07s\n",
      "epoch 5  | loss: 0.54907 | val_0_auc: 0.78272 |  0:00:08s\n",
      "epoch 6  | loss: 0.50321 | val_0_auc: 0.87008 |  0:00:09s\n",
      "epoch 7  | loss: 0.49234 | val_0_auc: 0.87825 |  0:00:11s\n",
      "epoch 8  | loss: 0.45842 | val_0_auc: 0.91833 |  0:00:12s\n",
      "epoch 9  | loss: 0.39414 | val_0_auc: 0.93114 |  0:00:14s\n",
      "epoch 10 | loss: 0.33636 | val_0_auc: 0.96024 |  0:00:15s\n",
      "epoch 11 | loss: 0.29234 | val_0_auc: 0.97383 |  0:00:16s\n",
      "epoch 12 | loss: 0.26456 | val_0_auc: 0.97014 |  0:00:18s\n",
      "epoch 13 | loss: 0.24785 | val_0_auc: 0.98519 |  0:00:19s\n",
      "epoch 14 | loss: 0.22085 | val_0_auc: 0.98625 |  0:00:20s\n",
      "epoch 15 | loss: 0.22222 | val_0_auc: 0.99024 |  0:00:22s\n",
      "epoch 16 | loss: 0.21548 | val_0_auc: 0.98827 |  0:00:23s\n",
      "epoch 17 | loss: 0.19483 | val_0_auc: 0.98145 |  0:00:25s\n",
      "epoch 18 | loss: 0.20023 | val_0_auc: 0.97863 |  0:00:26s\n",
      "epoch 19 | loss: 0.18865 | val_0_auc: 0.98218 |  0:00:27s\n",
      "epoch 20 | loss: 0.16497 | val_0_auc: 0.97984 |  0:00:29s\n",
      "epoch 21 | loss: 0.1741  | val_0_auc: 0.99008 |  0:00:30s\n",
      "epoch 22 | loss: 0.17091 | val_0_auc: 0.99275 |  0:00:31s\n",
      "epoch 23 | loss: 0.15516 | val_0_auc: 0.97424 |  0:00:33s\n",
      "epoch 24 | loss: 0.14878 | val_0_auc: 0.99361 |  0:00:34s\n",
      "epoch 25 | loss: 0.14326 | val_0_auc: 0.99363 |  0:00:36s\n",
      "epoch 26 | loss: 0.14599 | val_0_auc: 0.98561 |  0:00:37s\n",
      "epoch 27 | loss: 0.18817 | val_0_auc: 0.98871 |  0:00:38s\n",
      "epoch 28 | loss: 0.18365 | val_0_auc: 0.99494 |  0:00:40s\n",
      "epoch 29 | loss: 0.17499 | val_0_auc: 0.98295 |  0:00:41s\n",
      "epoch 30 | loss: 0.15763 | val_0_auc: 0.9952  |  0:00:43s\n",
      "epoch 31 | loss: 0.1626  | val_0_auc: 0.9819  |  0:00:44s\n",
      "epoch 32 | loss: 0.12952 | val_0_auc: 0.99551 |  0:00:45s\n",
      "epoch 33 | loss: 0.12739 | val_0_auc: 0.99136 |  0:00:47s\n",
      "epoch 34 | loss: 0.13369 | val_0_auc: 0.99065 |  0:00:48s\n",
      "epoch 35 | loss: 0.12702 | val_0_auc: 0.99427 |  0:00:50s\n",
      "epoch 36 | loss: 0.12197 | val_0_auc: 0.99625 |  0:00:51s\n",
      "epoch 37 | loss: 0.12069 | val_0_auc: 0.99837 |  0:00:52s\n",
      "epoch 38 | loss: 0.14234 | val_0_auc: 0.99617 |  0:00:54s\n",
      "epoch 39 | loss: 0.11658 | val_0_auc: 0.99438 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99837\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99836929  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 56.0730\n",
      "Function value obtained: -0.9984\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8122848642256, 'lambda_sparse': 0.0922897625389088, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.91954 | val_0_auc: 0.54111 |  0:00:00s\n",
      "epoch 1  | loss: 0.75724 | val_0_auc: 0.60261 |  0:00:01s\n",
      "epoch 2  | loss: 0.68586 | val_0_auc: 0.67339 |  0:00:02s\n",
      "epoch 3  | loss: 0.62893 | val_0_auc: 0.72914 |  0:00:03s\n",
      "epoch 4  | loss: 0.58629 | val_0_auc: 0.74545 |  0:00:04s\n",
      "epoch 5  | loss: 0.54521 | val_0_auc: 0.79441 |  0:00:05s\n",
      "epoch 6  | loss: 0.51406 | val_0_auc: 0.83675 |  0:00:06s\n",
      "epoch 7  | loss: 0.49174 | val_0_auc: 0.86821 |  0:00:07s\n",
      "epoch 8  | loss: 0.45192 | val_0_auc: 0.88492 |  0:00:08s\n",
      "epoch 9  | loss: 0.40573 | val_0_auc: 0.90734 |  0:00:09s\n",
      "epoch 10 | loss: 0.37346 | val_0_auc: 0.94298 |  0:00:09s\n",
      "epoch 11 | loss: 0.31354 | val_0_auc: 0.96143 |  0:00:10s\n",
      "epoch 12 | loss: 0.26654 | val_0_auc: 0.97909 |  0:00:11s\n",
      "epoch 13 | loss: 0.23376 | val_0_auc: 0.98641 |  0:00:12s\n",
      "epoch 14 | loss: 0.20726 | val_0_auc: 0.97514 |  0:00:13s\n",
      "epoch 15 | loss: 0.19556 | val_0_auc: 0.99453 |  0:00:14s\n",
      "epoch 16 | loss: 0.15744 | val_0_auc: 0.97955 |  0:00:15s\n",
      "epoch 17 | loss: 0.17513 | val_0_auc: 0.99162 |  0:00:16s\n",
      "epoch 18 | loss: 0.17272 | val_0_auc: 0.9971  |  0:00:17s\n",
      "epoch 19 | loss: 0.15785 | val_0_auc: 0.99332 |  0:00:18s\n",
      "epoch 20 | loss: 0.15257 | val_0_auc: 0.95709 |  0:00:19s\n",
      "epoch 21 | loss: 0.16078 | val_0_auc: 0.98543 |  0:00:19s\n",
      "epoch 22 | loss: 0.14479 | val_0_auc: 0.99716 |  0:00:20s\n",
      "epoch 23 | loss: 0.14915 | val_0_auc: 0.99633 |  0:00:21s\n",
      "epoch 24 | loss: 0.14198 | val_0_auc: 0.99167 |  0:00:22s\n",
      "epoch 25 | loss: 0.1653  | val_0_auc: 0.99757 |  0:00:23s\n",
      "epoch 26 | loss: 0.16792 | val_0_auc: 0.98476 |  0:00:24s\n",
      "epoch 27 | loss: 0.16408 | val_0_auc: 0.9939  |  0:00:25s\n",
      "epoch 28 | loss: 0.13223 | val_0_auc: 0.98613 |  0:00:26s\n",
      "epoch 29 | loss: 0.12113 | val_0_auc: 0.99713 |  0:00:27s\n",
      "epoch 30 | loss: 0.12629 | val_0_auc: 0.99185 |  0:00:27s\n",
      "epoch 31 | loss: 0.12038 | val_0_auc: 0.99382 |  0:00:28s\n",
      "epoch 32 | loss: 0.11827 | val_0_auc: 0.99663 |  0:00:29s\n",
      "epoch 33 | loss: 0.11885 | val_0_auc: 0.99649 |  0:00:30s\n",
      "epoch 34 | loss: 0.10357 | val_0_auc: 0.99615 |  0:00:31s\n",
      "epoch 35 | loss: 0.10579 | val_0_auc: 0.99776 |  0:00:32s\n",
      "epoch 36 | loss: 0.09121 | val_0_auc: 0.99878 |  0:00:33s\n",
      "epoch 37 | loss: 0.0994  | val_0_auc: 0.99889 |  0:00:34s\n",
      "epoch 38 | loss: 0.107   | val_0_auc: 0.99744 |  0:00:35s\n",
      "epoch 39 | loss: 0.10273 | val_0_auc: 0.99808 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99889\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99889389  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 36.2029\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8451608598399882, 'lambda_sparse': 0.09014557764879258, 'n_steps': 4, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.17564 | val_0_auc: 0.61122 |  0:00:01s\n",
      "epoch 1  | loss: 0.74918 | val_0_auc: 0.74929 |  0:00:02s\n",
      "epoch 2  | loss: 0.57818 | val_0_auc: 0.83719 |  0:00:04s\n",
      "epoch 3  | loss: 0.47348 | val_0_auc: 0.86417 |  0:00:05s\n",
      "epoch 4  | loss: 0.38559 | val_0_auc: 0.94706 |  0:00:07s\n",
      "epoch 5  | loss: 0.32106 | val_0_auc: 0.95754 |  0:00:08s\n",
      "epoch 6  | loss: 0.27751 | val_0_auc: 0.97956 |  0:00:10s\n",
      "epoch 7  | loss: 0.25624 | val_0_auc: 0.96803 |  0:00:11s\n",
      "epoch 8  | loss: 0.21141 | val_0_auc: 0.99175 |  0:00:12s\n",
      "epoch 9  | loss: 0.17266 | val_0_auc: 0.98775 |  0:00:14s\n",
      "epoch 10 | loss: 0.17128 | val_0_auc: 0.98735 |  0:00:15s\n",
      "epoch 11 | loss: 0.19146 | val_0_auc: 0.98772 |  0:00:17s\n",
      "epoch 12 | loss: 0.17977 | val_0_auc: 0.97951 |  0:00:18s\n",
      "epoch 13 | loss: 0.16358 | val_0_auc: 0.98948 |  0:00:19s\n",
      "epoch 14 | loss: 0.15238 | val_0_auc: 0.97378 |  0:00:21s\n",
      "epoch 15 | loss: 0.17894 | val_0_auc: 0.97429 |  0:00:22s\n",
      "epoch 16 | loss: 0.17134 | val_0_auc: 0.98735 |  0:00:24s\n",
      "epoch 17 | loss: 0.14279 | val_0_auc: 0.98832 |  0:00:25s\n",
      "epoch 18 | loss: 0.14475 | val_0_auc: 0.99214 |  0:00:26s\n",
      "epoch 19 | loss: 0.13585 | val_0_auc: 0.97976 |  0:00:28s\n",
      "epoch 20 | loss: 0.1503  | val_0_auc: 0.9963  |  0:00:29s\n",
      "epoch 21 | loss: 0.13573 | val_0_auc: 0.99467 |  0:00:31s\n",
      "epoch 22 | loss: 0.135   | val_0_auc: 0.98875 |  0:00:32s\n",
      "epoch 23 | loss: 0.12556 | val_0_auc: 0.98647 |  0:00:34s\n",
      "epoch 24 | loss: 0.12242 | val_0_auc: 0.99328 |  0:00:37s\n",
      "epoch 25 | loss: 0.1801  | val_0_auc: 0.97911 |  0:00:38s\n",
      "epoch 26 | loss: 0.14933 | val_0_auc: 0.9956  |  0:00:39s\n",
      "epoch 27 | loss: 0.15955 | val_0_auc: 0.99433 |  0:00:41s\n",
      "epoch 28 | loss: 0.14894 | val_0_auc: 0.99596 |  0:00:42s\n",
      "epoch 29 | loss: 0.14521 | val_0_auc: 0.99661 |  0:00:44s\n",
      "epoch 30 | loss: 0.11311 | val_0_auc: 0.99527 |  0:00:45s\n",
      "epoch 31 | loss: 0.1427  | val_0_auc: 0.99231 |  0:00:47s\n",
      "epoch 32 | loss: 0.1273  | val_0_auc: 0.99247 |  0:00:48s\n",
      "epoch 33 | loss: 0.11422 | val_0_auc: 0.99813 |  0:00:49s\n",
      "epoch 34 | loss: 0.12694 | val_0_auc: 0.99795 |  0:00:51s\n",
      "epoch 35 | loss: 0.11416 | val_0_auc: 0.99662 |  0:00:52s\n",
      "epoch 36 | loss: 0.11155 | val_0_auc: 0.99781 |  0:00:54s\n",
      "epoch 37 | loss: 0.10495 | val_0_auc: 0.99548 |  0:00:55s\n",
      "epoch 38 | loss: 0.1459  | val_0_auc: 0.99482 |  0:00:57s\n",
      "epoch 39 | loss: 0.11463 | val_0_auc: 0.98646 |  0:00:58s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.99813\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99813277  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 59.1469\n",
      "Function value obtained: -0.9981\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.351675257166946, 'lambda_sparse': 0.003674469281042001, 'n_steps': 9, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.11025 | val_0_auc: 0.62659 |  0:00:02s\n",
      "epoch 1  | loss: 0.93789 | val_0_auc: 0.62461 |  0:00:04s\n",
      "epoch 2  | loss: 0.79987 | val_0_auc: 0.60596 |  0:00:06s\n",
      "epoch 3  | loss: 0.70025 | val_0_auc: 0.66877 |  0:00:08s\n",
      "epoch 4  | loss: 0.70705 | val_0_auc: 0.65959 |  0:00:10s\n",
      "epoch 5  | loss: 0.66673 | val_0_auc: 0.66144 |  0:00:12s\n",
      "epoch 6  | loss: 0.65902 | val_0_auc: 0.67576 |  0:00:14s\n",
      "epoch 7  | loss: 0.66244 | val_0_auc: 0.71306 |  0:00:16s\n",
      "epoch 8  | loss: 0.63203 | val_0_auc: 0.74022 |  0:00:19s\n",
      "epoch 9  | loss: 0.59822 | val_0_auc: 0.75726 |  0:00:21s\n",
      "epoch 10 | loss: 0.59272 | val_0_auc: 0.70865 |  0:00:23s\n",
      "epoch 11 | loss: 0.58412 | val_0_auc: 0.73598 |  0:00:25s\n",
      "epoch 12 | loss: 0.56919 | val_0_auc: 0.75215 |  0:00:27s\n",
      "epoch 13 | loss: 0.54206 | val_0_auc: 0.78885 |  0:00:29s\n",
      "epoch 14 | loss: 0.50546 | val_0_auc: 0.85597 |  0:00:31s\n",
      "epoch 15 | loss: 0.47016 | val_0_auc: 0.85719 |  0:00:33s\n",
      "epoch 16 | loss: 0.4527  | val_0_auc: 0.88821 |  0:00:35s\n",
      "epoch 17 | loss: 0.48605 | val_0_auc: 0.90385 |  0:00:38s\n",
      "epoch 18 | loss: 0.39909 | val_0_auc: 0.90622 |  0:00:40s\n",
      "epoch 19 | loss: 0.38502 | val_0_auc: 0.92529 |  0:00:42s\n",
      "epoch 20 | loss: 0.37243 | val_0_auc: 0.90853 |  0:00:44s\n",
      "epoch 21 | loss: 0.34734 | val_0_auc: 0.93265 |  0:00:46s\n",
      "epoch 22 | loss: 0.31909 | val_0_auc: 0.95035 |  0:00:48s\n",
      "epoch 23 | loss: 0.32968 | val_0_auc: 0.94193 |  0:00:50s\n",
      "epoch 24 | loss: 0.29336 | val_0_auc: 0.95437 |  0:00:53s\n",
      "epoch 25 | loss: 0.26708 | val_0_auc: 0.96728 |  0:00:55s\n",
      "epoch 26 | loss: 0.25335 | val_0_auc: 0.97731 |  0:00:57s\n",
      "epoch 27 | loss: 0.25581 | val_0_auc: 0.96818 |  0:00:59s\n",
      "epoch 28 | loss: 0.28294 | val_0_auc: 0.97791 |  0:01:01s\n",
      "epoch 29 | loss: 0.25933 | val_0_auc: 0.96403 |  0:01:03s\n",
      "epoch 30 | loss: 0.25209 | val_0_auc: 0.97639 |  0:01:05s\n",
      "epoch 31 | loss: 0.21735 | val_0_auc: 0.98826 |  0:01:07s\n",
      "epoch 32 | loss: 0.20911 | val_0_auc: 0.96036 |  0:01:09s\n",
      "epoch 33 | loss: 0.19787 | val_0_auc: 0.98204 |  0:01:12s\n",
      "epoch 34 | loss: 0.20316 | val_0_auc: 0.98434 |  0:01:14s\n",
      "epoch 35 | loss: 0.18277 | val_0_auc: 0.97832 |  0:01:16s\n",
      "epoch 36 | loss: 0.20101 | val_0_auc: 0.98827 |  0:01:18s\n",
      "epoch 37 | loss: 0.17663 | val_0_auc: 0.99038 |  0:01:20s\n",
      "epoch 38 | loss: 0.17652 | val_0_auc: 0.97633 |  0:01:22s\n",
      "epoch 39 | loss: 0.18265 | val_0_auc: 0.99151 |  0:01:24s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99151\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99150856  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 85.4774\n",
      "Function value obtained: -0.9915\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.268755411224153, 'lambda_sparse': 0.015539665905253982, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.80034 | val_0_auc: 0.64701 |  0:00:00s\n",
      "epoch 1  | loss: 0.59378 | val_0_auc: 0.85737 |  0:00:01s\n",
      "epoch 2  | loss: 0.42339 | val_0_auc: 0.93585 |  0:00:02s\n",
      "epoch 3  | loss: 0.30187 | val_0_auc: 0.97026 |  0:00:03s\n",
      "epoch 4  | loss: 0.21288 | val_0_auc: 0.98551 |  0:00:04s\n",
      "epoch 5  | loss: 0.1572  | val_0_auc: 0.9913  |  0:00:05s\n",
      "epoch 6  | loss: 0.14242 | val_0_auc: 0.99424 |  0:00:06s\n",
      "epoch 7  | loss: 0.14058 | val_0_auc: 0.99492 |  0:00:07s\n",
      "epoch 8  | loss: 0.12388 | val_0_auc: 0.99599 |  0:00:08s\n",
      "epoch 9  | loss: 0.10718 | val_0_auc: 0.99675 |  0:00:08s\n",
      "epoch 10 | loss: 0.11065 | val_0_auc: 0.99884 |  0:00:09s\n",
      "epoch 11 | loss: 0.09066 | val_0_auc: 0.99752 |  0:00:10s\n",
      "epoch 12 | loss: 0.10212 | val_0_auc: 0.99859 |  0:00:11s\n",
      "epoch 13 | loss: 0.11429 | val_0_auc: 0.99678 |  0:00:12s\n",
      "epoch 14 | loss: 0.10484 | val_0_auc: 0.99344 |  0:00:13s\n",
      "epoch 15 | loss: 0.0995  | val_0_auc: 0.99823 |  0:00:14s\n",
      "epoch 16 | loss: 0.09456 | val_0_auc: 0.99834 |  0:00:15s\n",
      "epoch 17 | loss: 0.09062 | val_0_auc: 0.99896 |  0:00:16s\n",
      "epoch 18 | loss: 0.08959 | val_0_auc: 0.99906 |  0:00:16s\n",
      "epoch 19 | loss: 0.09636 | val_0_auc: 0.9953  |  0:00:17s\n",
      "epoch 20 | loss: 0.08933 | val_0_auc: 0.99861 |  0:00:18s\n",
      "epoch 21 | loss: 0.09215 | val_0_auc: 0.99735 |  0:00:19s\n",
      "epoch 22 | loss: 0.09399 | val_0_auc: 0.99813 |  0:00:20s\n",
      "epoch 23 | loss: 0.09813 | val_0_auc: 0.9987  |  0:00:21s\n",
      "epoch 24 | loss: 0.09881 | val_0_auc: 0.99887 |  0:00:22s\n",
      "epoch 25 | loss: 0.08175 | val_0_auc: 0.99746 |  0:00:23s\n",
      "epoch 26 | loss: 0.09694 | val_0_auc: 0.99897 |  0:00:24s\n",
      "epoch 27 | loss: 0.08079 | val_0_auc: 0.99927 |  0:00:24s\n",
      "epoch 28 | loss: 0.08932 | val_0_auc: 0.9984  |  0:00:25s\n",
      "epoch 29 | loss: 0.08903 | val_0_auc: 0.99759 |  0:00:26s\n",
      "epoch 30 | loss: 0.0716  | val_0_auc: 0.99901 |  0:00:27s\n",
      "epoch 31 | loss: 0.08745 | val_0_auc: 0.99954 |  0:00:28s\n",
      "epoch 32 | loss: 0.09059 | val_0_auc: 0.99908 |  0:00:29s\n",
      "epoch 33 | loss: 0.07892 | val_0_auc: 0.9993  |  0:00:30s\n",
      "epoch 34 | loss: 0.06618 | val_0_auc: 0.99968 |  0:00:31s\n",
      "epoch 35 | loss: 0.07318 | val_0_auc: 0.99969 |  0:00:32s\n",
      "epoch 36 | loss: 0.07066 | val_0_auc: 0.99862 |  0:00:32s\n",
      "epoch 37 | loss: 0.07897 | val_0_auc: 0.99834 |  0:00:33s\n",
      "epoch 38 | loss: 0.08166 | val_0_auc: 0.9994  |  0:00:34s\n",
      "epoch 39 | loss: 0.06272 | val_0_auc: 0.99919 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.99969\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99968702  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 36.0040\n",
      "Function value obtained: -0.9997\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3560418708940707, 'lambda_sparse': 0.09364695079060684, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.28743 | val_0_auc: 0.61057 |  0:00:01s\n",
      "epoch 1  | loss: 0.77553 | val_0_auc: 0.58185 |  0:00:03s\n",
      "epoch 2  | loss: 0.71653 | val_0_auc: 0.69695 |  0:00:04s\n",
      "epoch 3  | loss: 0.64197 | val_0_auc: 0.75072 |  0:00:06s\n",
      "epoch 4  | loss: 0.56597 | val_0_auc: 0.83161 |  0:00:07s\n",
      "epoch 5  | loss: 0.49263 | val_0_auc: 0.88807 |  0:00:09s\n",
      "epoch 6  | loss: 0.44863 | val_0_auc: 0.89815 |  0:00:10s\n",
      "epoch 7  | loss: 0.39696 | val_0_auc: 0.9398  |  0:00:12s\n",
      "epoch 8  | loss: 0.37199 | val_0_auc: 0.94601 |  0:00:13s\n",
      "epoch 9  | loss: 0.32913 | val_0_auc: 0.96032 |  0:00:15s\n",
      "epoch 10 | loss: 0.3061  | val_0_auc: 0.97019 |  0:00:16s\n",
      "epoch 11 | loss: 0.25973 | val_0_auc: 0.97299 |  0:00:18s\n",
      "epoch 12 | loss: 0.22821 | val_0_auc: 0.97824 |  0:00:19s\n",
      "epoch 13 | loss: 0.22906 | val_0_auc: 0.96558 |  0:00:21s\n",
      "epoch 14 | loss: 0.24    | val_0_auc: 0.98934 |  0:00:22s\n",
      "epoch 15 | loss: 0.21433 | val_0_auc: 0.99005 |  0:00:24s\n",
      "epoch 16 | loss: 0.18811 | val_0_auc: 0.99197 |  0:00:25s\n",
      "epoch 17 | loss: 0.16621 | val_0_auc: 0.99052 |  0:00:27s\n",
      "epoch 18 | loss: 0.15811 | val_0_auc: 0.98159 |  0:00:28s\n",
      "epoch 19 | loss: 0.15843 | val_0_auc: 0.99684 |  0:00:30s\n",
      "epoch 20 | loss: 0.13791 | val_0_auc: 0.99573 |  0:00:31s\n",
      "epoch 21 | loss: 0.13263 | val_0_auc: 0.99176 |  0:00:33s\n",
      "epoch 22 | loss: 0.12394 | val_0_auc: 0.99243 |  0:00:34s\n",
      "epoch 23 | loss: 0.14052 | val_0_auc: 0.99467 |  0:00:36s\n",
      "epoch 24 | loss: 0.13204 | val_0_auc: 0.9892  |  0:00:37s\n",
      "epoch 25 | loss: 0.14858 | val_0_auc: 0.99211 |  0:00:39s\n",
      "epoch 26 | loss: 0.14322 | val_0_auc: 0.9846  |  0:00:40s\n",
      "epoch 27 | loss: 0.14716 | val_0_auc: 0.99616 |  0:00:42s\n",
      "epoch 28 | loss: 0.13614 | val_0_auc: 0.99655 |  0:00:43s\n",
      "epoch 29 | loss: 0.14238 | val_0_auc: 0.9955  |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.99684\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9968435  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 45.7728\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2055695280206318, 'lambda_sparse': 0.09991207918958007, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.78179 | val_0_auc: 0.64732 |  0:00:01s\n",
      "epoch 1  | loss: 0.82703 | val_0_auc: 0.76003 |  0:00:03s\n",
      "epoch 2  | loss: 0.65196 | val_0_auc: 0.8135  |  0:00:05s\n",
      "epoch 3  | loss: 0.4967  | val_0_auc: 0.90723 |  0:00:06s\n",
      "epoch 4  | loss: 0.3844  | val_0_auc: 0.94928 |  0:00:08s\n",
      "epoch 5  | loss: 0.32923 | val_0_auc: 0.96091 |  0:00:10s\n",
      "epoch 6  | loss: 0.30119 | val_0_auc: 0.97912 |  0:00:11s\n",
      "epoch 7  | loss: 0.27818 | val_0_auc: 0.97217 |  0:00:13s\n",
      "epoch 8  | loss: 0.25475 | val_0_auc: 0.98578 |  0:00:15s\n",
      "epoch 9  | loss: 0.2129  | val_0_auc: 0.98618 |  0:00:17s\n",
      "epoch 10 | loss: 0.20841 | val_0_auc: 0.98614 |  0:00:18s\n",
      "epoch 11 | loss: 0.20284 | val_0_auc: 0.99341 |  0:00:20s\n",
      "epoch 12 | loss: 0.17645 | val_0_auc: 0.99352 |  0:00:22s\n",
      "epoch 13 | loss: 0.16893 | val_0_auc: 0.99575 |  0:00:23s\n",
      "epoch 14 | loss: 0.15757 | val_0_auc: 0.99542 |  0:00:25s\n",
      "epoch 15 | loss: 0.16077 | val_0_auc: 0.99455 |  0:00:27s\n",
      "epoch 16 | loss: 0.15128 | val_0_auc: 0.9958  |  0:00:29s\n",
      "epoch 17 | loss: 0.1765  | val_0_auc: 0.99253 |  0:00:30s\n",
      "epoch 18 | loss: 0.14095 | val_0_auc: 0.99641 |  0:00:32s\n",
      "epoch 19 | loss: 0.14873 | val_0_auc: 0.99678 |  0:00:34s\n",
      "epoch 20 | loss: 0.14157 | val_0_auc: 0.99612 |  0:00:35s\n",
      "epoch 21 | loss: 0.12893 | val_0_auc: 0.99434 |  0:00:37s\n",
      "epoch 22 | loss: 0.14069 | val_0_auc: 0.99758 |  0:00:39s\n",
      "epoch 23 | loss: 0.11582 | val_0_auc: 0.99746 |  0:00:40s\n",
      "epoch 24 | loss: 0.11666 | val_0_auc: 0.99716 |  0:00:42s\n",
      "epoch 25 | loss: 0.11632 | val_0_auc: 0.99539 |  0:00:44s\n",
      "epoch 26 | loss: 0.12544 | val_0_auc: 0.99761 |  0:00:46s\n",
      "epoch 27 | loss: 0.10974 | val_0_auc: 0.99714 |  0:00:47s\n",
      "epoch 28 | loss: 0.13588 | val_0_auc: 0.99707 |  0:00:49s\n",
      "epoch 29 | loss: 0.12444 | val_0_auc: 0.99834 |  0:00:51s\n",
      "epoch 30 | loss: 0.11905 | val_0_auc: 0.99749 |  0:00:52s\n",
      "epoch 31 | loss: 0.10561 | val_0_auc: 0.99734 |  0:00:54s\n",
      "epoch 32 | loss: 0.10572 | val_0_auc: 0.9982  |  0:00:56s\n",
      "epoch 33 | loss: 0.11355 | val_0_auc: 0.99649 |  0:00:57s\n",
      "epoch 34 | loss: 0.0977  | val_0_auc: 0.99685 |  0:00:59s\n",
      "epoch 35 | loss: 0.10795 | val_0_auc: 0.99665 |  0:01:01s\n",
      "epoch 36 | loss: 0.10154 | val_0_auc: 0.99875 |  0:01:03s\n",
      "epoch 37 | loss: 0.1122  | val_0_auc: 0.99816 |  0:01:04s\n",
      "epoch 38 | loss: 0.10098 | val_0_auc: 0.99881 |  0:01:06s\n",
      "epoch 39 | loss: 0.09535 | val_0_auc: 0.9989  |  0:01:08s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.9989\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99889745  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 68.7438\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6812223520507792, 'lambda_sparse': 0.07286728114737082, 'n_steps': 10, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.61455 | val_0_auc: 0.51185 |  0:00:02s\n",
      "epoch 1  | loss: 1.06743 | val_0_auc: 0.52898 |  0:00:04s\n",
      "epoch 2  | loss: 1.07345 | val_0_auc: 0.51494 |  0:00:06s\n",
      "epoch 3  | loss: 1.04519 | val_0_auc: 0.53162 |  0:00:08s\n",
      "epoch 4  | loss: 1.38215 | val_0_auc: 0.58625 |  0:00:10s\n",
      "epoch 5  | loss: 2.01112 | val_0_auc: 0.55361 |  0:00:12s\n",
      "epoch 6  | loss: 1.89336 | val_0_auc: 0.54229 |  0:00:14s\n",
      "epoch 7  | loss: 1.14681 | val_0_auc: 0.5349  |  0:00:17s\n",
      "epoch 8  | loss: 1.12492 | val_0_auc: 0.57302 |  0:00:19s\n",
      "epoch 9  | loss: 0.94503 | val_0_auc: 0.57308 |  0:00:21s\n",
      "epoch 10 | loss: 0.77131 | val_0_auc: 0.52357 |  0:00:23s\n",
      "epoch 11 | loss: 0.81907 | val_0_auc: 0.53643 |  0:00:25s\n",
      "epoch 12 | loss: 0.82145 | val_0_auc: 0.52341 |  0:00:27s\n",
      "epoch 13 | loss: 0.73689 | val_0_auc: 0.56679 |  0:00:29s\n",
      "epoch 14 | loss: 0.72602 | val_0_auc: 0.56838 |  0:00:31s\n",
      "\n",
      "Early stopping occurred at epoch 14 with best_epoch = 4 and best_val_0_auc = 0.58625\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.58624369  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 32.6929\n",
      "Function value obtained: -0.5862\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.993065884776898, 'lambda_sparse': 0.056153548379941576, 'n_steps': 3, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.78776 | val_0_auc: 0.66811 |  0:00:00s\n",
      "epoch 1  | loss: 0.64748 | val_0_auc: 0.73705 |  0:00:01s\n",
      "epoch 2  | loss: 0.56853 | val_0_auc: 0.83099 |  0:00:02s\n",
      "epoch 3  | loss: 0.5082  | val_0_auc: 0.894   |  0:00:03s\n",
      "epoch 4  | loss: 0.43269 | val_0_auc: 0.90566 |  0:00:04s\n",
      "epoch 5  | loss: 0.34973 | val_0_auc: 0.94473 |  0:00:04s\n",
      "epoch 6  | loss: 0.27284 | val_0_auc: 0.96745 |  0:00:05s\n",
      "epoch 7  | loss: 0.23803 | val_0_auc: 0.96934 |  0:00:06s\n",
      "epoch 8  | loss: 0.19348 | val_0_auc: 0.97877 |  0:00:07s\n",
      "epoch 9  | loss: 0.16745 | val_0_auc: 0.99151 |  0:00:08s\n",
      "epoch 10 | loss: 0.14147 | val_0_auc: 0.98912 |  0:00:09s\n",
      "epoch 11 | loss: 0.14082 | val_0_auc: 0.9929  |  0:00:09s\n",
      "epoch 12 | loss: 0.14447 | val_0_auc: 0.98989 |  0:00:10s\n",
      "epoch 13 | loss: 0.13518 | val_0_auc: 0.99393 |  0:00:11s\n",
      "epoch 14 | loss: 0.13651 | val_0_auc: 0.99626 |  0:00:12s\n",
      "epoch 15 | loss: 0.11997 | val_0_auc: 0.99392 |  0:00:13s\n",
      "epoch 16 | loss: 0.11439 | val_0_auc: 0.99557 |  0:00:14s\n",
      "epoch 17 | loss: 0.11354 | val_0_auc: 0.99794 |  0:00:14s\n",
      "epoch 18 | loss: 0.10614 | val_0_auc: 0.99844 |  0:00:15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.11325 | val_0_auc: 0.99707 |  0:00:16s\n",
      "epoch 20 | loss: 0.11244 | val_0_auc: 0.99756 |  0:00:17s\n",
      "epoch 21 | loss: 0.14119 | val_0_auc: 0.99258 |  0:00:18s\n",
      "epoch 22 | loss: 0.11025 | val_0_auc: 0.99471 |  0:00:18s\n",
      "epoch 23 | loss: 0.10713 | val_0_auc: 0.99397 |  0:00:19s\n",
      "epoch 24 | loss: 0.09672 | val_0_auc: 0.99891 |  0:00:20s\n",
      "epoch 25 | loss: 0.10645 | val_0_auc: 0.99879 |  0:00:21s\n",
      "epoch 26 | loss: 0.10988 | val_0_auc: 0.99628 |  0:00:22s\n",
      "epoch 27 | loss: 0.10997 | val_0_auc: 0.99434 |  0:00:23s\n",
      "epoch 28 | loss: 0.11816 | val_0_auc: 0.99391 |  0:00:23s\n",
      "epoch 29 | loss: 0.10198 | val_0_auc: 0.98921 |  0:00:24s\n",
      "epoch 30 | loss: 0.10417 | val_0_auc: 0.99764 |  0:00:25s\n",
      "epoch 31 | loss: 0.09797 | val_0_auc: 0.99079 |  0:00:26s\n",
      "epoch 32 | loss: 0.11698 | val_0_auc: 0.99392 |  0:00:27s\n",
      "epoch 33 | loss: 0.10514 | val_0_auc: 0.9957  |  0:00:27s\n",
      "epoch 34 | loss: 0.08511 | val_0_auc: 0.99784 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.99891\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99890634  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 29.1672\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8624324128957421, 'lambda_sparse': 0.021874449133212244, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.17664 | val_0_auc: 0.60993 |  0:00:01s\n",
      "epoch 1  | loss: 0.69995 | val_0_auc: 0.6966  |  0:00:03s\n",
      "epoch 2  | loss: 0.63104 | val_0_auc: 0.71234 |  0:00:04s\n",
      "epoch 3  | loss: 0.55906 | val_0_auc: 0.78748 |  0:00:06s\n",
      "epoch 4  | loss: 0.51136 | val_0_auc: 0.87791 |  0:00:07s\n",
      "epoch 5  | loss: 0.44728 | val_0_auc: 0.89095 |  0:00:09s\n",
      "epoch 6  | loss: 0.43516 | val_0_auc: 0.91462 |  0:00:10s\n",
      "epoch 7  | loss: 0.44709 | val_0_auc: 0.90622 |  0:00:12s\n",
      "epoch 8  | loss: 0.49286 | val_0_auc: 0.89263 |  0:00:13s\n",
      "epoch 9  | loss: 0.42973 | val_0_auc: 0.91556 |  0:00:15s\n",
      "epoch 10 | loss: 0.35158 | val_0_auc: 0.93286 |  0:00:16s\n",
      "epoch 11 | loss: 0.32015 | val_0_auc: 0.95447 |  0:00:18s\n",
      "epoch 12 | loss: 0.27258 | val_0_auc: 0.9512  |  0:00:19s\n",
      "epoch 13 | loss: 0.28066 | val_0_auc: 0.95714 |  0:00:21s\n",
      "epoch 14 | loss: 0.24448 | val_0_auc: 0.97437 |  0:00:22s\n",
      "epoch 15 | loss: 0.24656 | val_0_auc: 0.97658 |  0:00:24s\n",
      "epoch 16 | loss: 0.23168 | val_0_auc: 0.98061 |  0:00:25s\n",
      "epoch 17 | loss: 0.21849 | val_0_auc: 0.98674 |  0:00:27s\n",
      "epoch 18 | loss: 0.21373 | val_0_auc: 0.97226 |  0:00:28s\n",
      "epoch 19 | loss: 0.20728 | val_0_auc: 0.98258 |  0:00:30s\n",
      "epoch 20 | loss: 0.2315  | val_0_auc: 0.9764  |  0:00:31s\n",
      "epoch 21 | loss: 0.19951 | val_0_auc: 0.97883 |  0:00:33s\n",
      "epoch 22 | loss: 0.18956 | val_0_auc: 0.98873 |  0:00:34s\n",
      "epoch 23 | loss: 0.19391 | val_0_auc: 0.98885 |  0:00:36s\n",
      "epoch 24 | loss: 0.17028 | val_0_auc: 0.98744 |  0:00:37s\n",
      "epoch 25 | loss: 0.17764 | val_0_auc: 0.98785 |  0:00:39s\n",
      "epoch 26 | loss: 0.16837 | val_0_auc: 0.98705 |  0:00:40s\n",
      "epoch 27 | loss: 0.15634 | val_0_auc: 0.98968 |  0:00:42s\n",
      "epoch 28 | loss: 0.1375  | val_0_auc: 0.98778 |  0:00:43s\n",
      "epoch 29 | loss: 0.18586 | val_0_auc: 0.97207 |  0:00:45s\n",
      "epoch 30 | loss: 0.1643  | val_0_auc: 0.99051 |  0:00:46s\n",
      "epoch 31 | loss: 0.15891 | val_0_auc: 0.99033 |  0:00:48s\n",
      "epoch 32 | loss: 0.17112 | val_0_auc: 0.99273 |  0:00:49s\n",
      "epoch 33 | loss: 0.17071 | val_0_auc: 0.98463 |  0:00:51s\n",
      "epoch 34 | loss: 0.18508 | val_0_auc: 0.98364 |  0:00:52s\n",
      "epoch 35 | loss: 0.16395 | val_0_auc: 0.99323 |  0:00:54s\n",
      "epoch 36 | loss: 0.14343 | val_0_auc: 0.98622 |  0:00:55s\n",
      "epoch 37 | loss: 0.13432 | val_0_auc: 0.99049 |  0:00:57s\n",
      "epoch 38 | loss: 0.13654 | val_0_auc: 0.99401 |  0:00:58s\n",
      "epoch 39 | loss: 0.12555 | val_0_auc: 0.99174 |  0:01:00s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99401\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9940142  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 60.8850\n",
      "Function value obtained: -0.9940\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9762620478986261, 'lambda_sparse': 0.03226166812541134, 'n_steps': 4, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.88432 | val_0_auc: 0.60579 |  0:00:01s\n",
      "epoch 1  | loss: 0.69838 | val_0_auc: 0.64136 |  0:00:02s\n",
      "epoch 2  | loss: 0.64196 | val_0_auc: 0.72209 |  0:00:03s\n",
      "epoch 3  | loss: 0.57095 | val_0_auc: 0.82386 |  0:00:04s\n",
      "epoch 4  | loss: 0.47332 | val_0_auc: 0.9045  |  0:00:05s\n",
      "epoch 5  | loss: 0.36778 | val_0_auc: 0.94316 |  0:00:06s\n",
      "epoch 6  | loss: 0.27579 | val_0_auc: 0.97557 |  0:00:07s\n",
      "epoch 7  | loss: 0.22702 | val_0_auc: 0.98737 |  0:00:08s\n",
      "epoch 8  | loss: 0.21185 | val_0_auc: 0.98238 |  0:00:09s\n",
      "epoch 9  | loss: 0.21074 | val_0_auc: 0.98371 |  0:00:10s\n",
      "epoch 10 | loss: 0.20874 | val_0_auc: 0.98374 |  0:00:12s\n",
      "epoch 11 | loss: 0.22003 | val_0_auc: 0.98506 |  0:00:13s\n",
      "epoch 12 | loss: 0.218   | val_0_auc: 0.97587 |  0:00:14s\n",
      "epoch 13 | loss: 0.24563 | val_0_auc: 0.97268 |  0:00:15s\n",
      "epoch 14 | loss: 0.237   | val_0_auc: 0.97824 |  0:00:16s\n",
      "epoch 15 | loss: 0.20349 | val_0_auc: 0.98559 |  0:00:17s\n",
      "epoch 16 | loss: 0.19013 | val_0_auc: 0.98771 |  0:00:18s\n",
      "epoch 17 | loss: 0.17194 | val_0_auc: 0.98982 |  0:00:19s\n",
      "epoch 18 | loss: 0.14925 | val_0_auc: 0.99123 |  0:00:20s\n",
      "epoch 19 | loss: 0.15017 | val_0_auc: 0.99341 |  0:00:21s\n",
      "epoch 20 | loss: 0.12384 | val_0_auc: 0.99422 |  0:00:23s\n",
      "epoch 21 | loss: 0.12396 | val_0_auc: 0.99556 |  0:00:24s\n",
      "epoch 22 | loss: 0.1218  | val_0_auc: 0.99408 |  0:00:25s\n",
      "epoch 23 | loss: 0.12044 | val_0_auc: 0.99643 |  0:00:26s\n",
      "epoch 24 | loss: 0.10427 | val_0_auc: 0.9974  |  0:00:27s\n",
      "epoch 25 | loss: 0.10473 | val_0_auc: 0.99832 |  0:00:28s\n",
      "epoch 26 | loss: 0.10712 | val_0_auc: 0.99877 |  0:00:29s\n",
      "epoch 27 | loss: 0.09214 | val_0_auc: 0.99894 |  0:00:30s\n",
      "epoch 28 | loss: 0.09322 | val_0_auc: 0.99845 |  0:00:31s\n",
      "epoch 29 | loss: 0.10592 | val_0_auc: 0.99829 |  0:00:32s\n",
      "epoch 30 | loss: 0.09036 | val_0_auc: 0.99817 |  0:00:34s\n",
      "epoch 31 | loss: 0.09973 | val_0_auc: 0.99892 |  0:00:35s\n",
      "epoch 32 | loss: 0.10213 | val_0_auc: 0.99893 |  0:00:36s\n",
      "epoch 33 | loss: 0.08396 | val_0_auc: 0.99737 |  0:00:37s\n",
      "epoch 34 | loss: 0.11539 | val_0_auc: 0.99757 |  0:00:38s\n",
      "epoch 35 | loss: 0.08678 | val_0_auc: 0.99749 |  0:00:39s\n",
      "epoch 36 | loss: 0.10161 | val_0_auc: 0.99621 |  0:00:40s\n",
      "epoch 37 | loss: 0.09485 | val_0_auc: 0.99826 |  0:00:41s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.99894\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99894013  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 42.0848\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4159457985524422, 'lambda_sparse': 0.08429139152191804, 'n_steps': 4, 'n_a': 64, 'n_d': 64, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.2035  | val_0_auc: 0.54259 |  0:00:01s\n",
      "epoch 1  | loss: 0.78572 | val_0_auc: 0.78167 |  0:00:02s\n",
      "epoch 2  | loss: 0.58076 | val_0_auc: 0.88892 |  0:00:04s\n",
      "epoch 3  | loss: 0.40956 | val_0_auc: 0.95395 |  0:00:05s\n",
      "epoch 4  | loss: 0.29199 | val_0_auc: 0.96054 |  0:00:07s\n",
      "epoch 5  | loss: 0.22564 | val_0_auc: 0.99045 |  0:00:08s\n",
      "epoch 6  | loss: 0.18374 | val_0_auc: 0.99545 |  0:00:10s\n",
      "epoch 7  | loss: 0.15795 | val_0_auc: 0.99172 |  0:00:11s\n",
      "epoch 8  | loss: 0.13826 | val_0_auc: 0.9942  |  0:00:12s\n",
      "epoch 9  | loss: 0.12858 | val_0_auc: 0.99424 |  0:00:14s\n",
      "epoch 10 | loss: 0.17556 | val_0_auc: 0.9919  |  0:00:15s\n",
      "epoch 11 | loss: 0.15328 | val_0_auc: 0.99676 |  0:00:17s\n",
      "epoch 12 | loss: 0.12781 | val_0_auc: 0.99784 |  0:00:18s\n",
      "epoch 13 | loss: 0.12006 | val_0_auc: 0.99714 |  0:00:20s\n",
      "epoch 14 | loss: 0.10594 | val_0_auc: 0.9985  |  0:00:21s\n",
      "epoch 15 | loss: 0.107   | val_0_auc: 0.99563 |  0:00:22s\n",
      "epoch 16 | loss: 0.10519 | val_0_auc: 0.99893 |  0:00:24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17 | loss: 0.09701 | val_0_auc: 0.99705 |  0:00:25s\n",
      "epoch 18 | loss: 0.10987 | val_0_auc: 0.99634 |  0:00:27s\n",
      "epoch 19 | loss: 0.09554 | val_0_auc: 0.99607 |  0:00:28s\n",
      "epoch 20 | loss: 0.10804 | val_0_auc: 0.98994 |  0:00:30s\n",
      "epoch 21 | loss: 0.1109  | val_0_auc: 0.98811 |  0:00:31s\n",
      "epoch 22 | loss: 0.11768 | val_0_auc: 0.99713 |  0:00:32s\n",
      "epoch 23 | loss: 0.10746 | val_0_auc: 0.99581 |  0:00:34s\n",
      "epoch 24 | loss: 0.08808 | val_0_auc: 0.99855 |  0:00:35s\n",
      "epoch 25 | loss: 0.11511 | val_0_auc: 0.99839 |  0:00:37s\n",
      "epoch 26 | loss: 0.09612 | val_0_auc: 0.99754 |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.99893\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99893301  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 39.1892\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4208568560660013, 'lambda_sparse': 0.026623877816198877, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.00267 | val_0_auc: 0.52897 |  0:00:01s\n",
      "epoch 1  | loss: 0.79061 | val_0_auc: 0.67278 |  0:00:02s\n",
      "epoch 2  | loss: 0.63961 | val_0_auc: 0.75495 |  0:00:03s\n",
      "epoch 3  | loss: 0.53547 | val_0_auc: 0.8759  |  0:00:04s\n",
      "epoch 4  | loss: 0.42019 | val_0_auc: 0.92028 |  0:00:05s\n",
      "epoch 5  | loss: 0.35144 | val_0_auc: 0.90126 |  0:00:07s\n",
      "epoch 6  | loss: 0.29113 | val_0_auc: 0.95327 |  0:00:08s\n",
      "epoch 7  | loss: 0.26154 | val_0_auc: 0.98262 |  0:00:09s\n",
      "epoch 8  | loss: 0.23011 | val_0_auc: 0.9751  |  0:00:10s\n",
      "epoch 9  | loss: 0.21086 | val_0_auc: 0.96788 |  0:00:11s\n",
      "epoch 10 | loss: 0.21788 | val_0_auc: 0.96443 |  0:00:13s\n",
      "epoch 11 | loss: 0.20663 | val_0_auc: 0.98053 |  0:00:14s\n",
      "epoch 12 | loss: 0.20092 | val_0_auc: 0.98113 |  0:00:15s\n",
      "epoch 13 | loss: 0.18901 | val_0_auc: 0.98641 |  0:00:16s\n",
      "epoch 14 | loss: 0.19476 | val_0_auc: 0.98452 |  0:00:17s\n",
      "epoch 15 | loss: 0.18228 | val_0_auc: 0.97639 |  0:00:19s\n",
      "epoch 16 | loss: 0.17147 | val_0_auc: 0.98327 |  0:00:20s\n",
      "epoch 17 | loss: 0.14778 | val_0_auc: 0.9923  |  0:00:21s\n",
      "epoch 18 | loss: 0.16042 | val_0_auc: 0.99655 |  0:00:22s\n",
      "epoch 19 | loss: 0.14898 | val_0_auc: 0.99205 |  0:00:23s\n",
      "epoch 20 | loss: 0.13432 | val_0_auc: 0.99342 |  0:00:25s\n",
      "epoch 21 | loss: 0.14017 | val_0_auc: 0.99632 |  0:00:26s\n",
      "epoch 22 | loss: 0.15111 | val_0_auc: 0.99014 |  0:00:27s\n",
      "epoch 23 | loss: 0.16579 | val_0_auc: 0.9908  |  0:00:28s\n",
      "epoch 24 | loss: 0.17171 | val_0_auc: 0.9927  |  0:00:29s\n",
      "epoch 25 | loss: 0.15151 | val_0_auc: 0.99405 |  0:00:30s\n",
      "epoch 26 | loss: 0.13622 | val_0_auc: 0.9747  |  0:00:32s\n",
      "epoch 27 | loss: 0.14358 | val_0_auc: 0.99701 |  0:00:33s\n",
      "epoch 28 | loss: 0.14668 | val_0_auc: 0.9931  |  0:00:34s\n",
      "epoch 29 | loss: 0.14282 | val_0_auc: 0.99654 |  0:00:35s\n",
      "epoch 30 | loss: 0.14076 | val_0_auc: 0.99209 |  0:00:36s\n",
      "epoch 31 | loss: 0.13798 | val_0_auc: 0.99137 |  0:00:37s\n",
      "epoch 32 | loss: 0.1405  | val_0_auc: 0.99009 |  0:00:39s\n",
      "epoch 33 | loss: 0.13009 | val_0_auc: 0.99122 |  0:00:40s\n",
      "epoch 34 | loss: 0.11509 | val_0_auc: 0.99813 |  0:00:41s\n",
      "epoch 35 | loss: 0.12163 | val_0_auc: 0.99561 |  0:00:42s\n",
      "epoch 36 | loss: 0.12016 | val_0_auc: 0.98334 |  0:00:43s\n",
      "epoch 37 | loss: 0.10618 | val_0_auc: 0.99672 |  0:00:44s\n",
      "epoch 38 | loss: 0.11466 | val_0_auc: 0.99775 |  0:00:46s\n",
      "epoch 39 | loss: 0.11174 | val_0_auc: 0.99852 |  0:00:47s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99852\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.998524  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 47.7926\n",
      "Function value obtained: -0.9985\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0073242912120528, 'lambda_sparse': 0.08914246582349591, 'n_steps': 7, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.46565 | val_0_auc: 0.54978 |  0:00:01s\n",
      "epoch 1  | loss: 0.89675 | val_0_auc: 0.67036 |  0:00:03s\n",
      "epoch 2  | loss: 0.71257 | val_0_auc: 0.81535 |  0:00:04s\n",
      "epoch 3  | loss: 0.55698 | val_0_auc: 0.88589 |  0:00:06s\n",
      "epoch 4  | loss: 0.42927 | val_0_auc: 0.94118 |  0:00:07s\n",
      "epoch 5  | loss: 0.33613 | val_0_auc: 0.96732 |  0:00:09s\n",
      "epoch 6  | loss: 0.27147 | val_0_auc: 0.98026 |  0:00:11s\n",
      "epoch 7  | loss: 0.22528 | val_0_auc: 0.98996 |  0:00:12s\n",
      "epoch 8  | loss: 0.22151 | val_0_auc: 0.98902 |  0:00:14s\n",
      "epoch 9  | loss: 0.2045  | val_0_auc: 0.9914  |  0:00:15s\n",
      "epoch 10 | loss: 0.20459 | val_0_auc: 0.98899 |  0:00:17s\n",
      "epoch 11 | loss: 0.19773 | val_0_auc: 0.99532 |  0:00:18s\n",
      "epoch 12 | loss: 0.18483 | val_0_auc: 0.99374 |  0:00:20s\n",
      "epoch 13 | loss: 0.16908 | val_0_auc: 0.99528 |  0:00:21s\n",
      "epoch 14 | loss: 0.17442 | val_0_auc: 0.99743 |  0:00:23s\n",
      "epoch 15 | loss: 0.1825  | val_0_auc: 0.99587 |  0:00:25s\n",
      "epoch 16 | loss: 0.1792  | val_0_auc: 0.99811 |  0:00:26s\n",
      "epoch 17 | loss: 0.16855 | val_0_auc: 0.9978  |  0:00:28s\n",
      "epoch 18 | loss: 0.18228 | val_0_auc: 0.99185 |  0:00:29s\n",
      "epoch 19 | loss: 0.16572 | val_0_auc: 0.99229 |  0:00:31s\n",
      "epoch 20 | loss: 0.17363 | val_0_auc: 0.99494 |  0:00:32s\n",
      "epoch 21 | loss: 0.17119 | val_0_auc: 0.99386 |  0:00:34s\n",
      "epoch 22 | loss: 0.15881 | val_0_auc: 0.99491 |  0:00:35s\n",
      "epoch 23 | loss: 0.15508 | val_0_auc: 0.99689 |  0:00:37s\n",
      "epoch 24 | loss: 0.16851 | val_0_auc: 0.9964  |  0:00:38s\n",
      "epoch 25 | loss: 0.17937 | val_0_auc: 0.99262 |  0:00:40s\n",
      "epoch 26 | loss: 0.17121 | val_0_auc: 0.9912  |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.99811\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99811143  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 42.6437\n",
      "Function value obtained: -0.9981\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6787063455983298, 'lambda_sparse': 0.03595396306581229, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.10113 | val_0_auc: 0.55268 |  0:00:01s\n",
      "epoch 1  | loss: 0.76505 | val_0_auc: 0.67325 |  0:00:02s\n",
      "epoch 2  | loss: 0.64099 | val_0_auc: 0.77869 |  0:00:03s\n",
      "epoch 3  | loss: 0.54746 | val_0_auc: 0.84181 |  0:00:05s\n",
      "epoch 4  | loss: 0.48568 | val_0_auc: 0.90178 |  0:00:06s\n",
      "epoch 5  | loss: 0.39674 | val_0_auc: 0.9209  |  0:00:07s\n",
      "epoch 6  | loss: 0.33536 | val_0_auc: 0.95984 |  0:00:09s\n",
      "epoch 7  | loss: 0.28988 | val_0_auc: 0.96756 |  0:00:10s\n",
      "epoch 8  | loss: 0.24644 | val_0_auc: 0.97449 |  0:00:11s\n",
      "epoch 9  | loss: 0.22565 | val_0_auc: 0.97625 |  0:00:13s\n",
      "epoch 10 | loss: 0.20637 | val_0_auc: 0.98204 |  0:00:14s\n",
      "epoch 11 | loss: 0.19822 | val_0_auc: 0.98547 |  0:00:15s\n",
      "epoch 12 | loss: 0.19519 | val_0_auc: 0.98652 |  0:00:17s\n",
      "epoch 13 | loss: 0.19444 | val_0_auc: 0.99013 |  0:00:18s\n",
      "epoch 14 | loss: 0.1717  | val_0_auc: 0.99485 |  0:00:19s\n",
      "epoch 15 | loss: 0.16279 | val_0_auc: 0.99437 |  0:00:20s\n",
      "epoch 16 | loss: 0.16415 | val_0_auc: 0.98695 |  0:00:22s\n",
      "epoch 17 | loss: 0.16663 | val_0_auc: 0.98965 |  0:00:23s\n",
      "epoch 18 | loss: 0.19134 | val_0_auc: 0.98693 |  0:00:24s\n",
      "epoch 19 | loss: 0.15077 | val_0_auc: 0.99539 |  0:00:26s\n",
      "epoch 20 | loss: 0.14314 | val_0_auc: 0.9886  |  0:00:27s\n",
      "epoch 21 | loss: 0.13765 | val_0_auc: 0.99634 |  0:00:28s\n",
      "epoch 22 | loss: 0.13961 | val_0_auc: 0.99783 |  0:00:29s\n",
      "epoch 23 | loss: 0.12626 | val_0_auc: 0.99686 |  0:00:31s\n",
      "epoch 24 | loss: 0.14541 | val_0_auc: 0.99722 |  0:00:32s\n",
      "epoch 25 | loss: 0.14035 | val_0_auc: 0.98378 |  0:00:33s\n",
      "epoch 26 | loss: 0.13954 | val_0_auc: 0.99445 |  0:00:35s\n",
      "epoch 27 | loss: 0.12167 | val_0_auc: 0.99705 |  0:00:36s\n",
      "epoch 28 | loss: 0.11696 | val_0_auc: 0.99281 |  0:00:37s\n",
      "epoch 29 | loss: 0.13638 | val_0_auc: 0.99612 |  0:00:38s\n",
      "epoch 30 | loss: 0.12336 | val_0_auc: 0.99652 |  0:00:40s\n",
      "epoch 31 | loss: 0.12528 | val_0_auc: 0.99635 |  0:00:41s\n",
      "epoch 32 | loss: 0.15149 | val_0_auc: 0.99666 |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.99783\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9978269  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 43.2554\n",
      "Function value obtained: -0.9978\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.719249669982485, 'lambda_sparse': 0.09039306253419738, 'n_steps': 4, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.97258 | val_0_auc: 0.54514 |  0:00:01s\n",
      "epoch 1  | loss: 0.7151  | val_0_auc: 0.69121 |  0:00:02s\n",
      "epoch 2  | loss: 0.60891 | val_0_auc: 0.83745 |  0:00:03s\n",
      "epoch 3  | loss: 0.53057 | val_0_auc: 0.88178 |  0:00:04s\n",
      "epoch 4  | loss: 0.46619 | val_0_auc: 0.89214 |  0:00:05s\n",
      "epoch 5  | loss: 0.41558 | val_0_auc: 0.9299  |  0:00:06s\n",
      "epoch 6  | loss: 0.37334 | val_0_auc: 0.94623 |  0:00:07s\n",
      "epoch 7  | loss: 0.33096 | val_0_auc: 0.94847 |  0:00:08s\n",
      "epoch 8  | loss: 0.3096  | val_0_auc: 0.9681  |  0:00:09s\n",
      "epoch 9  | loss: 0.29313 | val_0_auc: 0.96771 |  0:00:10s\n",
      "epoch 10 | loss: 0.25841 | val_0_auc: 0.96973 |  0:00:11s\n",
      "epoch 11 | loss: 0.24626 | val_0_auc: 0.98208 |  0:00:12s\n",
      "epoch 12 | loss: 0.2186  | val_0_auc: 0.98116 |  0:00:13s\n",
      "epoch 13 | loss: 0.20371 | val_0_auc: 0.99405 |  0:00:14s\n",
      "epoch 14 | loss: 0.17407 | val_0_auc: 0.99195 |  0:00:15s\n",
      "epoch 15 | loss: 0.18654 | val_0_auc: 0.98733 |  0:00:16s\n",
      "epoch 16 | loss: 0.17829 | val_0_auc: 0.97485 |  0:00:17s\n",
      "epoch 17 | loss: 0.16201 | val_0_auc: 0.9867  |  0:00:18s\n",
      "epoch 18 | loss: 0.14223 | val_0_auc: 0.99567 |  0:00:19s\n",
      "epoch 19 | loss: 0.14508 | val_0_auc: 0.99663 |  0:00:20s\n",
      "epoch 20 | loss: 0.16917 | val_0_auc: 0.99196 |  0:00:21s\n",
      "epoch 21 | loss: 0.1727  | val_0_auc: 0.99607 |  0:00:22s\n",
      "epoch 22 | loss: 0.14912 | val_0_auc: 0.99686 |  0:00:23s\n",
      "epoch 23 | loss: 0.14179 | val_0_auc: 0.99535 |  0:00:24s\n",
      "epoch 24 | loss: 0.15122 | val_0_auc: 0.97953 |  0:00:25s\n",
      "epoch 25 | loss: 0.1349  | val_0_auc: 0.99812 |  0:00:26s\n",
      "epoch 26 | loss: 0.1325  | val_0_auc: 0.99382 |  0:00:27s\n",
      "epoch 27 | loss: 0.14676 | val_0_auc: 0.99302 |  0:00:28s\n",
      "epoch 28 | loss: 0.11748 | val_0_auc: 0.99363 |  0:00:29s\n",
      "epoch 29 | loss: 0.12441 | val_0_auc: 0.99238 |  0:00:30s\n",
      "epoch 30 | loss: 0.13204 | val_0_auc: 0.99858 |  0:00:31s\n",
      "epoch 31 | loss: 0.11759 | val_0_auc: 0.9964  |  0:00:32s\n",
      "epoch 32 | loss: 0.10775 | val_0_auc: 0.99361 |  0:00:33s\n",
      "epoch 33 | loss: 0.1163  | val_0_auc: 0.99548 |  0:00:34s\n",
      "epoch 34 | loss: 0.09128 | val_0_auc: 0.99554 |  0:00:35s\n",
      "epoch 35 | loss: 0.09649 | val_0_auc: 0.99338 |  0:00:36s\n",
      "epoch 36 | loss: 0.12422 | val_0_auc: 0.99615 |  0:00:37s\n",
      "epoch 37 | loss: 0.09977 | val_0_auc: 0.99453 |  0:00:38s\n",
      "epoch 38 | loss: 0.09733 | val_0_auc: 0.99849 |  0:00:39s\n",
      "epoch 39 | loss: 0.10933 | val_0_auc: 0.99741 |  0:00:40s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 30 and best_val_0_auc = 0.99858\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99858269  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 40.6279\n",
      "Function value obtained: -0.9986\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0927665639198525, 'lambda_sparse': 0.019096446938880934, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.20714 | val_0_auc: 0.52118 |  0:00:01s\n",
      "epoch 1  | loss: 0.76572 | val_0_auc: 0.68096 |  0:00:03s\n",
      "epoch 2  | loss: 0.63205 | val_0_auc: 0.80699 |  0:00:05s\n",
      "epoch 3  | loss: 0.5233  | val_0_auc: 0.87088 |  0:00:06s\n",
      "epoch 4  | loss: 0.44675 | val_0_auc: 0.89751 |  0:00:08s\n",
      "epoch 5  | loss: 0.38846 | val_0_auc: 0.90729 |  0:00:10s\n",
      "epoch 6  | loss: 0.34084 | val_0_auc: 0.95104 |  0:00:12s\n",
      "epoch 7  | loss: 0.30567 | val_0_auc: 0.96231 |  0:00:13s\n",
      "epoch 8  | loss: 0.27675 | val_0_auc: 0.94099 |  0:00:15s\n",
      "epoch 9  | loss: 0.25076 | val_0_auc: 0.97602 |  0:00:17s\n",
      "epoch 10 | loss: 0.23083 | val_0_auc: 0.98299 |  0:00:18s\n",
      "epoch 11 | loss: 0.21164 | val_0_auc: 0.97482 |  0:00:20s\n",
      "epoch 12 | loss: 0.18526 | val_0_auc: 0.98331 |  0:00:22s\n",
      "epoch 13 | loss: 0.21097 | val_0_auc: 0.98664 |  0:00:24s\n",
      "epoch 14 | loss: 0.21283 | val_0_auc: 0.98949 |  0:00:25s\n",
      "epoch 15 | loss: 0.17873 | val_0_auc: 0.98398 |  0:00:27s\n",
      "epoch 16 | loss: 0.16629 | val_0_auc: 0.99157 |  0:00:29s\n",
      "epoch 17 | loss: 0.14712 | val_0_auc: 0.99495 |  0:00:31s\n",
      "epoch 18 | loss: 0.14259 | val_0_auc: 0.99214 |  0:00:32s\n",
      "epoch 19 | loss: 0.13642 | val_0_auc: 0.99398 |  0:00:34s\n",
      "epoch 20 | loss: 0.13382 | val_0_auc: 0.9931  |  0:00:36s\n",
      "epoch 21 | loss: 0.14182 | val_0_auc: 0.99061 |  0:00:37s\n",
      "epoch 22 | loss: 0.16415 | val_0_auc: 0.9946  |  0:00:39s\n",
      "epoch 23 | loss: 0.13996 | val_0_auc: 0.98664 |  0:00:41s\n",
      "epoch 24 | loss: 0.13084 | val_0_auc: 0.99448 |  0:00:43s\n",
      "epoch 25 | loss: 0.12732 | val_0_auc: 0.98384 |  0:00:44s\n",
      "epoch 26 | loss: 0.13086 | val_0_auc: 0.99517 |  0:00:46s\n",
      "epoch 27 | loss: 0.13354 | val_0_auc: 0.99589 |  0:00:48s\n",
      "epoch 28 | loss: 0.12383 | val_0_auc: 0.99295 |  0:00:50s\n",
      "epoch 29 | loss: 0.13077 | val_0_auc: 0.9952  |  0:00:51s\n",
      "epoch 30 | loss: 0.12921 | val_0_auc: 0.99573 |  0:00:53s\n",
      "epoch 31 | loss: 0.12219 | val_0_auc: 0.99738 |  0:00:55s\n",
      "epoch 32 | loss: 0.123   | val_0_auc: 0.99704 |  0:00:56s\n",
      "epoch 33 | loss: 0.12706 | val_0_auc: 0.99755 |  0:00:58s\n",
      "epoch 34 | loss: 0.11776 | val_0_auc: 0.99781 |  0:01:00s\n",
      "epoch 35 | loss: 0.11621 | val_0_auc: 0.99319 |  0:01:02s\n",
      "epoch 36 | loss: 0.11039 | val_0_auc: 0.99552 |  0:01:03s\n",
      "epoch 37 | loss: 0.1051  | val_0_auc: 0.99621 |  0:01:05s\n",
      "epoch 38 | loss: 0.12673 | val_0_auc: 0.9982  |  0:01:07s\n",
      "epoch 39 | loss: 0.10525 | val_0_auc: 0.99688 |  0:01:08s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.9982\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99820213  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 69.4340\n",
      "Function value obtained: -0.9982\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5137450672706998, 'lambda_sparse': 0.06145994593718287, 'n_steps': 8, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.83618 | val_0_auc: 0.53659 |  0:00:02s\n",
      "epoch 1  | loss: 1.29291 | val_0_auc: 0.53517 |  0:00:04s\n",
      "epoch 2  | loss: 1.48911 | val_0_auc: 0.53978 |  0:00:07s\n",
      "epoch 3  | loss: 1.05964 | val_0_auc: 0.58741 |  0:00:09s\n",
      "epoch 4  | loss: 0.8644  | val_0_auc: 0.59203 |  0:00:12s\n",
      "epoch 5  | loss: 0.82588 | val_0_auc: 0.61043 |  0:00:15s\n",
      "epoch 6  | loss: 0.76589 | val_0_auc: 0.64867 |  0:00:17s\n",
      "epoch 7  | loss: 0.70715 | val_0_auc: 0.65491 |  0:00:19s\n",
      "epoch 8  | loss: 0.66637 | val_0_auc: 0.7304  |  0:00:22s\n",
      "epoch 9  | loss: 0.63166 | val_0_auc: 0.7796  |  0:00:24s\n",
      "epoch 10 | loss: 0.58359 | val_0_auc: 0.80881 |  0:00:27s\n",
      "epoch 11 | loss: 0.50652 | val_0_auc: 0.82295 |  0:00:29s\n",
      "epoch 12 | loss: 0.50257 | val_0_auc: 0.8334  |  0:00:32s\n",
      "epoch 13 | loss: 0.47317 | val_0_auc: 0.87807 |  0:00:34s\n",
      "epoch 14 | loss: 0.4527  | val_0_auc: 0.87845 |  0:00:37s\n",
      "epoch 15 | loss: 0.40726 | val_0_auc: 0.9189  |  0:00:39s\n",
      "epoch 16 | loss: 0.38083 | val_0_auc: 0.93447 |  0:00:42s\n",
      "epoch 17 | loss: 0.34644 | val_0_auc: 0.93069 |  0:00:44s\n",
      "epoch 18 | loss: 0.32637 | val_0_auc: 0.95233 |  0:00:47s\n",
      "epoch 19 | loss: 0.30727 | val_0_auc: 0.96226 |  0:00:49s\n",
      "epoch 20 | loss: 0.33458 | val_0_auc: 0.95381 |  0:00:52s\n",
      "epoch 21 | loss: 0.35211 | val_0_auc: 0.94237 |  0:00:54s\n",
      "epoch 22 | loss: 0.32839 | val_0_auc: 0.93769 |  0:00:57s\n",
      "epoch 23 | loss: 0.32379 | val_0_auc: 0.94811 |  0:00:59s\n",
      "epoch 24 | loss: 0.30558 | val_0_auc: 0.94925 |  0:01:02s\n",
      "epoch 25 | loss: 0.28663 | val_0_auc: 0.9577  |  0:01:04s\n",
      "epoch 26 | loss: 0.24862 | val_0_auc: 0.97464 |  0:01:07s\n",
      "epoch 27 | loss: 0.24533 | val_0_auc: 0.94882 |  0:01:09s\n",
      "epoch 28 | loss: 0.24859 | val_0_auc: 0.98243 |  0:01:12s\n",
      "epoch 29 | loss: 0.22025 | val_0_auc: 0.98898 |  0:01:14s\n",
      "epoch 30 | loss: 0.22052 | val_0_auc: 0.97198 |  0:01:17s\n",
      "epoch 31 | loss: 0.214   | val_0_auc: 0.98306 |  0:01:19s\n",
      "epoch 32 | loss: 0.21804 | val_0_auc: 0.98901 |  0:01:22s\n",
      "epoch 33 | loss: 0.22082 | val_0_auc: 0.9881  |  0:01:24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | loss: 0.22548 | val_0_auc: 0.97713 |  0:01:27s\n",
      "epoch 35 | loss: 0.21585 | val_0_auc: 0.9852  |  0:01:29s\n",
      "epoch 36 | loss: 0.19689 | val_0_auc: 0.98495 |  0:01:32s\n",
      "epoch 37 | loss: 0.19396 | val_0_auc: 0.98683 |  0:01:34s\n",
      "epoch 38 | loss: 0.17581 | val_0_auc: 0.9934  |  0:01:37s\n",
      "epoch 39 | loss: 0.17951 | val_0_auc: 0.99185 |  0:01:39s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.9934\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99339713  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 100.4755\n",
      "Function value obtained: -0.9934\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5557035610754368, 'lambda_sparse': 0.0397448722597452, 'n_steps': 9, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.10433 | val_0_auc: 0.58422 |  0:00:02s\n",
      "epoch 1  | loss: 1.13804 | val_0_auc: 0.62032 |  0:00:04s\n",
      "epoch 2  | loss: 1.023   | val_0_auc: 0.63938 |  0:00:06s\n",
      "epoch 3  | loss: 1.12742 | val_0_auc: 0.66278 |  0:00:08s\n",
      "epoch 4  | loss: 0.8171  | val_0_auc: 0.69763 |  0:00:10s\n",
      "epoch 5  | loss: 0.77176 | val_0_auc: 0.69439 |  0:00:12s\n",
      "epoch 6  | loss: 0.75643 | val_0_auc: 0.69551 |  0:00:14s\n",
      "epoch 7  | loss: 0.74108 | val_0_auc: 0.73032 |  0:00:17s\n",
      "epoch 8  | loss: 0.71101 | val_0_auc: 0.78968 |  0:00:19s\n",
      "epoch 9  | loss: 0.76441 | val_0_auc: 0.81079 |  0:00:21s\n",
      "epoch 10 | loss: 0.66559 | val_0_auc: 0.81556 |  0:00:23s\n",
      "epoch 11 | loss: 0.65079 | val_0_auc: 0.84051 |  0:00:25s\n",
      "epoch 12 | loss: 0.56984 | val_0_auc: 0.83207 |  0:00:27s\n",
      "epoch 13 | loss: 0.57855 | val_0_auc: 0.86567 |  0:00:29s\n",
      "epoch 14 | loss: 0.47818 | val_0_auc: 0.88943 |  0:00:31s\n",
      "epoch 15 | loss: 0.45378 | val_0_auc: 0.86042 |  0:00:33s\n",
      "epoch 16 | loss: 0.51228 | val_0_auc: 0.84709 |  0:00:36s\n",
      "epoch 17 | loss: 0.51316 | val_0_auc: 0.82221 |  0:00:38s\n",
      "epoch 18 | loss: 0.51079 | val_0_auc: 0.83518 |  0:00:40s\n",
      "epoch 19 | loss: 0.47587 | val_0_auc: 0.86516 |  0:00:42s\n",
      "epoch 20 | loss: 0.44617 | val_0_auc: 0.86442 |  0:00:44s\n",
      "epoch 21 | loss: 0.39932 | val_0_auc: 0.89176 |  0:00:46s\n",
      "epoch 22 | loss: 0.39478 | val_0_auc: 0.89556 |  0:00:48s\n",
      "epoch 23 | loss: 0.39326 | val_0_auc: 0.89483 |  0:00:50s\n",
      "epoch 24 | loss: 0.37225 | val_0_auc: 0.91896 |  0:00:53s\n",
      "epoch 25 | loss: 0.35511 | val_0_auc: 0.91145 |  0:00:55s\n",
      "epoch 26 | loss: 0.36213 | val_0_auc: 0.89639 |  0:00:57s\n",
      "epoch 27 | loss: 0.34523 | val_0_auc: 0.95237 |  0:00:59s\n",
      "epoch 28 | loss: 0.31316 | val_0_auc: 0.95817 |  0:01:01s\n",
      "epoch 29 | loss: 0.31948 | val_0_auc: 0.95027 |  0:01:03s\n",
      "epoch 30 | loss: 0.29842 | val_0_auc: 0.96358 |  0:01:05s\n",
      "epoch 31 | loss: 0.30832 | val_0_auc: 0.96055 |  0:01:07s\n",
      "epoch 32 | loss: 0.2984  | val_0_auc: 0.9564  |  0:01:10s\n",
      "epoch 33 | loss: 0.29525 | val_0_auc: 0.96657 |  0:01:12s\n",
      "epoch 34 | loss: 0.28341 | val_0_auc: 0.96357 |  0:01:14s\n",
      "epoch 35 | loss: 0.25413 | val_0_auc: 0.95886 |  0:01:16s\n",
      "epoch 36 | loss: 0.26555 | val_0_auc: 0.96304 |  0:01:18s\n",
      "epoch 37 | loss: 0.24725 | val_0_auc: 0.97172 |  0:01:20s\n",
      "epoch 38 | loss: 0.26406 | val_0_auc: 0.95717 |  0:01:22s\n",
      "epoch 39 | loss: 0.27131 | val_0_auc: 0.96105 |  0:01:24s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.97172\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97172484  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 85.6203\n",
      "Function value obtained: -0.9717\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.982987424331422, 'lambda_sparse': 0.09517076405153095, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.59041 | val_0_auc: 0.53704 |  0:00:01s\n",
      "epoch 1  | loss: 0.98021 | val_0_auc: 0.55737 |  0:00:03s\n",
      "epoch 2  | loss: 0.89243 | val_0_auc: 0.53586 |  0:00:05s\n",
      "epoch 3  | loss: 0.84082 | val_0_auc: 0.58174 |  0:00:06s\n",
      "epoch 4  | loss: 0.74767 | val_0_auc: 0.5891  |  0:00:08s\n",
      "epoch 5  | loss: 0.79148 | val_0_auc: 0.61828 |  0:00:10s\n",
      "epoch 6  | loss: 0.84168 | val_0_auc: 0.56925 |  0:00:11s\n",
      "epoch 7  | loss: 0.73676 | val_0_auc: 0.66196 |  0:00:13s\n",
      "epoch 8  | loss: 0.69238 | val_0_auc: 0.67    |  0:00:15s\n",
      "epoch 9  | loss: 0.64292 | val_0_auc: 0.69215 |  0:00:17s\n",
      "epoch 10 | loss: 0.64288 | val_0_auc: 0.74358 |  0:00:18s\n",
      "epoch 11 | loss: 0.61707 | val_0_auc: 0.78301 |  0:00:20s\n",
      "epoch 12 | loss: 0.56057 | val_0_auc: 0.79051 |  0:00:22s\n",
      "epoch 13 | loss: 0.52681 | val_0_auc: 0.84063 |  0:00:24s\n",
      "epoch 14 | loss: 0.51346 | val_0_auc: 0.84564 |  0:00:25s\n",
      "epoch 15 | loss: 0.48586 | val_0_auc: 0.86527 |  0:00:27s\n",
      "epoch 16 | loss: 0.45424 | val_0_auc: 0.87231 |  0:00:29s\n",
      "epoch 17 | loss: 0.44548 | val_0_auc: 0.89688 |  0:00:30s\n",
      "epoch 18 | loss: 0.42774 | val_0_auc: 0.92441 |  0:00:32s\n",
      "epoch 19 | loss: 0.38349 | val_0_auc: 0.92114 |  0:00:34s\n",
      "epoch 20 | loss: 0.37291 | val_0_auc: 0.92995 |  0:00:36s\n",
      "epoch 21 | loss: 0.34931 | val_0_auc: 0.945   |  0:00:37s\n",
      "epoch 22 | loss: 0.32157 | val_0_auc: 0.9513  |  0:00:39s\n",
      "epoch 23 | loss: 0.2992  | val_0_auc: 0.96075 |  0:00:41s\n",
      "epoch 24 | loss: 0.29862 | val_0_auc: 0.95691 |  0:00:42s\n",
      "epoch 25 | loss: 0.27812 | val_0_auc: 0.96126 |  0:00:44s\n",
      "epoch 26 | loss: 0.25124 | val_0_auc: 0.95523 |  0:00:46s\n",
      "epoch 27 | loss: 0.23511 | val_0_auc: 0.97504 |  0:00:48s\n",
      "epoch 28 | loss: 0.21536 | val_0_auc: 0.97651 |  0:00:49s\n",
      "epoch 29 | loss: 0.22254 | val_0_auc: 0.97597 |  0:00:51s\n",
      "epoch 30 | loss: 0.20874 | val_0_auc: 0.98284 |  0:00:53s\n",
      "epoch 31 | loss: 0.2056  | val_0_auc: 0.98649 |  0:00:54s\n",
      "epoch 32 | loss: 0.17669 | val_0_auc: 0.99137 |  0:00:56s\n",
      "epoch 33 | loss: 0.16842 | val_0_auc: 0.9929  |  0:00:58s\n",
      "epoch 34 | loss: 0.16077 | val_0_auc: 0.98849 |  0:01:00s\n",
      "epoch 35 | loss: 0.17376 | val_0_auc: 0.99138 |  0:01:01s\n",
      "epoch 36 | loss: 0.17891 | val_0_auc: 0.98613 |  0:01:03s\n",
      "epoch 37 | loss: 0.17408 | val_0_auc: 0.99357 |  0:01:05s\n",
      "epoch 38 | loss: 0.1604  | val_0_auc: 0.99058 |  0:01:06s\n",
      "epoch 39 | loss: 0.16847 | val_0_auc: 0.97933 |  0:01:08s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99357\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99356962  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 69.1689\n",
      "Function value obtained: -0.9936\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9950708492465128, 'lambda_sparse': 0.012660397300611798, 'n_steps': 4, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.16661 | val_0_auc: 0.61806 |  0:00:01s\n",
      "epoch 1  | loss: 0.81856 | val_0_auc: 0.73557 |  0:00:02s\n",
      "epoch 2  | loss: 0.59701 | val_0_auc: 0.85101 |  0:00:04s\n",
      "epoch 3  | loss: 0.44397 | val_0_auc: 0.91815 |  0:00:05s\n",
      "epoch 4  | loss: 0.34417 | val_0_auc: 0.92143 |  0:00:07s\n",
      "epoch 5  | loss: 0.29177 | val_0_auc: 0.96861 |  0:00:08s\n",
      "epoch 6  | loss: 0.26605 | val_0_auc: 0.97237 |  0:00:09s\n",
      "epoch 7  | loss: 0.23124 | val_0_auc: 0.97134 |  0:00:11s\n",
      "epoch 8  | loss: 0.22473 | val_0_auc: 0.97994 |  0:00:12s\n",
      "epoch 9  | loss: 0.18986 | val_0_auc: 0.98745 |  0:00:14s\n",
      "epoch 10 | loss: 0.16779 | val_0_auc: 0.99402 |  0:00:15s\n",
      "epoch 11 | loss: 0.15821 | val_0_auc: 0.99266 |  0:00:16s\n",
      "epoch 12 | loss: 0.15376 | val_0_auc: 0.99475 |  0:00:18s\n",
      "epoch 13 | loss: 0.13791 | val_0_auc: 0.98744 |  0:00:19s\n",
      "epoch 14 | loss: 0.11362 | val_0_auc: 0.99625 |  0:00:21s\n",
      "epoch 15 | loss: 0.11365 | val_0_auc: 0.99193 |  0:00:22s\n",
      "epoch 16 | loss: 0.10877 | val_0_auc: 0.99783 |  0:00:23s\n",
      "epoch 17 | loss: 0.10332 | val_0_auc: 0.99617 |  0:00:25s\n",
      "epoch 18 | loss: 0.12048 | val_0_auc: 0.9962  |  0:00:26s\n",
      "epoch 19 | loss: 0.11654 | val_0_auc: 0.99723 |  0:00:28s\n",
      "epoch 20 | loss: 0.10129 | val_0_auc: 0.99586 |  0:00:29s\n",
      "epoch 21 | loss: 0.10704 | val_0_auc: 0.99233 |  0:00:30s\n",
      "epoch 22 | loss: 0.11    | val_0_auc: 0.99744 |  0:00:32s\n",
      "epoch 23 | loss: 0.10765 | val_0_auc: 0.99724 |  0:00:33s\n",
      "epoch 24 | loss: 0.09805 | val_0_auc: 0.99876 |  0:00:35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | loss: 0.11243 | val_0_auc: 0.99691 |  0:00:36s\n",
      "epoch 26 | loss: 0.09972 | val_0_auc: 0.99884 |  0:00:38s\n",
      "epoch 27 | loss: 0.09214 | val_0_auc: 0.99874 |  0:00:39s\n",
      "epoch 28 | loss: 0.09739 | val_0_auc: 0.99786 |  0:00:40s\n",
      "epoch 29 | loss: 0.10585 | val_0_auc: 0.99706 |  0:00:42s\n",
      "epoch 30 | loss: 0.09613 | val_0_auc: 0.99672 |  0:00:43s\n",
      "epoch 31 | loss: 0.10733 | val_0_auc: 0.99767 |  0:00:45s\n",
      "epoch 32 | loss: 0.1007  | val_0_auc: 0.99163 |  0:00:46s\n",
      "epoch 33 | loss: 0.11127 | val_0_auc: 0.99376 |  0:00:47s\n",
      "epoch 34 | loss: 0.11037 | val_0_auc: 0.99678 |  0:00:49s\n",
      "epoch 35 | loss: 0.10063 | val_0_auc: 0.99813 |  0:00:50s\n",
      "epoch 36 | loss: 0.08975 | val_0_auc: 0.99629 |  0:00:52s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.99884\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99883876  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 52.7236\n",
      "Function value obtained: -0.9988\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3296036991530098, 'lambda_sparse': 0.001675385376420477, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.98215 | val_0_auc: 0.50794 |  0:00:01s\n",
      "epoch 1  | loss: 0.7693  | val_0_auc: 0.55128 |  0:00:02s\n",
      "epoch 2  | loss: 0.71852 | val_0_auc: 0.60703 |  0:00:03s\n",
      "epoch 3  | loss: 0.67628 | val_0_auc: 0.67818 |  0:00:04s\n",
      "epoch 4  | loss: 0.62648 | val_0_auc: 0.74468 |  0:00:06s\n",
      "epoch 5  | loss: 0.59298 | val_0_auc: 0.77553 |  0:00:07s\n",
      "epoch 6  | loss: 0.55531 | val_0_auc: 0.81743 |  0:00:08s\n",
      "epoch 7  | loss: 0.51533 | val_0_auc: 0.84642 |  0:00:09s\n",
      "epoch 8  | loss: 0.47742 | val_0_auc: 0.88568 |  0:00:11s\n",
      "epoch 9  | loss: 0.4067  | val_0_auc: 0.91285 |  0:00:12s\n",
      "epoch 10 | loss: 0.3612  | val_0_auc: 0.91437 |  0:00:13s\n",
      "epoch 11 | loss: 0.31247 | val_0_auc: 0.95217 |  0:00:14s\n",
      "epoch 12 | loss: 0.27995 | val_0_auc: 0.95603 |  0:00:16s\n",
      "epoch 13 | loss: 0.27234 | val_0_auc: 0.95834 |  0:00:17s\n",
      "epoch 14 | loss: 0.24401 | val_0_auc: 0.97694 |  0:00:18s\n",
      "epoch 15 | loss: 0.22279 | val_0_auc: 0.9527  |  0:00:19s\n",
      "epoch 16 | loss: 0.20519 | val_0_auc: 0.98438 |  0:00:21s\n",
      "epoch 17 | loss: 0.18315 | val_0_auc: 0.98973 |  0:00:22s\n",
      "epoch 18 | loss: 0.15814 | val_0_auc: 0.99195 |  0:00:23s\n",
      "epoch 19 | loss: 0.14612 | val_0_auc: 0.98808 |  0:00:24s\n",
      "epoch 20 | loss: 0.14637 | val_0_auc: 0.9865  |  0:00:25s\n",
      "epoch 21 | loss: 0.1507  | val_0_auc: 0.99002 |  0:00:27s\n",
      "epoch 22 | loss: 0.14725 | val_0_auc: 0.98639 |  0:00:28s\n",
      "epoch 23 | loss: 0.13657 | val_0_auc: 0.99532 |  0:00:29s\n",
      "epoch 24 | loss: 0.13594 | val_0_auc: 0.99096 |  0:00:30s\n",
      "epoch 25 | loss: 0.12505 | val_0_auc: 0.98805 |  0:00:32s\n",
      "epoch 26 | loss: 0.12497 | val_0_auc: 0.99167 |  0:00:33s\n",
      "epoch 27 | loss: 0.11767 | val_0_auc: 0.99447 |  0:00:34s\n",
      "epoch 28 | loss: 0.12542 | val_0_auc: 0.99143 |  0:00:35s\n",
      "epoch 29 | loss: 0.1013  | val_0_auc: 0.99638 |  0:00:36s\n",
      "epoch 30 | loss: 0.10892 | val_0_auc: 0.99679 |  0:00:38s\n",
      "epoch 31 | loss: 0.10743 | val_0_auc: 0.98648 |  0:00:39s\n",
      "epoch 32 | loss: 0.11181 | val_0_auc: 0.99707 |  0:00:40s\n",
      "epoch 33 | loss: 0.10563 | val_0_auc: 0.99289 |  0:00:41s\n",
      "epoch 34 | loss: 0.1015  | val_0_auc: 0.99312 |  0:00:43s\n",
      "epoch 35 | loss: 0.08789 | val_0_auc: 0.99425 |  0:00:44s\n",
      "epoch 36 | loss: 0.08938 | val_0_auc: 0.99701 |  0:00:45s\n",
      "epoch 37 | loss: 0.09325 | val_0_auc: 0.99637 |  0:00:46s\n",
      "epoch 38 | loss: 0.08225 | val_0_auc: 0.99685 |  0:00:47s\n",
      "epoch 39 | loss: 0.08597 | val_0_auc: 0.99681 |  0:00:49s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 32 and best_val_0_auc = 0.99707\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99706934  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 49.4920\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.890257497697018, 'lambda_sparse': 0.03411077940011832, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.99451 | val_0_auc: 0.54026 |  0:00:01s\n",
      "epoch 1  | loss: 0.81531 | val_0_auc: 0.59727 |  0:00:02s\n",
      "epoch 2  | loss: 0.70768 | val_0_auc: 0.61088 |  0:00:03s\n",
      "epoch 3  | loss: 0.66047 | val_0_auc: 0.69072 |  0:00:04s\n",
      "epoch 4  | loss: 0.63671 | val_0_auc: 0.712   |  0:00:06s\n",
      "epoch 5  | loss: 0.6222  | val_0_auc: 0.72908 |  0:00:07s\n",
      "epoch 6  | loss: 0.58177 | val_0_auc: 0.80234 |  0:00:08s\n",
      "epoch 7  | loss: 0.52545 | val_0_auc: 0.84269 |  0:00:09s\n",
      "epoch 8  | loss: 0.46025 | val_0_auc: 0.90792 |  0:00:10s\n",
      "epoch 9  | loss: 0.4003  | val_0_auc: 0.9274  |  0:00:12s\n",
      "epoch 10 | loss: 0.38823 | val_0_auc: 0.92301 |  0:00:13s\n",
      "epoch 11 | loss: 0.32968 | val_0_auc: 0.9375  |  0:00:14s\n",
      "epoch 12 | loss: 0.30504 | val_0_auc: 0.94832 |  0:00:15s\n",
      "epoch 13 | loss: 0.28638 | val_0_auc: 0.96174 |  0:00:16s\n",
      "epoch 14 | loss: 0.26975 | val_0_auc: 0.96365 |  0:00:17s\n",
      "epoch 15 | loss: 0.28884 | val_0_auc: 0.95148 |  0:00:19s\n",
      "epoch 16 | loss: 0.26505 | val_0_auc: 0.96925 |  0:00:20s\n",
      "epoch 17 | loss: 0.24586 | val_0_auc: 0.97978 |  0:00:21s\n",
      "epoch 18 | loss: 0.25345 | val_0_auc: 0.96952 |  0:00:22s\n",
      "epoch 19 | loss: 0.23026 | val_0_auc: 0.97895 |  0:00:23s\n",
      "epoch 20 | loss: 0.18258 | val_0_auc: 0.98012 |  0:00:25s\n",
      "epoch 21 | loss: 0.20075 | val_0_auc: 0.98911 |  0:00:26s\n",
      "epoch 22 | loss: 0.18625 | val_0_auc: 0.99186 |  0:00:27s\n",
      "epoch 23 | loss: 0.17899 | val_0_auc: 0.99001 |  0:00:28s\n",
      "epoch 24 | loss: 0.18805 | val_0_auc: 0.98618 |  0:00:29s\n",
      "epoch 25 | loss: 0.16094 | val_0_auc: 0.99306 |  0:00:30s\n",
      "epoch 26 | loss: 0.14457 | val_0_auc: 0.99156 |  0:00:32s\n",
      "epoch 27 | loss: 0.15484 | val_0_auc: 0.9909  |  0:00:33s\n",
      "epoch 28 | loss: 0.15627 | val_0_auc: 0.99542 |  0:00:34s\n",
      "epoch 29 | loss: 0.13397 | val_0_auc: 0.99315 |  0:00:35s\n",
      "epoch 30 | loss: 0.13148 | val_0_auc: 0.99006 |  0:00:36s\n",
      "epoch 31 | loss: 0.12437 | val_0_auc: 0.99725 |  0:00:38s\n",
      "epoch 32 | loss: 0.12532 | val_0_auc: 0.99771 |  0:00:39s\n",
      "epoch 33 | loss: 0.12825 | val_0_auc: 0.99731 |  0:00:40s\n",
      "epoch 34 | loss: 0.11584 | val_0_auc: 0.9982  |  0:00:41s\n",
      "epoch 35 | loss: 0.14178 | val_0_auc: 0.99691 |  0:00:42s\n",
      "epoch 36 | loss: 0.12861 | val_0_auc: 0.99162 |  0:00:44s\n",
      "epoch 37 | loss: 0.11317 | val_0_auc: 0.99786 |  0:00:45s\n",
      "epoch 38 | loss: 0.13076 | val_0_auc: 0.99897 |  0:00:46s\n",
      "epoch 39 | loss: 0.10942 | val_0_auc: 0.99733 |  0:00:47s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99897\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99897391  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 48.0458\n",
      "Function value obtained: -0.9990\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9734517422911204, 'lambda_sparse': 0.02428881036992177, 'n_steps': 5, 'n_a': 8, 'n_d': 8, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.04619 | val_0_auc: 0.51699 |  0:00:00s\n",
      "epoch 1  | loss: 0.82208 | val_0_auc: 0.6046  |  0:00:01s\n",
      "epoch 2  | loss: 0.71966 | val_0_auc: 0.6123  |  0:00:02s\n",
      "epoch 3  | loss: 0.68405 | val_0_auc: 0.65105 |  0:00:03s\n",
      "epoch 4  | loss: 0.66104 | val_0_auc: 0.62407 |  0:00:04s\n",
      "epoch 5  | loss: 0.65438 | val_0_auc: 0.65236 |  0:00:05s\n",
      "epoch 6  | loss: 0.63727 | val_0_auc: 0.69296 |  0:00:06s\n",
      "epoch 7  | loss: 0.5984  | val_0_auc: 0.72846 |  0:00:07s\n",
      "epoch 8  | loss: 0.54029 | val_0_auc: 0.83968 |  0:00:08s\n",
      "epoch 9  | loss: 0.4798  | val_0_auc: 0.89361 |  0:00:08s\n",
      "epoch 10 | loss: 0.42779 | val_0_auc: 0.87917 |  0:00:09s\n",
      "epoch 11 | loss: 0.39654 | val_0_auc: 0.9076  |  0:00:10s\n",
      "epoch 12 | loss: 0.36779 | val_0_auc: 0.92237 |  0:00:11s\n",
      "epoch 13 | loss: 0.33708 | val_0_auc: 0.94523 |  0:00:12s\n",
      "epoch 14 | loss: 0.32046 | val_0_auc: 0.95036 |  0:00:13s\n",
      "epoch 15 | loss: 0.28198 | val_0_auc: 0.9542  |  0:00:14s\n",
      "epoch 16 | loss: 0.24905 | val_0_auc: 0.98024 |  0:00:15s\n",
      "epoch 17 | loss: 0.22328 | val_0_auc: 0.98098 |  0:00:15s\n",
      "epoch 18 | loss: 0.22933 | val_0_auc: 0.97578 |  0:00:16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.23059 | val_0_auc: 0.95223 |  0:00:17s\n",
      "epoch 20 | loss: 0.25827 | val_0_auc: 0.9711  |  0:00:18s\n",
      "epoch 21 | loss: 0.22716 | val_0_auc: 0.96764 |  0:00:19s\n",
      "epoch 22 | loss: 0.18442 | val_0_auc: 0.98507 |  0:00:20s\n",
      "epoch 23 | loss: 0.18091 | val_0_auc: 0.97964 |  0:00:21s\n",
      "epoch 24 | loss: 0.1768  | val_0_auc: 0.98246 |  0:00:22s\n",
      "epoch 25 | loss: 0.16335 | val_0_auc: 0.9706  |  0:00:23s\n",
      "epoch 26 | loss: 0.1469  | val_0_auc: 0.99211 |  0:00:23s\n",
      "epoch 27 | loss: 0.15968 | val_0_auc: 0.98341 |  0:00:24s\n",
      "epoch 28 | loss: 0.14989 | val_0_auc: 0.99254 |  0:00:25s\n",
      "epoch 29 | loss: 0.17773 | val_0_auc: 0.97727 |  0:00:26s\n",
      "epoch 30 | loss: 0.14675 | val_0_auc: 0.99508 |  0:00:27s\n",
      "epoch 31 | loss: 0.14136 | val_0_auc: 0.99382 |  0:00:28s\n",
      "epoch 32 | loss: 0.13138 | val_0_auc: 0.99197 |  0:00:29s\n",
      "epoch 33 | loss: 0.14659 | val_0_auc: 0.99054 |  0:00:30s\n",
      "epoch 34 | loss: 0.12874 | val_0_auc: 0.99093 |  0:00:30s\n",
      "epoch 35 | loss: 0.14046 | val_0_auc: 0.99357 |  0:00:31s\n",
      "epoch 36 | loss: 0.13261 | val_0_auc: 0.99583 |  0:00:32s\n",
      "epoch 37 | loss: 0.15044 | val_0_auc: 0.98824 |  0:00:33s\n",
      "epoch 38 | loss: 0.14621 | val_0_auc: 0.99635 |  0:00:34s\n",
      "epoch 39 | loss: 0.11704 | val_0_auc: 0.99204 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99635\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99635446  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 35.5773\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0056246763853502, 'lambda_sparse': 0.05734072721211749, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.99432 | val_0_auc: 0.66372 |  0:00:01s\n",
      "epoch 1  | loss: 0.69756 | val_0_auc: 0.77461 |  0:00:02s\n",
      "epoch 2  | loss: 0.56929 | val_0_auc: 0.87763 |  0:00:03s\n",
      "epoch 3  | loss: 0.44483 | val_0_auc: 0.92527 |  0:00:04s\n",
      "epoch 4  | loss: 0.36712 | val_0_auc: 0.95496 |  0:00:05s\n",
      "epoch 5  | loss: 0.28402 | val_0_auc: 0.97159 |  0:00:07s\n",
      "epoch 6  | loss: 0.22074 | val_0_auc: 0.98895 |  0:00:08s\n",
      "epoch 7  | loss: 0.21321 | val_0_auc: 0.98754 |  0:00:09s\n",
      "epoch 8  | loss: 0.19638 | val_0_auc: 0.99299 |  0:00:10s\n",
      "epoch 9  | loss: 0.17479 | val_0_auc: 0.98017 |  0:00:11s\n",
      "epoch 10 | loss: 0.19628 | val_0_auc: 0.9865  |  0:00:13s\n",
      "epoch 11 | loss: 0.17919 | val_0_auc: 0.99252 |  0:00:14s\n",
      "epoch 12 | loss: 0.179   | val_0_auc: 0.98902 |  0:00:15s\n",
      "epoch 13 | loss: 0.1661  | val_0_auc: 0.99239 |  0:00:16s\n",
      "epoch 14 | loss: 0.17278 | val_0_auc: 0.9935  |  0:00:17s\n",
      "epoch 15 | loss: 0.15763 | val_0_auc: 0.99395 |  0:00:19s\n",
      "epoch 16 | loss: 0.1446  | val_0_auc: 0.99366 |  0:00:20s\n",
      "epoch 17 | loss: 0.12553 | val_0_auc: 0.99736 |  0:00:21s\n",
      "epoch 18 | loss: 0.15041 | val_0_auc: 0.99643 |  0:00:22s\n",
      "epoch 19 | loss: 0.14449 | val_0_auc: 0.99585 |  0:00:23s\n",
      "epoch 20 | loss: 0.13256 | val_0_auc: 0.99097 |  0:00:25s\n",
      "epoch 21 | loss: 0.13967 | val_0_auc: 0.99625 |  0:00:26s\n",
      "epoch 22 | loss: 0.12767 | val_0_auc: 0.99878 |  0:00:27s\n",
      "epoch 23 | loss: 0.11799 | val_0_auc: 0.99709 |  0:00:28s\n",
      "epoch 24 | loss: 0.12846 | val_0_auc: 0.99682 |  0:00:29s\n",
      "epoch 25 | loss: 0.13169 | val_0_auc: 0.9975  |  0:00:31s\n",
      "epoch 26 | loss: 0.12633 | val_0_auc: 0.99561 |  0:00:32s\n",
      "epoch 27 | loss: 0.12863 | val_0_auc: 0.99837 |  0:00:33s\n",
      "epoch 28 | loss: 0.12744 | val_0_auc: 0.99382 |  0:00:34s\n",
      "epoch 29 | loss: 0.12825 | val_0_auc: 0.99779 |  0:00:35s\n",
      "epoch 30 | loss: 0.12693 | val_0_auc: 0.99684 |  0:00:36s\n",
      "epoch 31 | loss: 0.12315 | val_0_auc: 0.9982  |  0:00:38s\n",
      "epoch 32 | loss: 0.12447 | val_0_auc: 0.99674 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.99878\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9987783  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 39.8009\n",
      "Function value obtained: -0.9988\n",
      "Current minimum: -0.9997\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.024700779870837, 'lambda_sparse': 0.0814487133943405, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.0187  | val_0_auc: 0.62656 |  0:00:01s\n",
      "epoch 1  | loss: 0.74159 | val_0_auc: 0.77184 |  0:00:02s\n",
      "epoch 2  | loss: 0.58696 | val_0_auc: 0.88701 |  0:00:03s\n",
      "epoch 3  | loss: 0.43094 | val_0_auc: 0.95179 |  0:00:04s\n",
      "epoch 4  | loss: 0.32481 | val_0_auc: 0.98072 |  0:00:06s\n",
      "epoch 5  | loss: 0.24209 | val_0_auc: 0.98406 |  0:00:07s\n",
      "epoch 6  | loss: 0.20335 | val_0_auc: 0.99049 |  0:00:08s\n",
      "epoch 7  | loss: 0.2015  | val_0_auc: 0.98614 |  0:00:09s\n",
      "epoch 8  | loss: 0.21069 | val_0_auc: 0.9947  |  0:00:10s\n",
      "epoch 9  | loss: 0.18398 | val_0_auc: 0.99153 |  0:00:12s\n",
      "epoch 10 | loss: 0.1782  | val_0_auc: 0.99479 |  0:00:13s\n",
      "epoch 11 | loss: 0.15902 | val_0_auc: 0.9981  |  0:00:14s\n",
      "epoch 12 | loss: 0.16107 | val_0_auc: 0.99533 |  0:00:15s\n",
      "epoch 13 | loss: 0.15812 | val_0_auc: 0.99704 |  0:00:16s\n",
      "epoch 14 | loss: 0.16162 | val_0_auc: 0.99699 |  0:00:17s\n",
      "epoch 15 | loss: 0.15181 | val_0_auc: 0.99395 |  0:00:19s\n",
      "epoch 16 | loss: 0.14404 | val_0_auc: 0.9963  |  0:00:20s\n",
      "epoch 17 | loss: 0.12643 | val_0_auc: 0.99639 |  0:00:21s\n",
      "epoch 18 | loss: 0.15938 | val_0_auc: 0.99541 |  0:00:22s\n",
      "epoch 19 | loss: 0.14309 | val_0_auc: 0.99589 |  0:00:23s\n",
      "epoch 20 | loss: 0.13748 | val_0_auc: 0.99478 |  0:00:24s\n",
      "epoch 21 | loss: 0.142   | val_0_auc: 0.9979  |  0:00:26s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.9981\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99809898  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 29.2669\n",
      "Function value obtained: -0.9981\n",
      "Current minimum: -0.9997\n",
      "STARTED: syn2\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.27877248771316376, 'max_depth': 15, 'n_estimators': 850}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9971016  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.1184\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9971\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2827682137669927, 'max_depth': 6, 'n_estimators': 293}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99643161  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.0927\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9971\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3308768920798869, 'max_depth': 13, 'n_estimators': 622}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99727743  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.1040\n",
      "Function value obtained: -0.9973\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19424062920934906, 'max_depth': 10, 'n_estimators': 937}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99750694  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.1219\n",
      "Function value obtained: -0.9975\n",
      "Current minimum: -0.9975\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2941597143675297, 'max_depth': 14, 'n_estimators': 207}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99756431  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.1373\n",
      "Function value obtained: -0.9976\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3484888537812663, 'max_depth': 13, 'n_estimators': 108}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.997655  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.0938\n",
      "Function value obtained: -0.9977\n",
      "Current minimum: -0.9977\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10679480246309742, 'max_depth': 11, 'n_estimators': 526}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99778456  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.2097\n",
      "Function value obtained: -0.9978\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.34993916211340964, 'max_depth': 6, 'n_estimators': 828}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99678511  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.1074\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49177567266675815, 'max_depth': 4, 'n_estimators': 456}\n",
      "AUC:  0.996985  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.0656\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.35772323716939425, 'max_depth': 12, 'n_estimators': 370}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99750694  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.0898\n",
      "Function value obtained: -0.9975\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3246834928087887, 'max_depth': 10, 'n_estimators': 869}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99723301  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0859\n",
      "Function value obtained: -0.9972\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14521440664074914, 'max_depth': 5, 'n_estimators': 574}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99705533  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1528\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4513399365218822, 'max_depth': 14, 'n_estimators': 698}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99676661  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.0671\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4687170678841964, 'max_depth': 8, 'n_estimators': 425}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99695817  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0558\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2920184623297537, 'max_depth': 3, 'n_estimators': 750}\n",
      "AUC:  0.99713492  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0714\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4352723468461842, 'max_depth': 6, 'n_estimators': 769}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99695724  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.1086\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1719532452244295, 'max_depth': 6, 'n_estimators': 994}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99704978  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.1301\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.26241145193993143, 'max_depth': 13, 'n_estimators': 517}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99767536  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.1124\n",
      "Function value obtained: -0.9977\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.42327798277404916, 'max_depth': 5, 'n_estimators': 578}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9969739  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.1099\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.13170554967849377, 'max_depth': 4, 'n_estimators': 211}\n",
      "AUC:  0.99731075  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.1922\n",
      "Function value obtained: -0.9973\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.47821284117531515, 'max_depth': 8, 'n_estimators': 150}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99712567  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.0730\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4038400889715602, 'max_depth': 9, 'n_estimators': 756}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99681843  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.0970\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4762133156645052, 'max_depth': 12, 'n_estimators': 510}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99733111  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.0702\n",
      "Function value obtained: -0.9973\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3107595780073301, 'max_depth': 5, 'n_estimators': 155}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9964131  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0973\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2295484649288923, 'max_depth': 12, 'n_estimators': 249}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99776235  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.1049\n",
      "Function value obtained: -0.9978\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.267268012372163, 'max_depth': 3, 'n_estimators': 714}\n",
      "AUC:  0.99756061  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.0993\n",
      "Function value obtained: -0.9976\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1752247750294911, 'max_depth': 5, 'n_estimators': 330}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99682583  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.1463\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.44551552096549973, 'max_depth': 7, 'n_estimators': 803}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99689986  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.0807\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9978\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.23995731404370924, 'max_depth': 13, 'n_estimators': 483}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99795669  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.1232\n",
      "Function value obtained: -0.9980\n",
      "Current minimum: -0.9980\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.36998143379336634, 'max_depth': 5, 'n_estimators': 899}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99627614  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.9839\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9980\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29615234675507385, 'max_depth': 12, 'n_estimators': 386}\n",
      "[01:35:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99744031  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.7318\n",
      "Function value obtained: -0.9974\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4346170991254703, 'max_depth': 6, 'n_estimators': 313}\n",
      "[01:35:36] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9960818  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.2694\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7708603435994399, 'max_depth': 7, 'n_estimators': 575}\n",
      "[01:35:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99661669  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.5048\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.38702047922641447, 'max_depth': 15, 'n_estimators': 562}\n",
      "[01:35:37] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99678696  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.5065\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.47061712651646403, 'max_depth': 5, 'n_estimators': 946}\n",
      "[01:35:38] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99602813  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.2465\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7310848712063793, 'max_depth': 7, 'n_estimators': 304}\n",
      "[01:35:38] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99570423  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.4215\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.11088140417295016, 'max_depth': 12, 'n_estimators': 290}\n",
      "[01:35:38] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99686285  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 1.4221\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.44495695946682146, 'max_depth': 10, 'n_estimators': 466}\n",
      "[01:35:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99689246  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.5237\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9436839989381949, 'max_depth': 9, 'n_estimators': 444}\n",
      "[01:35:40] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99649453  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.4564\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7954541605310916, 'max_depth': 13, 'n_estimators': 624}\n",
      "[01:35:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99636868  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.3866\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5081168428319455, 'max_depth': 8, 'n_estimators': 793}\n",
      "[01:35:41] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99723486  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.3971\n",
      "Function value obtained: -0.9972\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.20380944211860708, 'max_depth': 5, 'n_estimators': 872}\n",
      "[01:35:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9964205  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.4077\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8235980697611055, 'max_depth': 12, 'n_estimators': 359}\n",
      "[01:35:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99572459  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.4215\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2555181981290401, 'max_depth': 4, 'n_estimators': 766}\n",
      "[01:35:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99644271  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.2357\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4178499379899073, 'max_depth': 4, 'n_estimators': 265}\n",
      "[01:35:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99680917  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.3267\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1185788094613059, 'max_depth': 13, 'n_estimators': 900}\n",
      "[01:35:43] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99691652  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 1.3601\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.781207774863944, 'max_depth': 3, 'n_estimators': 493}\n",
      "[01:35:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99677586  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.1500\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7495988703724431, 'max_depth': 4, 'n_estimators': 234}\n",
      "[01:35:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99683509  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.1831\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16596338653768117, 'max_depth': 6, 'n_estimators': 832}\n",
      "[01:35:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99638904  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.4698\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5768499294553737, 'max_depth': 4, 'n_estimators': 943}\n",
      "[01:35:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99705163  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.2413\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.26538369777802373, 'max_depth': 13, 'n_estimators': 896}\n",
      "[01:35:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99677031  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.5146\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.37244656920706354, 'max_depth': 8, 'n_estimators': 809}\n",
      "[01:35:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99695724  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.4463\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16003639547229845, 'max_depth': 10, 'n_estimators': 880}\n",
      "[01:35:46] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99687025  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.9895\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16399397313273678, 'max_depth': 8, 'n_estimators': 310}\n",
      "[01:35:47] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99743105  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.7106\n",
      "Function value obtained: -0.9974\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.39362527632568456, 'max_depth': 8, 'n_estimators': 765}\n",
      "[01:35:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99650749  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.3549\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8818534640033984, 'max_depth': 9, 'n_estimators': 529}\n",
      "[01:35:48] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99552285  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.2925\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49712669880467064, 'max_depth': 10, 'n_estimators': 692}\n",
      "[01:35:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99710716  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.3927\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8668966952251435, 'max_depth': 14, 'n_estimators': 952}\n",
      "[01:35:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99659448  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.3123\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2819030653520942, 'max_depth': 6, 'n_estimators': 319}\n",
      "[01:35:49] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99643346  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.2621\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6447409872836299, 'max_depth': 13, 'n_estimators': 937}\n",
      "[01:35:50] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99735702  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.9830\n",
      "Function value obtained: -0.9974\n",
      "Current minimum: -0.9974\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7599444282166643, 'lambda_sparse': 0.06280179682468862, 'n_steps': 5, 'n_a': 8, 'n_d': 8, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.91549 | val_0_auc: 0.64753 |  0:00:00s\n",
      "epoch 1  | loss: 0.67917 | val_0_auc: 0.68177 |  0:00:01s\n",
      "epoch 2  | loss: 0.62754 | val_0_auc: 0.76768 |  0:00:02s\n",
      "epoch 3  | loss: 0.60134 | val_0_auc: 0.76643 |  0:00:03s\n",
      "epoch 4  | loss: 0.57514 | val_0_auc: 0.78776 |  0:00:04s\n",
      "epoch 5  | loss: 0.55396 | val_0_auc: 0.80424 |  0:00:05s\n",
      "epoch 6  | loss: 0.53753 | val_0_auc: 0.81548 |  0:00:06s\n",
      "epoch 7  | loss: 0.51929 | val_0_auc: 0.81972 |  0:00:07s\n",
      "epoch 8  | loss: 0.50027 | val_0_auc: 0.84152 |  0:00:07s\n",
      "epoch 9  | loss: 0.48591 | val_0_auc: 0.83755 |  0:00:08s\n",
      "epoch 10 | loss: 0.47722 | val_0_auc: 0.85733 |  0:00:09s\n",
      "epoch 11 | loss: 0.45365 | val_0_auc: 0.87203 |  0:00:10s\n",
      "epoch 12 | loss: 0.45197 | val_0_auc: 0.871   |  0:00:11s\n",
      "epoch 13 | loss: 0.44936 | val_0_auc: 0.87139 |  0:00:12s\n",
      "epoch 14 | loss: 0.44304 | val_0_auc: 0.88885 |  0:00:13s\n",
      "epoch 15 | loss: 0.4227  | val_0_auc: 0.89333 |  0:00:14s\n",
      "epoch 16 | loss: 0.4008  | val_0_auc: 0.90181 |  0:00:14s\n",
      "epoch 17 | loss: 0.39845 | val_0_auc: 0.89216 |  0:00:15s\n",
      "epoch 18 | loss: 0.39005 | val_0_auc: 0.90041 |  0:00:16s\n",
      "epoch 19 | loss: 0.38132 | val_0_auc: 0.89284 |  0:00:17s\n",
      "epoch 20 | loss: 0.37268 | val_0_auc: 0.90842 |  0:00:18s\n",
      "epoch 21 | loss: 0.37677 | val_0_auc: 0.89313 |  0:00:19s\n",
      "epoch 22 | loss: 0.36982 | val_0_auc: 0.91491 |  0:00:20s\n",
      "epoch 23 | loss: 0.36321 | val_0_auc: 0.9216  |  0:00:21s\n",
      "epoch 24 | loss: 0.35881 | val_0_auc: 0.90728 |  0:00:21s\n",
      "epoch 25 | loss: 0.36343 | val_0_auc: 0.91411 |  0:00:22s\n",
      "epoch 26 | loss: 0.35962 | val_0_auc: 0.9077  |  0:00:23s\n",
      "epoch 27 | loss: 0.35955 | val_0_auc: 0.9248  |  0:00:24s\n",
      "epoch 28 | loss: 0.3468  | val_0_auc: 0.91904 |  0:00:25s\n",
      "epoch 29 | loss: 0.33066 | val_0_auc: 0.94339 |  0:00:26s\n",
      "epoch 30 | loss: 0.32924 | val_0_auc: 0.94082 |  0:00:27s\n",
      "epoch 31 | loss: 0.32567 | val_0_auc: 0.95362 |  0:00:28s\n",
      "epoch 32 | loss: 0.31837 | val_0_auc: 0.95064 |  0:00:29s\n",
      "epoch 33 | loss: 0.30404 | val_0_auc: 0.95594 |  0:00:30s\n",
      "epoch 34 | loss: 0.29121 | val_0_auc: 0.95618 |  0:00:30s\n",
      "epoch 35 | loss: 0.30055 | val_0_auc: 0.9524  |  0:00:31s\n",
      "epoch 36 | loss: 0.29471 | val_0_auc: 0.95762 |  0:00:32s\n",
      "epoch 37 | loss: 0.29056 | val_0_auc: 0.95982 |  0:00:33s\n",
      "epoch 38 | loss: 0.28304 | val_0_auc: 0.9685  |  0:00:34s\n",
      "epoch 39 | loss: 0.27402 | val_0_auc: 0.96773 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.9685\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96849892  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 35.6345\n",
      "Function value obtained: -0.9685\n",
      "Current minimum: -0.9685\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.976817157154673, 'lambda_sparse': 0.05193339794828156, 'n_steps': 9, 'n_a': 32, 'n_d': 32, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.04935 | val_0_auc: 0.75596 |  0:00:02s\n",
      "epoch 1  | loss: 0.64037 | val_0_auc: 0.81319 |  0:00:04s\n",
      "epoch 2  | loss: 0.56363 | val_0_auc: 0.83785 |  0:00:06s\n",
      "epoch 3  | loss: 0.49652 | val_0_auc: 0.87074 |  0:00:08s\n",
      "epoch 4  | loss: 0.48479 | val_0_auc: 0.86175 |  0:00:10s\n",
      "epoch 5  | loss: 0.57761 | val_0_auc: 0.85449 |  0:00:12s\n",
      "epoch 6  | loss: 0.52252 | val_0_auc: 0.86027 |  0:00:14s\n",
      "epoch 7  | loss: 0.51063 | val_0_auc: 0.8611  |  0:00:16s\n",
      "epoch 8  | loss: 0.48012 | val_0_auc: 0.86074 |  0:00:19s\n",
      "epoch 9  | loss: 0.51122 | val_0_auc: 0.86344 |  0:00:21s\n",
      "epoch 10 | loss: 0.54856 | val_0_auc: 0.86037 |  0:00:23s\n",
      "epoch 11 | loss: 0.71981 | val_0_auc: 0.84512 |  0:00:25s\n",
      "epoch 12 | loss: 0.64343 | val_0_auc: 0.86218 |  0:00:27s\n",
      "epoch 13 | loss: 0.50002 | val_0_auc: 0.87067 |  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.87074\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.87073824  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 30.4802\n",
      "Function value obtained: -0.8707\n",
      "Current minimum: -0.9685\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5829215164147765, 'lambda_sparse': 0.013789287040019398, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.37005 | val_0_auc: 0.69983 |  0:00:01s\n",
      "epoch 1  | loss: 0.64015 | val_0_auc: 0.77518 |  0:00:03s\n",
      "epoch 2  | loss: 0.56855 | val_0_auc: 0.77052 |  0:00:05s\n",
      "epoch 3  | loss: 0.51601 | val_0_auc: 0.80966 |  0:00:06s\n",
      "epoch 4  | loss: 0.50213 | val_0_auc: 0.84865 |  0:00:08s\n",
      "epoch 5  | loss: 0.48597 | val_0_auc: 0.85419 |  0:00:10s\n",
      "epoch 6  | loss: 0.4633  | val_0_auc: 0.83843 |  0:00:12s\n",
      "epoch 7  | loss: 0.46497 | val_0_auc: 0.83807 |  0:00:13s\n",
      "epoch 8  | loss: 0.47214 | val_0_auc: 0.85571 |  0:00:15s\n",
      "epoch 9  | loss: 0.47413 | val_0_auc: 0.84941 |  0:00:17s\n",
      "epoch 10 | loss: 0.49009 | val_0_auc: 0.86351 |  0:00:18s\n",
      "epoch 11 | loss: 0.52487 | val_0_auc: 0.87437 |  0:00:20s\n",
      "epoch 12 | loss: 0.46969 | val_0_auc: 0.86737 |  0:00:22s\n",
      "epoch 13 | loss: 0.45212 | val_0_auc: 0.87905 |  0:00:24s\n",
      "epoch 14 | loss: 0.42833 | val_0_auc: 0.86787 |  0:00:25s\n",
      "epoch 15 | loss: 0.42778 | val_0_auc: 0.86882 |  0:00:27s\n",
      "epoch 16 | loss: 0.42586 | val_0_auc: 0.8596  |  0:00:29s\n",
      "epoch 17 | loss: 0.42589 | val_0_auc: 0.88347 |  0:00:31s\n",
      "epoch 18 | loss: 0.39877 | val_0_auc: 0.88425 |  0:00:32s\n",
      "epoch 19 | loss: 0.37747 | val_0_auc: 0.90053 |  0:00:34s\n",
      "epoch 20 | loss: 0.37123 | val_0_auc: 0.90707 |  0:00:36s\n",
      "epoch 21 | loss: 0.36675 | val_0_auc: 0.91845 |  0:00:38s\n",
      "epoch 22 | loss: 0.35546 | val_0_auc: 0.92776 |  0:00:39s\n",
      "epoch 23 | loss: 0.34495 | val_0_auc: 0.93328 |  0:00:41s\n",
      "epoch 24 | loss: 0.32374 | val_0_auc: 0.93291 |  0:00:43s\n",
      "epoch 25 | loss: 0.31215 | val_0_auc: 0.93693 |  0:00:44s\n",
      "epoch 26 | loss: 0.31131 | val_0_auc: 0.94365 |  0:00:46s\n",
      "epoch 27 | loss: 0.30133 | val_0_auc: 0.94873 |  0:00:48s\n",
      "epoch 28 | loss: 0.30232 | val_0_auc: 0.94627 |  0:00:50s\n",
      "epoch 29 | loss: 0.28372 | val_0_auc: 0.9583  |  0:00:51s\n",
      "epoch 30 | loss: 0.28138 | val_0_auc: 0.95411 |  0:00:53s\n",
      "epoch 31 | loss: 0.27872 | val_0_auc: 0.95904 |  0:00:55s\n",
      "epoch 32 | loss: 0.25847 | val_0_auc: 0.96199 |  0:00:57s\n",
      "epoch 33 | loss: 0.25803 | val_0_auc: 0.96375 |  0:00:58s\n",
      "epoch 34 | loss: 0.26476 | val_0_auc: 0.97173 |  0:01:00s\n",
      "epoch 35 | loss: 0.25389 | val_0_auc: 0.95599 |  0:01:02s\n",
      "epoch 36 | loss: 0.25218 | val_0_auc: 0.96958 |  0:01:04s\n",
      "epoch 37 | loss: 0.25035 | val_0_auc: 0.96932 |  0:01:05s\n",
      "epoch 38 | loss: 0.23495 | val_0_auc: 0.97236 |  0:01:07s\n",
      "epoch 39 | loss: 0.21816 | val_0_auc: 0.96815 |  0:01:09s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.97236\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9723616  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 69.8983\n",
      "Function value obtained: -0.9724\n",
      "Current minimum: -0.9724\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7595410148681516, 'lambda_sparse': 0.09329917157674439, 'n_steps': 4, 'n_a': 8, 'n_d': 8, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.84743 | val_0_auc: 0.65964 |  0:00:00s\n",
      "epoch 1  | loss: 0.67338 | val_0_auc: 0.74623 |  0:00:01s\n",
      "epoch 2  | loss: 0.59819 | val_0_auc: 0.80152 |  0:00:02s\n",
      "epoch 3  | loss: 0.54769 | val_0_auc: 0.8448  |  0:00:03s\n",
      "epoch 4  | loss: 0.50683 | val_0_auc: 0.84311 |  0:00:03s\n",
      "epoch 5  | loss: 0.4857  | val_0_auc: 0.84927 |  0:00:04s\n",
      "epoch 6  | loss: 0.48488 | val_0_auc: 0.85929 |  0:00:05s\n",
      "epoch 7  | loss: 0.48077 | val_0_auc: 0.85685 |  0:00:06s\n",
      "epoch 8  | loss: 0.47667 | val_0_auc: 0.87239 |  0:00:06s\n",
      "epoch 9  | loss: 0.47423 | val_0_auc: 0.85884 |  0:00:07s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 | loss: 0.45681 | val_0_auc: 0.87444 |  0:00:08s\n",
      "epoch 11 | loss: 0.44378 | val_0_auc: 0.87052 |  0:00:09s\n",
      "epoch 12 | loss: 0.41905 | val_0_auc: 0.88336 |  0:00:09s\n",
      "epoch 13 | loss: 0.40888 | val_0_auc: 0.89353 |  0:00:10s\n",
      "epoch 14 | loss: 0.39722 | val_0_auc: 0.89167 |  0:00:11s\n",
      "epoch 15 | loss: 0.40312 | val_0_auc: 0.88265 |  0:00:12s\n",
      "epoch 16 | loss: 0.40592 | val_0_auc: 0.89212 |  0:00:12s\n",
      "epoch 17 | loss: 0.3982  | val_0_auc: 0.91178 |  0:00:13s\n",
      "epoch 18 | loss: 0.38045 | val_0_auc: 0.90484 |  0:00:14s\n",
      "epoch 19 | loss: 0.37107 | val_0_auc: 0.90762 |  0:00:15s\n",
      "epoch 20 | loss: 0.37372 | val_0_auc: 0.90351 |  0:00:15s\n",
      "epoch 21 | loss: 0.37322 | val_0_auc: 0.90314 |  0:00:16s\n",
      "epoch 22 | loss: 0.36924 | val_0_auc: 0.90984 |  0:00:17s\n",
      "epoch 23 | loss: 0.35944 | val_0_auc: 0.9125  |  0:00:18s\n",
      "epoch 24 | loss: 0.35202 | val_0_auc: 0.9288  |  0:00:18s\n",
      "epoch 25 | loss: 0.333   | val_0_auc: 0.92811 |  0:00:19s\n",
      "epoch 26 | loss: 0.33105 | val_0_auc: 0.92531 |  0:00:20s\n",
      "epoch 27 | loss: 0.31503 | val_0_auc: 0.9298  |  0:00:21s\n",
      "epoch 28 | loss: 0.31777 | val_0_auc: 0.93378 |  0:00:21s\n",
      "epoch 29 | loss: 0.32412 | val_0_auc: 0.93807 |  0:00:22s\n",
      "epoch 30 | loss: 0.30492 | val_0_auc: 0.95037 |  0:00:23s\n",
      "epoch 31 | loss: 0.30367 | val_0_auc: 0.94252 |  0:00:24s\n",
      "epoch 32 | loss: 0.29586 | val_0_auc: 0.93843 |  0:00:24s\n",
      "epoch 33 | loss: 0.29066 | val_0_auc: 0.93686 |  0:00:25s\n",
      "epoch 34 | loss: 0.30342 | val_0_auc: 0.9478  |  0:00:26s\n",
      "epoch 35 | loss: 0.28584 | val_0_auc: 0.9476  |  0:00:27s\n",
      "epoch 36 | loss: 0.28914 | val_0_auc: 0.95494 |  0:00:27s\n",
      "epoch 37 | loss: 0.27485 | val_0_auc: 0.95401 |  0:00:28s\n",
      "epoch 38 | loss: 0.26418 | val_0_auc: 0.96343 |  0:00:29s\n",
      "epoch 39 | loss: 0.27285 | val_0_auc: 0.96851 |  0:00:30s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.96851\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96850818  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 30.4543\n",
      "Function value obtained: -0.9685\n",
      "Current minimum: -0.9724\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8076854356358985, 'lambda_sparse': 0.09507559975493053, 'n_steps': 8, 'n_a': 8, 'n_d': 8, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.83209 | val_0_auc: 0.67168 |  0:00:01s\n",
      "epoch 1  | loss: 0.6488  | val_0_auc: 0.74809 |  0:00:02s\n",
      "epoch 2  | loss: 0.61596 | val_0_auc: 0.76808 |  0:00:03s\n",
      "epoch 3  | loss: 0.59435 | val_0_auc: 0.77611 |  0:00:05s\n",
      "epoch 4  | loss: 0.57177 | val_0_auc: 0.79426 |  0:00:06s\n",
      "epoch 5  | loss: 0.54446 | val_0_auc: 0.8401  |  0:00:07s\n",
      "epoch 6  | loss: 0.52983 | val_0_auc: 0.83401 |  0:00:09s\n",
      "epoch 7  | loss: 0.50345 | val_0_auc: 0.83746 |  0:00:10s\n",
      "epoch 8  | loss: 0.50522 | val_0_auc: 0.86    |  0:00:11s\n",
      "epoch 9  | loss: 0.48222 | val_0_auc: 0.84825 |  0:00:13s\n",
      "epoch 10 | loss: 0.48169 | val_0_auc: 0.85342 |  0:00:14s\n",
      "epoch 11 | loss: 0.47435 | val_0_auc: 0.8676  |  0:00:15s\n",
      "epoch 12 | loss: 0.478   | val_0_auc: 0.86571 |  0:00:17s\n",
      "epoch 13 | loss: 0.47082 | val_0_auc: 0.85774 |  0:00:18s\n",
      "epoch 14 | loss: 0.4647  | val_0_auc: 0.87794 |  0:00:19s\n",
      "epoch 15 | loss: 0.43414 | val_0_auc: 0.91188 |  0:00:20s\n",
      "epoch 16 | loss: 0.41289 | val_0_auc: 0.90858 |  0:00:22s\n",
      "epoch 17 | loss: 0.41942 | val_0_auc: 0.88023 |  0:00:23s\n",
      "epoch 18 | loss: 0.4086  | val_0_auc: 0.89755 |  0:00:24s\n",
      "epoch 19 | loss: 0.38466 | val_0_auc: 0.91485 |  0:00:26s\n",
      "epoch 20 | loss: 0.38794 | val_0_auc: 0.93059 |  0:00:27s\n",
      "epoch 21 | loss: 0.36499 | val_0_auc: 0.91464 |  0:00:28s\n",
      "epoch 22 | loss: 0.35247 | val_0_auc: 0.92961 |  0:00:29s\n",
      "epoch 23 | loss: 0.38473 | val_0_auc: 0.93096 |  0:00:31s\n",
      "epoch 24 | loss: 0.37086 | val_0_auc: 0.92994 |  0:00:32s\n",
      "epoch 25 | loss: 0.35663 | val_0_auc: 0.91904 |  0:00:33s\n",
      "epoch 26 | loss: 0.338   | val_0_auc: 0.93934 |  0:00:35s\n",
      "epoch 27 | loss: 0.33517 | val_0_auc: 0.92819 |  0:00:36s\n",
      "epoch 28 | loss: 0.34167 | val_0_auc: 0.91385 |  0:00:37s\n",
      "epoch 29 | loss: 0.34924 | val_0_auc: 0.94579 |  0:00:38s\n",
      "epoch 30 | loss: 0.34615 | val_0_auc: 0.92812 |  0:00:40s\n",
      "epoch 31 | loss: 0.33407 | val_0_auc: 0.92994 |  0:00:41s\n",
      "epoch 32 | loss: 0.33739 | val_0_auc: 0.92343 |  0:00:42s\n",
      "epoch 33 | loss: 0.35232 | val_0_auc: 0.92884 |  0:00:44s\n",
      "epoch 34 | loss: 0.33903 | val_0_auc: 0.9408  |  0:00:45s\n",
      "epoch 35 | loss: 0.32379 | val_0_auc: 0.94991 |  0:00:46s\n",
      "epoch 36 | loss: 0.31805 | val_0_auc: 0.92764 |  0:00:47s\n",
      "epoch 37 | loss: 0.31982 | val_0_auc: 0.9476  |  0:00:49s\n",
      "epoch 38 | loss: 0.3591  | val_0_auc: 0.93933 |  0:00:50s\n",
      "epoch 39 | loss: 0.37204 | val_0_auc: 0.949   |  0:00:51s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.94991\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.94991292  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 52.1083\n",
      "Function value obtained: -0.9499\n",
      "Current minimum: -0.9724\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7487362241369455, 'lambda_sparse': 0.08687738595880352, 'n_steps': 8, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.83521 | val_0_auc: 0.75198 |  0:00:02s\n",
      "epoch 1  | loss: 1.06102 | val_0_auc: 0.77447 |  0:00:05s\n",
      "epoch 2  | loss: 1.06227 | val_0_auc: 0.79501 |  0:00:07s\n",
      "epoch 3  | loss: 2.27036 | val_0_auc: 0.81153 |  0:00:10s\n",
      "epoch 4  | loss: 1.31658 | val_0_auc: 0.82864 |  0:00:13s\n",
      "epoch 5  | loss: 1.0274  | val_0_auc: 0.81879 |  0:00:15s\n",
      "epoch 6  | loss: 0.65946 | val_0_auc: 0.85127 |  0:00:18s\n",
      "epoch 7  | loss: 0.52551 | val_0_auc: 0.85828 |  0:00:20s\n",
      "epoch 8  | loss: 0.52322 | val_0_auc: 0.82313 |  0:00:23s\n",
      "epoch 9  | loss: 0.53047 | val_0_auc: 0.86318 |  0:00:26s\n",
      "epoch 10 | loss: 0.58061 | val_0_auc: 0.89847 |  0:00:28s\n",
      "epoch 11 | loss: 0.49774 | val_0_auc: 0.89724 |  0:00:31s\n",
      "epoch 12 | loss: 0.43694 | val_0_auc: 0.91196 |  0:00:33s\n",
      "epoch 13 | loss: 0.47158 | val_0_auc: 0.91867 |  0:00:36s\n",
      "epoch 14 | loss: 0.38011 | val_0_auc: 0.90183 |  0:00:39s\n",
      "epoch 15 | loss: 0.38062 | val_0_auc: 0.90674 |  0:00:41s\n",
      "epoch 16 | loss: 0.426   | val_0_auc: 0.91083 |  0:00:44s\n",
      "epoch 17 | loss: 0.38993 | val_0_auc: 0.91672 |  0:00:46s\n",
      "epoch 18 | loss: 0.37992 | val_0_auc: 0.93199 |  0:00:49s\n",
      "epoch 19 | loss: 0.36864 | val_0_auc: 0.92388 |  0:00:51s\n",
      "epoch 20 | loss: 0.37021 | val_0_auc: 0.93873 |  0:00:54s\n",
      "epoch 21 | loss: 0.34769 | val_0_auc: 0.93477 |  0:00:56s\n",
      "epoch 22 | loss: 0.34465 | val_0_auc: 0.91642 |  0:00:59s\n",
      "epoch 23 | loss: 0.34613 | val_0_auc: 0.9398  |  0:01:01s\n",
      "epoch 24 | loss: 0.34842 | val_0_auc: 0.93445 |  0:01:04s\n",
      "epoch 25 | loss: 0.3383  | val_0_auc: 0.94174 |  0:01:07s\n",
      "epoch 26 | loss: 0.3225  | val_0_auc: 0.9297  |  0:01:09s\n",
      "epoch 27 | loss: 0.31738 | val_0_auc: 0.94185 |  0:01:12s\n",
      "epoch 28 | loss: 0.3199  | val_0_auc: 0.94085 |  0:01:14s\n",
      "epoch 29 | loss: 0.3489  | val_0_auc: 0.95684 |  0:01:17s\n",
      "epoch 30 | loss: 0.33289 | val_0_auc: 0.94345 |  0:01:20s\n",
      "epoch 31 | loss: 0.30893 | val_0_auc: 0.95152 |  0:01:23s\n",
      "epoch 32 | loss: 0.29814 | val_0_auc: 0.95692 |  0:01:26s\n",
      "epoch 33 | loss: 0.27681 | val_0_auc: 0.96299 |  0:01:29s\n",
      "epoch 34 | loss: 0.26165 | val_0_auc: 0.96614 |  0:01:31s\n",
      "epoch 35 | loss: 0.25811 | val_0_auc: 0.96625 |  0:01:34s\n",
      "epoch 36 | loss: 0.267   | val_0_auc: 0.95919 |  0:01:37s\n",
      "epoch 37 | loss: 0.27626 | val_0_auc: 0.96407 |  0:01:39s\n",
      "epoch 38 | loss: 0.26839 | val_0_auc: 0.96504 |  0:01:42s\n",
      "epoch 39 | loss: 0.25978 | val_0_auc: 0.97555 |  0:01:45s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.97555\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97554502  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 106.1860\n",
      "Function value obtained: -0.9755\n",
      "Current minimum: -0.9755\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7499580676212965, 'lambda_sparse': 0.01102287085666087, 'n_steps': 9, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.58313 | val_0_auc: 0.80701 |  0:00:02s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  | loss: 0.9671  | val_0_auc: 0.76707 |  0:00:05s\n",
      "epoch 2  | loss: 0.84629 | val_0_auc: 0.80201 |  0:00:08s\n",
      "epoch 3  | loss: 0.67922 | val_0_auc: 0.82755 |  0:00:11s\n",
      "epoch 4  | loss: 0.72094 | val_0_auc: 0.81576 |  0:00:14s\n",
      "epoch 5  | loss: 0.71686 | val_0_auc: 0.82699 |  0:00:17s\n",
      "epoch 6  | loss: 0.57188 | val_0_auc: 0.80253 |  0:00:19s\n",
      "epoch 7  | loss: 1.1668  | val_0_auc: 0.79749 |  0:00:22s\n",
      "epoch 8  | loss: 1.00106 | val_0_auc: 0.84344 |  0:00:25s\n",
      "epoch 9  | loss: 0.89731 | val_0_auc: 0.86335 |  0:00:28s\n",
      "epoch 10 | loss: 0.66909 | val_0_auc: 0.84499 |  0:00:31s\n",
      "epoch 11 | loss: 0.723   | val_0_auc: 0.84026 |  0:00:33s\n",
      "epoch 12 | loss: 0.54643 | val_0_auc: 0.87597 |  0:00:36s\n",
      "epoch 13 | loss: 0.69941 | val_0_auc: 0.8765  |  0:00:39s\n",
      "epoch 14 | loss: 0.45584 | val_0_auc: 0.89302 |  0:00:42s\n",
      "epoch 15 | loss: 0.43514 | val_0_auc: 0.87841 |  0:00:45s\n",
      "epoch 16 | loss: 0.4056  | val_0_auc: 0.90358 |  0:00:48s\n",
      "epoch 17 | loss: 0.3731  | val_0_auc: 0.92584 |  0:00:51s\n",
      "epoch 18 | loss: 0.42644 | val_0_auc: 0.93049 |  0:00:53s\n",
      "epoch 19 | loss: 0.47755 | val_0_auc: 0.91835 |  0:00:56s\n",
      "epoch 20 | loss: 0.49454 | val_0_auc: 0.91901 |  0:00:59s\n",
      "epoch 21 | loss: 0.53968 | val_0_auc: 0.93121 |  0:01:02s\n",
      "epoch 22 | loss: 0.38199 | val_0_auc: 0.93866 |  0:01:05s\n",
      "epoch 23 | loss: 0.39713 | val_0_auc: 0.92815 |  0:01:08s\n",
      "epoch 24 | loss: 0.39492 | val_0_auc: 0.91197 |  0:01:10s\n",
      "epoch 25 | loss: 0.38841 | val_0_auc: 0.90983 |  0:01:13s\n",
      "epoch 26 | loss: 0.3499  | val_0_auc: 0.93118 |  0:01:16s\n",
      "epoch 27 | loss: 0.34992 | val_0_auc: 0.94595 |  0:01:19s\n",
      "epoch 28 | loss: 0.32196 | val_0_auc: 0.94071 |  0:01:22s\n",
      "epoch 29 | loss: 0.34288 | val_0_auc: 0.94123 |  0:01:25s\n",
      "epoch 30 | loss: 0.33417 | val_0_auc: 0.92438 |  0:01:27s\n",
      "epoch 31 | loss: 0.32381 | val_0_auc: 0.94757 |  0:01:30s\n",
      "epoch 32 | loss: 0.31282 | val_0_auc: 0.94279 |  0:01:33s\n",
      "epoch 33 | loss: 0.36168 | val_0_auc: 0.92518 |  0:01:36s\n",
      "epoch 34 | loss: 0.33613 | val_0_auc: 0.93587 |  0:01:39s\n",
      "epoch 35 | loss: 0.33841 | val_0_auc: 0.93748 |  0:01:41s\n",
      "epoch 36 | loss: 0.3329  | val_0_auc: 0.92974 |  0:01:44s\n",
      "epoch 37 | loss: 0.33022 | val_0_auc: 0.93385 |  0:01:47s\n",
      "epoch 38 | loss: 0.33037 | val_0_auc: 0.9341  |  0:01:50s\n",
      "epoch 39 | loss: 0.32242 | val_0_auc: 0.93259 |  0:01:53s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 31 and best_val_0_auc = 0.94757\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.94757162  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 114.1457\n",
      "Function value obtained: -0.9476\n",
      "Current minimum: -0.9755\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1035478159135887, 'lambda_sparse': 0.0024158448318291788, 'n_steps': 8, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.05024 | val_0_auc: 0.75794 |  0:00:01s\n",
      "epoch 1  | loss: 0.7166  | val_0_auc: 0.80675 |  0:00:03s\n",
      "epoch 2  | loss: 0.65862 | val_0_auc: 0.86343 |  0:00:05s\n",
      "epoch 3  | loss: 0.49403 | val_0_auc: 0.87802 |  0:00:07s\n",
      "epoch 4  | loss: 0.42987 | val_0_auc: 0.90178 |  0:00:09s\n",
      "epoch 5  | loss: 0.38687 | val_0_auc: 0.89933 |  0:00:11s\n",
      "epoch 6  | loss: 0.36295 | val_0_auc: 0.92192 |  0:00:13s\n",
      "epoch 7  | loss: 0.34095 | val_0_auc: 0.91768 |  0:00:15s\n",
      "epoch 8  | loss: 0.32952 | val_0_auc: 0.94282 |  0:00:17s\n",
      "epoch 9  | loss: 0.29476 | val_0_auc: 0.94388 |  0:00:19s\n",
      "epoch 10 | loss: 0.31119 | val_0_auc: 0.95467 |  0:00:21s\n",
      "epoch 11 | loss: 0.25436 | val_0_auc: 0.97144 |  0:00:23s\n",
      "epoch 12 | loss: 0.23697 | val_0_auc: 0.97124 |  0:00:25s\n",
      "epoch 13 | loss: 0.22671 | val_0_auc: 0.96483 |  0:00:27s\n",
      "epoch 14 | loss: 0.23364 | val_0_auc: 0.97592 |  0:00:29s\n",
      "epoch 15 | loss: 0.22793 | val_0_auc: 0.97496 |  0:00:30s\n",
      "epoch 16 | loss: 0.23075 | val_0_auc: 0.97889 |  0:00:32s\n",
      "epoch 17 | loss: 0.22528 | val_0_auc: 0.97727 |  0:00:34s\n",
      "epoch 18 | loss: 0.21735 | val_0_auc: 0.97969 |  0:00:36s\n",
      "epoch 19 | loss: 0.20377 | val_0_auc: 0.97596 |  0:00:38s\n",
      "epoch 20 | loss: 0.19534 | val_0_auc: 0.97528 |  0:00:40s\n",
      "epoch 21 | loss: 0.21151 | val_0_auc: 0.98353 |  0:00:42s\n",
      "epoch 22 | loss: 0.18747 | val_0_auc: 0.98016 |  0:00:44s\n",
      "epoch 23 | loss: 0.18772 | val_0_auc: 0.97587 |  0:00:46s\n",
      "epoch 24 | loss: 0.20766 | val_0_auc: 0.97906 |  0:00:48s\n",
      "epoch 25 | loss: 0.17417 | val_0_auc: 0.97792 |  0:00:50s\n",
      "epoch 26 | loss: 0.18254 | val_0_auc: 0.98581 |  0:00:52s\n",
      "epoch 27 | loss: 0.17578 | val_0_auc: 0.98521 |  0:00:54s\n",
      "epoch 28 | loss: 0.16643 | val_0_auc: 0.98225 |  0:00:55s\n",
      "epoch 29 | loss: 0.16441 | val_0_auc: 0.98797 |  0:00:57s\n",
      "epoch 30 | loss: 0.15659 | val_0_auc: 0.98788 |  0:00:59s\n",
      "epoch 31 | loss: 0.15026 | val_0_auc: 0.98667 |  0:01:01s\n",
      "epoch 32 | loss: 0.15389 | val_0_auc: 0.9881  |  0:01:03s\n",
      "epoch 33 | loss: 0.16863 | val_0_auc: 0.98735 |  0:01:05s\n",
      "epoch 34 | loss: 0.15181 | val_0_auc: 0.981   |  0:01:07s\n",
      "epoch 35 | loss: 0.15645 | val_0_auc: 0.9883  |  0:01:09s\n",
      "epoch 36 | loss: 0.15531 | val_0_auc: 0.98338 |  0:01:11s\n",
      "epoch 37 | loss: 0.15937 | val_0_auc: 0.98974 |  0:01:13s\n",
      "epoch 38 | loss: 0.14318 | val_0_auc: 0.98958 |  0:01:15s\n",
      "epoch 39 | loss: 0.14274 | val_0_auc: 0.98929 |  0:01:17s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.98974\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98974457  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 77.7238\n",
      "Function value obtained: -0.9897\n",
      "Current minimum: -0.9897\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0001242790396596, 'lambda_sparse': 0.09801205977162133, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.97059 | val_0_auc: 0.80033 |  0:00:01s\n",
      "epoch 1  | loss: 0.60088 | val_0_auc: 0.82847 |  0:00:02s\n",
      "epoch 2  | loss: 0.51771 | val_0_auc: 0.87649 |  0:00:03s\n",
      "epoch 3  | loss: 0.44588 | val_0_auc: 0.91316 |  0:00:05s\n",
      "epoch 4  | loss: 0.39581 | val_0_auc: 0.92741 |  0:00:06s\n",
      "epoch 5  | loss: 0.36202 | val_0_auc: 0.94038 |  0:00:07s\n",
      "epoch 6  | loss: 0.32987 | val_0_auc: 0.95838 |  0:00:09s\n",
      "epoch 7  | loss: 0.31823 | val_0_auc: 0.95424 |  0:00:10s\n",
      "epoch 8  | loss: 0.28937 | val_0_auc: 0.97987 |  0:00:11s\n",
      "epoch 9  | loss: 0.27086 | val_0_auc: 0.97842 |  0:00:13s\n",
      "epoch 10 | loss: 0.25376 | val_0_auc: 0.9821  |  0:00:14s\n",
      "epoch 11 | loss: 0.2496  | val_0_auc: 0.97826 |  0:00:15s\n",
      "epoch 12 | loss: 0.25226 | val_0_auc: 0.98354 |  0:00:16s\n",
      "epoch 13 | loss: 0.26821 | val_0_auc: 0.98745 |  0:00:18s\n",
      "epoch 14 | loss: 0.25252 | val_0_auc: 0.98036 |  0:00:19s\n",
      "epoch 15 | loss: 0.25795 | val_0_auc: 0.99014 |  0:00:20s\n",
      "epoch 16 | loss: 0.24463 | val_0_auc: 0.98112 |  0:00:22s\n",
      "epoch 17 | loss: 0.23028 | val_0_auc: 0.98631 |  0:00:23s\n",
      "epoch 18 | loss: 0.22394 | val_0_auc: 0.98596 |  0:00:24s\n",
      "epoch 19 | loss: 0.22588 | val_0_auc: 0.98927 |  0:00:26s\n",
      "epoch 20 | loss: 0.22907 | val_0_auc: 0.98431 |  0:00:27s\n",
      "epoch 21 | loss: 0.22646 | val_0_auc: 0.98818 |  0:00:28s\n",
      "epoch 22 | loss: 0.22527 | val_0_auc: 0.98889 |  0:00:30s\n",
      "epoch 23 | loss: 0.22651 | val_0_auc: 0.98011 |  0:00:31s\n",
      "epoch 24 | loss: 0.2168  | val_0_auc: 0.98649 |  0:00:32s\n",
      "epoch 25 | loss: 0.22383 | val_0_auc: 0.98649 |  0:00:33s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.99014\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99013879  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 34.4065\n",
      "Function value obtained: -0.9901\n",
      "Current minimum: -0.9901\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4012962781025435, 'lambda_sparse': 0.06356251893600791, 'n_steps': 5, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.80091 | val_0_auc: 0.70715 |  0:00:01s\n",
      "epoch 1  | loss: 0.57784 | val_0_auc: 0.81104 |  0:00:02s\n",
      "epoch 2  | loss: 0.52571 | val_0_auc: 0.84063 |  0:00:03s\n",
      "epoch 3  | loss: 0.48684 | val_0_auc: 0.84369 |  0:00:04s\n",
      "epoch 4  | loss: 0.45816 | val_0_auc: 0.85892 |  0:00:05s\n",
      "epoch 5  | loss: 0.43301 | val_0_auc: 0.87266 |  0:00:06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 0.41496 | val_0_auc: 0.87869 |  0:00:07s\n",
      "epoch 7  | loss: 0.42009 | val_0_auc: 0.89996 |  0:00:08s\n",
      "epoch 8  | loss: 0.3892  | val_0_auc: 0.91118 |  0:00:09s\n",
      "epoch 9  | loss: 0.37701 | val_0_auc: 0.90928 |  0:00:10s\n",
      "epoch 10 | loss: 0.3785  | val_0_auc: 0.91989 |  0:00:11s\n",
      "epoch 11 | loss: 0.37046 | val_0_auc: 0.92852 |  0:00:12s\n",
      "epoch 12 | loss: 0.35853 | val_0_auc: 0.94028 |  0:00:13s\n",
      "epoch 13 | loss: 0.36636 | val_0_auc: 0.92351 |  0:00:15s\n",
      "epoch 14 | loss: 0.34759 | val_0_auc: 0.92551 |  0:00:16s\n",
      "epoch 15 | loss: 0.34628 | val_0_auc: 0.93141 |  0:00:17s\n",
      "epoch 16 | loss: 0.33961 | val_0_auc: 0.94135 |  0:00:18s\n",
      "epoch 17 | loss: 0.33348 | val_0_auc: 0.92503 |  0:00:19s\n",
      "epoch 18 | loss: 0.31059 | val_0_auc: 0.94123 |  0:00:20s\n",
      "epoch 19 | loss: 0.30577 | val_0_auc: 0.95033 |  0:00:21s\n",
      "epoch 20 | loss: 0.2897  | val_0_auc: 0.95476 |  0:00:22s\n",
      "epoch 21 | loss: 0.28894 | val_0_auc: 0.95756 |  0:00:23s\n",
      "epoch 22 | loss: 0.265   | val_0_auc: 0.97157 |  0:00:24s\n",
      "epoch 23 | loss: 0.25072 | val_0_auc: 0.97302 |  0:00:25s\n",
      "epoch 24 | loss: 0.24129 | val_0_auc: 0.9748  |  0:00:26s\n",
      "epoch 25 | loss: 0.24327 | val_0_auc: 0.97014 |  0:00:27s\n",
      "epoch 26 | loss: 0.22628 | val_0_auc: 0.96817 |  0:00:28s\n",
      "epoch 27 | loss: 0.21506 | val_0_auc: 0.97066 |  0:00:29s\n",
      "epoch 28 | loss: 0.20965 | val_0_auc: 0.9782  |  0:00:30s\n",
      "epoch 29 | loss: 0.22407 | val_0_auc: 0.98277 |  0:00:31s\n",
      "epoch 30 | loss: 0.21789 | val_0_auc: 0.97421 |  0:00:33s\n",
      "epoch 31 | loss: 0.21151 | val_0_auc: 0.98238 |  0:00:34s\n",
      "epoch 32 | loss: 0.20712 | val_0_auc: 0.97902 |  0:00:35s\n",
      "epoch 33 | loss: 0.20249 | val_0_auc: 0.98302 |  0:00:36s\n",
      "epoch 34 | loss: 0.19483 | val_0_auc: 0.98183 |  0:00:37s\n",
      "epoch 35 | loss: 0.20647 | val_0_auc: 0.97512 |  0:00:38s\n",
      "epoch 36 | loss: 0.20547 | val_0_auc: 0.98183 |  0:00:39s\n",
      "epoch 37 | loss: 0.21362 | val_0_auc: 0.98395 |  0:00:40s\n",
      "epoch 38 | loss: 0.21182 | val_0_auc: 0.98492 |  0:00:41s\n",
      "epoch 39 | loss: 0.1912  | val_0_auc: 0.98356 |  0:00:42s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.98492\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98492316  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 42.9936\n",
      "Function value obtained: -0.9849\n",
      "Current minimum: -0.9901\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8912524088049079, 'lambda_sparse': 0.051844017297468604, 'n_steps': 9, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 3.25333 | val_0_auc: 0.71837 |  0:00:02s\n",
      "epoch 1  | loss: 2.63697 | val_0_auc: 0.69109 |  0:00:05s\n",
      "epoch 2  | loss: 2.20532 | val_0_auc: 0.78988 |  0:00:08s\n",
      "epoch 3  | loss: 1.09503 | val_0_auc: 0.7896  |  0:00:11s\n",
      "epoch 4  | loss: 1.38608 | val_0_auc: 0.77454 |  0:00:14s\n",
      "epoch 5  | loss: 0.80761 | val_0_auc: 0.841   |  0:00:17s\n",
      "epoch 6  | loss: 0.80416 | val_0_auc: 0.81168 |  0:00:20s\n",
      "epoch 7  | loss: 0.62972 | val_0_auc: 0.82266 |  0:00:23s\n",
      "epoch 8  | loss: 0.59668 | val_0_auc: 0.83424 |  0:00:26s\n",
      "epoch 9  | loss: 0.68265 | val_0_auc: 0.8477  |  0:00:29s\n",
      "epoch 10 | loss: 0.69461 | val_0_auc: 0.85093 |  0:00:32s\n",
      "epoch 11 | loss: 0.7704  | val_0_auc: 0.87412 |  0:00:35s\n",
      "epoch 12 | loss: 0.68405 | val_0_auc: 0.86277 |  0:00:38s\n",
      "epoch 13 | loss: 0.66258 | val_0_auc: 0.89593 |  0:00:41s\n",
      "epoch 14 | loss: 0.52635 | val_0_auc: 0.87593 |  0:00:44s\n",
      "epoch 15 | loss: 0.42485 | val_0_auc: 0.89622 |  0:00:46s\n",
      "epoch 16 | loss: 0.36348 | val_0_auc: 0.92013 |  0:00:49s\n",
      "epoch 17 | loss: 0.35529 | val_0_auc: 0.9115  |  0:00:52s\n",
      "epoch 18 | loss: 0.36556 | val_0_auc: 0.91428 |  0:00:55s\n",
      "epoch 19 | loss: 0.36806 | val_0_auc: 0.92813 |  0:00:58s\n",
      "epoch 20 | loss: 0.39641 | val_0_auc: 0.92877 |  0:01:01s\n",
      "epoch 21 | loss: 0.40384 | val_0_auc: 0.91854 |  0:01:04s\n",
      "epoch 22 | loss: 0.34978 | val_0_auc: 0.941   |  0:01:07s\n",
      "epoch 23 | loss: 0.33803 | val_0_auc: 0.94405 |  0:01:10s\n",
      "epoch 24 | loss: 0.33488 | val_0_auc: 0.94887 |  0:01:13s\n",
      "epoch 25 | loss: 0.3109  | val_0_auc: 0.93527 |  0:01:16s\n",
      "epoch 26 | loss: 0.34043 | val_0_auc: 0.94698 |  0:01:18s\n",
      "epoch 27 | loss: 0.33603 | val_0_auc: 0.95644 |  0:01:21s\n",
      "epoch 28 | loss: 0.30871 | val_0_auc: 0.9546  |  0:01:24s\n",
      "epoch 29 | loss: 0.28495 | val_0_auc: 0.95644 |  0:01:27s\n",
      "epoch 30 | loss: 0.28497 | val_0_auc: 0.96008 |  0:01:30s\n",
      "epoch 31 | loss: 0.27256 | val_0_auc: 0.96448 |  0:01:33s\n",
      "epoch 32 | loss: 0.26998 | val_0_auc: 0.96215 |  0:01:36s\n",
      "epoch 33 | loss: 0.27128 | val_0_auc: 0.95997 |  0:01:38s\n",
      "epoch 34 | loss: 0.25939 | val_0_auc: 0.96875 |  0:01:41s\n",
      "epoch 35 | loss: 0.26793 | val_0_auc: 0.96817 |  0:01:44s\n",
      "epoch 36 | loss: 0.27307 | val_0_auc: 0.9602  |  0:01:47s\n",
      "epoch 37 | loss: 0.24678 | val_0_auc: 0.97709 |  0:01:50s\n",
      "epoch 38 | loss: 0.23596 | val_0_auc: 0.97634 |  0:01:53s\n",
      "epoch 39 | loss: 0.22304 | val_0_auc: 0.98258 |  0:01:55s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.98258\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98258372  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 116.9351\n",
      "Function value obtained: -0.9826\n",
      "Current minimum: -0.9901\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.180516846092147, 'lambda_sparse': 0.0704096570970051, 'n_steps': 7, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.96262 | val_0_auc: 0.75581 |  0:00:01s\n",
      "epoch 1  | loss: 0.60282 | val_0_auc: 0.79964 |  0:00:02s\n",
      "epoch 2  | loss: 0.53381 | val_0_auc: 0.83923 |  0:00:04s\n",
      "epoch 3  | loss: 0.49611 | val_0_auc: 0.86544 |  0:00:05s\n",
      "epoch 4  | loss: 0.47347 | val_0_auc: 0.88222 |  0:00:07s\n",
      "epoch 5  | loss: 0.45107 | val_0_auc: 0.88558 |  0:00:08s\n",
      "epoch 6  | loss: 0.42957 | val_0_auc: 0.90118 |  0:00:10s\n",
      "epoch 7  | loss: 0.4178  | val_0_auc: 0.92047 |  0:00:11s\n",
      "epoch 8  | loss: 0.41763 | val_0_auc: 0.91896 |  0:00:12s\n",
      "epoch 9  | loss: 0.37954 | val_0_auc: 0.93528 |  0:00:14s\n",
      "epoch 10 | loss: 0.36463 | val_0_auc: 0.93403 |  0:00:15s\n",
      "epoch 11 | loss: 0.33326 | val_0_auc: 0.9476  |  0:00:17s\n",
      "epoch 12 | loss: 0.32291 | val_0_auc: 0.9495  |  0:00:18s\n",
      "epoch 13 | loss: 0.31466 | val_0_auc: 0.96161 |  0:00:19s\n",
      "epoch 14 | loss: 0.27925 | val_0_auc: 0.96895 |  0:00:21s\n",
      "epoch 15 | loss: 0.2821  | val_0_auc: 0.97711 |  0:00:22s\n",
      "epoch 16 | loss: 0.27506 | val_0_auc: 0.9745  |  0:00:24s\n",
      "epoch 17 | loss: 0.27428 | val_0_auc: 0.97373 |  0:00:25s\n",
      "epoch 18 | loss: 0.27111 | val_0_auc: 0.97396 |  0:00:26s\n",
      "epoch 19 | loss: 0.25084 | val_0_auc: 0.97145 |  0:00:28s\n",
      "epoch 20 | loss: 0.25586 | val_0_auc: 0.97719 |  0:00:29s\n",
      "epoch 21 | loss: 0.25713 | val_0_auc: 0.97392 |  0:00:31s\n",
      "epoch 22 | loss: 0.24462 | val_0_auc: 0.97455 |  0:00:32s\n",
      "epoch 23 | loss: 0.25302 | val_0_auc: 0.97555 |  0:00:33s\n",
      "epoch 24 | loss: 0.21954 | val_0_auc: 0.98758 |  0:00:35s\n",
      "epoch 25 | loss: 0.22415 | val_0_auc: 0.98287 |  0:00:36s\n",
      "epoch 26 | loss: 0.20484 | val_0_auc: 0.98331 |  0:00:38s\n",
      "epoch 27 | loss: 0.20735 | val_0_auc: 0.98672 |  0:00:39s\n",
      "epoch 28 | loss: 0.2098  | val_0_auc: 0.98145 |  0:00:40s\n",
      "epoch 29 | loss: 0.19726 | val_0_auc: 0.98684 |  0:00:42s\n",
      "epoch 30 | loss: 0.18697 | val_0_auc: 0.98652 |  0:00:43s\n",
      "epoch 31 | loss: 0.1979  | val_0_auc: 0.98841 |  0:00:44s\n",
      "epoch 32 | loss: 0.2022  | val_0_auc: 0.98857 |  0:00:46s\n",
      "epoch 33 | loss: 0.201   | val_0_auc: 0.98955 |  0:00:47s\n",
      "epoch 34 | loss: 0.19352 | val_0_auc: 0.98255 |  0:00:49s\n",
      "epoch 35 | loss: 0.1973  | val_0_auc: 0.98204 |  0:00:50s\n",
      "epoch 36 | loss: 0.18263 | val_0_auc: 0.98666 |  0:00:51s\n",
      "epoch 37 | loss: 0.17711 | val_0_auc: 0.99007 |  0:00:53s\n",
      "epoch 38 | loss: 0.17814 | val_0_auc: 0.98064 |  0:00:54s\n",
      "epoch 39 | loss: 0.18064 | val_0_auc: 0.98793 |  0:00:56s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99007\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99006846  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 56.5948\n",
      "Function value obtained: -0.9901\n",
      "Current minimum: -0.9901\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3383645137018403, 'lambda_sparse': 0.05706257633714025, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.6}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.02463 | val_0_auc: 0.72245 |  0:00:01s\n",
      "epoch 1  | loss: 0.65602 | val_0_auc: 0.77847 |  0:00:02s\n",
      "epoch 2  | loss: 0.56103 | val_0_auc: 0.82898 |  0:00:04s\n",
      "epoch 3  | loss: 0.51879 | val_0_auc: 0.84133 |  0:00:05s\n",
      "epoch 4  | loss: 0.47948 | val_0_auc: 0.85663 |  0:00:06s\n",
      "epoch 5  | loss: 0.46426 | val_0_auc: 0.87549 |  0:00:08s\n",
      "epoch 6  | loss: 0.4397  | val_0_auc: 0.90303 |  0:00:09s\n",
      "epoch 7  | loss: 0.4305  | val_0_auc: 0.89386 |  0:00:11s\n",
      "epoch 8  | loss: 0.42922 | val_0_auc: 0.91049 |  0:00:12s\n",
      "epoch 9  | loss: 0.43409 | val_0_auc: 0.91204 |  0:00:13s\n",
      "epoch 10 | loss: 0.40277 | val_0_auc: 0.92258 |  0:00:15s\n",
      "epoch 11 | loss: 0.35093 | val_0_auc: 0.93765 |  0:00:16s\n",
      "epoch 12 | loss: 0.34828 | val_0_auc: 0.94092 |  0:00:17s\n",
      "epoch 13 | loss: 0.31169 | val_0_auc: 0.94018 |  0:00:19s\n",
      "epoch 14 | loss: 0.30671 | val_0_auc: 0.95525 |  0:00:20s\n",
      "epoch 15 | loss: 0.29425 | val_0_auc: 0.96085 |  0:00:22s\n",
      "epoch 16 | loss: 0.29317 | val_0_auc: 0.95897 |  0:00:23s\n",
      "epoch 17 | loss: 0.27752 | val_0_auc: 0.95975 |  0:00:24s\n",
      "epoch 18 | loss: 0.31297 | val_0_auc: 0.96143 |  0:00:26s\n",
      "epoch 19 | loss: 0.28355 | val_0_auc: 0.96014 |  0:00:27s\n",
      "epoch 20 | loss: 0.26764 | val_0_auc: 0.97298 |  0:00:28s\n",
      "epoch 21 | loss: 0.24951 | val_0_auc: 0.9762  |  0:00:30s\n",
      "epoch 22 | loss: 0.24539 | val_0_auc: 0.97262 |  0:00:31s\n",
      "epoch 23 | loss: 0.26081 | val_0_auc: 0.97284 |  0:00:33s\n",
      "epoch 24 | loss: 0.27284 | val_0_auc: 0.96963 |  0:00:34s\n",
      "epoch 25 | loss: 0.25628 | val_0_auc: 0.96818 |  0:00:35s\n",
      "epoch 26 | loss: 0.26652 | val_0_auc: 0.97717 |  0:00:37s\n",
      "epoch 27 | loss: 0.28799 | val_0_auc: 0.97354 |  0:00:38s\n",
      "epoch 28 | loss: 0.30404 | val_0_auc: 0.97497 |  0:00:39s\n",
      "epoch 29 | loss: 0.27406 | val_0_auc: 0.97795 |  0:00:41s\n",
      "epoch 30 | loss: 0.27658 | val_0_auc: 0.97626 |  0:00:42s\n",
      "epoch 31 | loss: 0.29409 | val_0_auc: 0.97865 |  0:00:43s\n",
      "epoch 32 | loss: 0.26229 | val_0_auc: 0.98242 |  0:00:45s\n",
      "epoch 33 | loss: 0.24141 | val_0_auc: 0.98627 |  0:00:46s\n",
      "epoch 34 | loss: 0.221   | val_0_auc: 0.98149 |  0:00:48s\n",
      "epoch 35 | loss: 0.22276 | val_0_auc: 0.97312 |  0:00:49s\n",
      "epoch 36 | loss: 0.21835 | val_0_auc: 0.97916 |  0:00:50s\n",
      "epoch 37 | loss: 0.20611 | val_0_auc: 0.97515 |  0:00:52s\n",
      "epoch 38 | loss: 0.22071 | val_0_auc: 0.97412 |  0:00:53s\n",
      "epoch 39 | loss: 0.22801 | val_0_auc: 0.97918 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.98627\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98626871  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 55.2988\n",
      "Function value obtained: -0.9863\n",
      "Current minimum: -0.9901\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2065124877528246, 'lambda_sparse': 0.05818373167191515, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.93265 | val_0_auc: 0.75874 |  0:00:01s\n",
      "epoch 1  | loss: 0.61115 | val_0_auc: 0.79744 |  0:00:02s\n",
      "epoch 2  | loss: 0.52241 | val_0_auc: 0.83314 |  0:00:03s\n",
      "epoch 3  | loss: 0.48871 | val_0_auc: 0.84827 |  0:00:05s\n",
      "epoch 4  | loss: 0.45873 | val_0_auc: 0.8734  |  0:00:06s\n",
      "epoch 5  | loss: 0.43371 | val_0_auc: 0.91346 |  0:00:07s\n",
      "epoch 6  | loss: 0.40002 | val_0_auc: 0.91741 |  0:00:09s\n",
      "epoch 7  | loss: 0.38453 | val_0_auc: 0.92172 |  0:00:10s\n",
      "epoch 8  | loss: 0.36367 | val_0_auc: 0.9434  |  0:00:11s\n",
      "epoch 9  | loss: 0.33628 | val_0_auc: 0.95345 |  0:00:13s\n",
      "epoch 10 | loss: 0.30394 | val_0_auc: 0.9567  |  0:00:14s\n",
      "epoch 11 | loss: 0.28799 | val_0_auc: 0.9677  |  0:00:15s\n",
      "epoch 12 | loss: 0.27613 | val_0_auc: 0.96946 |  0:00:17s\n",
      "epoch 13 | loss: 0.2676  | val_0_auc: 0.97171 |  0:00:18s\n",
      "epoch 14 | loss: 0.26767 | val_0_auc: 0.97472 |  0:00:19s\n",
      "epoch 15 | loss: 0.25451 | val_0_auc: 0.97106 |  0:00:21s\n",
      "epoch 16 | loss: 0.24079 | val_0_auc: 0.98054 |  0:00:22s\n",
      "epoch 17 | loss: 0.23509 | val_0_auc: 0.9807  |  0:00:23s\n",
      "epoch 18 | loss: 0.22147 | val_0_auc: 0.98114 |  0:00:24s\n",
      "epoch 19 | loss: 0.22243 | val_0_auc: 0.9778  |  0:00:26s\n",
      "epoch 20 | loss: 0.21313 | val_0_auc: 0.97818 |  0:00:27s\n",
      "epoch 21 | loss: 0.22885 | val_0_auc: 0.97894 |  0:00:28s\n",
      "epoch 22 | loss: 0.20982 | val_0_auc: 0.98522 |  0:00:30s\n",
      "epoch 23 | loss: 0.21255 | val_0_auc: 0.97895 |  0:00:31s\n",
      "epoch 24 | loss: 0.22225 | val_0_auc: 0.98485 |  0:00:32s\n",
      "epoch 25 | loss: 0.21926 | val_0_auc: 0.98862 |  0:00:33s\n",
      "epoch 26 | loss: 0.211   | val_0_auc: 0.98634 |  0:00:35s\n",
      "epoch 27 | loss: 0.1975  | val_0_auc: 0.98616 |  0:00:36s\n",
      "epoch 28 | loss: 0.20012 | val_0_auc: 0.9893  |  0:00:37s\n",
      "epoch 29 | loss: 0.20896 | val_0_auc: 0.99196 |  0:00:39s\n",
      "epoch 30 | loss: 0.18433 | val_0_auc: 0.99099 |  0:00:40s\n",
      "epoch 31 | loss: 0.19477 | val_0_auc: 0.98699 |  0:00:41s\n",
      "epoch 32 | loss: 0.18761 | val_0_auc: 0.9918  |  0:00:43s\n",
      "epoch 33 | loss: 0.17692 | val_0_auc: 0.99035 |  0:00:44s\n",
      "epoch 34 | loss: 0.19155 | val_0_auc: 0.9925  |  0:00:45s\n",
      "epoch 35 | loss: 0.17774 | val_0_auc: 0.99214 |  0:00:46s\n",
      "epoch 36 | loss: 0.17598 | val_0_auc: 0.98584 |  0:00:48s\n",
      "epoch 37 | loss: 0.18498 | val_0_auc: 0.9921  |  0:00:49s\n",
      "epoch 38 | loss: 0.1609  | val_0_auc: 0.98959 |  0:00:50s\n",
      "epoch 39 | loss: 0.17585 | val_0_auc: 0.99285 |  0:00:52s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99285\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99284655  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 52.6211\n",
      "Function value obtained: -0.9928\n",
      "Current minimum: -0.9928\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5329280145625486, 'lambda_sparse': 0.032337444455093754, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.90245 | val_0_auc: 0.7002  |  0:00:01s\n",
      "epoch 1  | loss: 0.64286 | val_0_auc: 0.78209 |  0:00:03s\n",
      "epoch 2  | loss: 0.57698 | val_0_auc: 0.81348 |  0:00:04s\n",
      "epoch 3  | loss: 0.49269 | val_0_auc: 0.82747 |  0:00:06s\n",
      "epoch 4  | loss: 0.49754 | val_0_auc: 0.85227 |  0:00:07s\n",
      "epoch 5  | loss: 0.46379 | val_0_auc: 0.88353 |  0:00:09s\n",
      "epoch 6  | loss: 0.43236 | val_0_auc: 0.88551 |  0:00:10s\n",
      "epoch 7  | loss: 0.43431 | val_0_auc: 0.88799 |  0:00:12s\n",
      "epoch 8  | loss: 0.417   | val_0_auc: 0.89155 |  0:00:14s\n",
      "epoch 9  | loss: 0.40511 | val_0_auc: 0.90371 |  0:00:15s\n",
      "epoch 10 | loss: 0.47458 | val_0_auc: 0.90087 |  0:00:17s\n",
      "epoch 11 | loss: 0.38616 | val_0_auc: 0.90924 |  0:00:18s\n",
      "epoch 12 | loss: 0.37145 | val_0_auc: 0.91597 |  0:00:20s\n",
      "epoch 13 | loss: 0.38417 | val_0_auc: 0.93081 |  0:00:21s\n",
      "epoch 14 | loss: 0.40322 | val_0_auc: 0.90861 |  0:00:23s\n",
      "epoch 15 | loss: 0.90946 | val_0_auc: 0.90831 |  0:00:25s\n",
      "epoch 16 | loss: 0.41631 | val_0_auc: 0.91618 |  0:00:26s\n",
      "epoch 17 | loss: 0.37923 | val_0_auc: 0.93749 |  0:00:28s\n",
      "epoch 18 | loss: 0.42415 | val_0_auc: 0.93872 |  0:00:29s\n",
      "epoch 19 | loss: 0.3635  | val_0_auc: 0.94297 |  0:00:31s\n",
      "epoch 20 | loss: 0.3472  | val_0_auc: 0.92526 |  0:00:32s\n",
      "epoch 21 | loss: 0.34785 | val_0_auc: 0.93173 |  0:00:34s\n",
      "epoch 22 | loss: 0.35364 | val_0_auc: 0.93424 |  0:00:35s\n",
      "epoch 23 | loss: 0.34005 | val_0_auc: 0.93741 |  0:00:37s\n",
      "epoch 24 | loss: 0.37144 | val_0_auc: 0.91912 |  0:00:39s\n",
      "epoch 25 | loss: 0.37928 | val_0_auc: 0.92349 |  0:00:40s\n",
      "epoch 26 | loss: 0.3459  | val_0_auc: 0.9404  |  0:00:42s\n",
      "epoch 27 | loss: 0.35933 | val_0_auc: 0.93095 |  0:00:43s\n",
      "epoch 28 | loss: 0.38031 | val_0_auc: 0.92766 |  0:00:45s\n",
      "epoch 29 | loss: 0.35822 | val_0_auc: 0.94691 |  0:00:46s\n",
      "epoch 30 | loss: 0.33198 | val_0_auc: 0.94316 |  0:00:48s\n",
      "epoch 31 | loss: 0.33133 | val_0_auc: 0.94854 |  0:00:50s\n",
      "epoch 32 | loss: 0.31865 | val_0_auc: 0.9468  |  0:00:51s\n",
      "epoch 33 | loss: 0.30498 | val_0_auc: 0.95189 |  0:00:53s\n",
      "epoch 34 | loss: 0.3039  | val_0_auc: 0.94748 |  0:00:54s\n",
      "epoch 35 | loss: 0.3084  | val_0_auc: 0.95227 |  0:00:56s\n",
      "epoch 36 | loss: 0.29097 | val_0_auc: 0.95456 |  0:00:57s\n",
      "epoch 37 | loss: 0.29999 | val_0_auc: 0.95302 |  0:00:59s\n",
      "epoch 38 | loss: 0.28952 | val_0_auc: 0.96343 |  0:01:00s\n",
      "epoch 39 | loss: 0.28379 | val_0_auc: 0.95413 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.96343\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.96342766  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 63.0210\n",
      "Function value obtained: -0.9634\n",
      "Current minimum: -0.9928\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0756266143045858, 'lambda_sparse': 0.08091300669497235, 'n_steps': 8, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.64838 | val_0_auc: 0.751   |  0:00:01s\n",
      "epoch 1  | loss: 0.83757 | val_0_auc: 0.80273 |  0:00:03s\n",
      "epoch 2  | loss: 0.5806  | val_0_auc: 0.841   |  0:00:05s\n",
      "epoch 3  | loss: 0.49883 | val_0_auc: 0.88233 |  0:00:06s\n",
      "epoch 4  | loss: 0.45601 | val_0_auc: 0.90786 |  0:00:08s\n",
      "epoch 5  | loss: 0.43928 | val_0_auc: 0.91259 |  0:00:10s\n",
      "epoch 6  | loss: 0.4058  | val_0_auc: 0.92435 |  0:00:12s\n",
      "epoch 7  | loss: 0.37979 | val_0_auc: 0.93155 |  0:00:13s\n",
      "epoch 8  | loss: 0.37332 | val_0_auc: 0.94133 |  0:00:15s\n",
      "epoch 9  | loss: 0.35825 | val_0_auc: 0.95464 |  0:00:17s\n",
      "epoch 10 | loss: 0.34727 | val_0_auc: 0.94663 |  0:00:19s\n",
      "epoch 11 | loss: 0.32887 | val_0_auc: 0.96085 |  0:00:20s\n",
      "epoch 12 | loss: 0.30036 | val_0_auc: 0.96259 |  0:00:22s\n",
      "epoch 13 | loss: 0.27548 | val_0_auc: 0.96326 |  0:00:24s\n",
      "epoch 14 | loss: 0.27475 | val_0_auc: 0.97314 |  0:00:26s\n",
      "epoch 15 | loss: 0.25351 | val_0_auc: 0.96654 |  0:00:28s\n",
      "epoch 16 | loss: 0.24732 | val_0_auc: 0.98137 |  0:00:29s\n",
      "epoch 17 | loss: 0.27218 | val_0_auc: 0.97612 |  0:00:31s\n",
      "epoch 18 | loss: 0.25827 | val_0_auc: 0.9785  |  0:00:33s\n",
      "epoch 19 | loss: 0.27049 | val_0_auc: 0.98158 |  0:00:34s\n",
      "epoch 20 | loss: 0.23998 | val_0_auc: 0.97572 |  0:00:36s\n",
      "epoch 21 | loss: 0.23787 | val_0_auc: 0.97863 |  0:00:38s\n",
      "epoch 22 | loss: 0.23555 | val_0_auc: 0.98381 |  0:00:40s\n",
      "epoch 23 | loss: 0.22783 | val_0_auc: 0.98631 |  0:00:41s\n",
      "epoch 24 | loss: 0.21304 | val_0_auc: 0.98708 |  0:00:43s\n",
      "epoch 25 | loss: 0.20372 | val_0_auc: 0.98921 |  0:00:45s\n",
      "epoch 26 | loss: 0.20925 | val_0_auc: 0.98686 |  0:00:46s\n",
      "epoch 27 | loss: 0.22063 | val_0_auc: 0.98885 |  0:00:48s\n",
      "epoch 28 | loss: 0.20892 | val_0_auc: 0.98775 |  0:00:50s\n",
      "epoch 29 | loss: 0.21027 | val_0_auc: 0.98973 |  0:00:52s\n",
      "epoch 30 | loss: 0.20458 | val_0_auc: 0.99019 |  0:00:53s\n",
      "epoch 31 | loss: 0.20754 | val_0_auc: 0.98832 |  0:00:55s\n",
      "epoch 32 | loss: 0.20862 | val_0_auc: 0.98941 |  0:00:57s\n",
      "epoch 33 | loss: 0.20159 | val_0_auc: 0.98577 |  0:00:58s\n",
      "epoch 34 | loss: 0.19612 | val_0_auc: 0.99009 |  0:01:00s\n",
      "epoch 35 | loss: 0.18824 | val_0_auc: 0.98745 |  0:01:02s\n",
      "epoch 36 | loss: 0.19307 | val_0_auc: 0.98362 |  0:01:04s\n",
      "epoch 37 | loss: 0.19462 | val_0_auc: 0.99181 |  0:01:05s\n",
      "epoch 38 | loss: 0.18879 | val_0_auc: 0.98694 |  0:01:07s\n",
      "epoch 39 | loss: 0.18163 | val_0_auc: 0.98987 |  0:01:09s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99181\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99180639  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 69.7980\n",
      "Function value obtained: -0.9918\n",
      "Current minimum: -0.9928\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6770395231202972, 'lambda_sparse': 0.06335317336345526, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.92922 | val_0_auc: 0.69376 |  0:00:01s\n",
      "epoch 1  | loss: 0.63988 | val_0_auc: 0.79334 |  0:00:03s\n",
      "epoch 2  | loss: 0.55308 | val_0_auc: 0.8009  |  0:00:04s\n",
      "epoch 3  | loss: 0.55752 | val_0_auc: 0.82424 |  0:00:06s\n",
      "epoch 4  | loss: 0.52071 | val_0_auc: 0.83947 |  0:00:07s\n",
      "epoch 5  | loss: 0.54969 | val_0_auc: 0.81296 |  0:00:09s\n",
      "epoch 6  | loss: 0.55115 | val_0_auc: 0.85578 |  0:00:11s\n",
      "epoch 7  | loss: 0.53188 | val_0_auc: 0.86088 |  0:00:12s\n",
      "epoch 8  | loss: 0.59641 | val_0_auc: 0.85308 |  0:00:14s\n",
      "epoch 9  | loss: 0.50386 | val_0_auc: 0.8492  |  0:00:15s\n",
      "epoch 10 | loss: 0.51704 | val_0_auc: 0.85412 |  0:00:17s\n",
      "epoch 11 | loss: 0.4728  | val_0_auc: 0.8853  |  0:00:18s\n",
      "epoch 12 | loss: 0.44019 | val_0_auc: 0.89195 |  0:00:20s\n",
      "epoch 13 | loss: 0.43857 | val_0_auc: 0.87868 |  0:00:21s\n",
      "epoch 14 | loss: 0.4816  | val_0_auc: 0.87873 |  0:00:23s\n",
      "epoch 15 | loss: 0.42674 | val_0_auc: 0.89901 |  0:00:24s\n",
      "epoch 16 | loss: 0.41738 | val_0_auc: 0.90352 |  0:00:26s\n",
      "epoch 17 | loss: 0.4158  | val_0_auc: 0.90114 |  0:00:28s\n",
      "epoch 18 | loss: 0.40296 | val_0_auc: 0.91006 |  0:00:29s\n",
      "epoch 19 | loss: 0.40472 | val_0_auc: 0.90471 |  0:00:31s\n",
      "epoch 20 | loss: 0.39492 | val_0_auc: 0.91561 |  0:00:32s\n",
      "epoch 21 | loss: 0.38791 | val_0_auc: 0.90598 |  0:00:34s\n",
      "epoch 22 | loss: 0.43214 | val_0_auc: 0.91417 |  0:00:35s\n",
      "epoch 23 | loss: 0.3879  | val_0_auc: 0.92035 |  0:00:37s\n",
      "epoch 24 | loss: 0.36342 | val_0_auc: 0.9127  |  0:00:39s\n",
      "epoch 25 | loss: 0.36286 | val_0_auc: 0.92831 |  0:00:40s\n",
      "epoch 26 | loss: 0.35443 | val_0_auc: 0.93123 |  0:00:42s\n",
      "epoch 27 | loss: 0.35192 | val_0_auc: 0.91513 |  0:00:43s\n",
      "epoch 28 | loss: 0.36758 | val_0_auc: 0.91755 |  0:00:45s\n",
      "epoch 29 | loss: 0.38765 | val_0_auc: 0.91899 |  0:00:46s\n",
      "epoch 30 | loss: 0.39221 | val_0_auc: 0.91669 |  0:00:48s\n",
      "epoch 31 | loss: 0.37474 | val_0_auc: 0.91483 |  0:00:49s\n",
      "epoch 32 | loss: 0.37513 | val_0_auc: 0.91648 |  0:00:51s\n",
      "epoch 33 | loss: 0.35874 | val_0_auc: 0.9255  |  0:00:52s\n",
      "epoch 34 | loss: 0.34186 | val_0_auc: 0.93426 |  0:00:54s\n",
      "epoch 35 | loss: 0.34192 | val_0_auc: 0.95142 |  0:00:56s\n",
      "epoch 36 | loss: 0.32497 | val_0_auc: 0.94007 |  0:00:57s\n",
      "epoch 37 | loss: 0.34482 | val_0_auc: 0.94091 |  0:00:59s\n",
      "epoch 38 | loss: 0.33438 | val_0_auc: 0.93815 |  0:01:00s\n",
      "epoch 39 | loss: 0.31336 | val_0_auc: 0.9532  |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.9532\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95319629  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 62.7282\n",
      "Function value obtained: -0.9532\n",
      "Current minimum: -0.9928\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0202090465482214, 'lambda_sparse': 0.030245422904344538, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.8757  | val_0_auc: 0.74776 |  0:00:01s\n",
      "epoch 1  | loss: 0.58123 | val_0_auc: 0.8305  |  0:00:03s\n",
      "epoch 2  | loss: 0.50806 | val_0_auc: 0.8576  |  0:00:04s\n",
      "epoch 3  | loss: 0.45301 | val_0_auc: 0.90185 |  0:00:06s\n",
      "epoch 4  | loss: 0.40392 | val_0_auc: 0.91439 |  0:00:07s\n",
      "epoch 5  | loss: 0.38593 | val_0_auc: 0.93608 |  0:00:09s\n",
      "epoch 6  | loss: 0.35966 | val_0_auc: 0.94608 |  0:00:11s\n",
      "epoch 7  | loss: 0.34203 | val_0_auc: 0.95457 |  0:00:12s\n",
      "epoch 8  | loss: 0.29886 | val_0_auc: 0.96392 |  0:00:14s\n",
      "epoch 9  | loss: 0.27662 | val_0_auc: 0.96183 |  0:00:15s\n",
      "epoch 10 | loss: 0.27862 | val_0_auc: 0.96384 |  0:00:17s\n",
      "epoch 11 | loss: 0.25802 | val_0_auc: 0.98321 |  0:00:18s\n",
      "epoch 12 | loss: 0.24257 | val_0_auc: 0.9834  |  0:00:20s\n",
      "epoch 13 | loss: 0.23174 | val_0_auc: 0.9772  |  0:00:22s\n",
      "epoch 14 | loss: 0.22369 | val_0_auc: 0.98421 |  0:00:23s\n",
      "epoch 15 | loss: 0.21077 | val_0_auc: 0.98515 |  0:00:25s\n",
      "epoch 16 | loss: 0.21094 | val_0_auc: 0.98778 |  0:00:26s\n",
      "epoch 17 | loss: 0.21455 | val_0_auc: 0.98441 |  0:00:28s\n",
      "epoch 18 | loss: 0.21295 | val_0_auc: 0.97962 |  0:00:29s\n",
      "epoch 19 | loss: 0.19308 | val_0_auc: 0.98234 |  0:00:31s\n",
      "epoch 20 | loss: 0.18906 | val_0_auc: 0.983   |  0:00:32s\n",
      "epoch 21 | loss: 0.18566 | val_0_auc: 0.98616 |  0:00:34s\n",
      "epoch 22 | loss: 0.1924  | val_0_auc: 0.98846 |  0:00:35s\n",
      "epoch 23 | loss: 0.17094 | val_0_auc: 0.99045 |  0:00:37s\n",
      "epoch 24 | loss: 0.18088 | val_0_auc: 0.98652 |  0:00:39s\n",
      "epoch 25 | loss: 0.18995 | val_0_auc: 0.98723 |  0:00:40s\n",
      "epoch 26 | loss: 0.18245 | val_0_auc: 0.98758 |  0:00:42s\n",
      "epoch 27 | loss: 0.16657 | val_0_auc: 0.98936 |  0:00:43s\n",
      "epoch 28 | loss: 0.17141 | val_0_auc: 0.98869 |  0:00:45s\n",
      "epoch 29 | loss: 0.1757  | val_0_auc: 0.99012 |  0:00:46s\n",
      "epoch 30 | loss: 0.17154 | val_0_auc: 0.99161 |  0:00:48s\n",
      "epoch 31 | loss: 0.16596 | val_0_auc: 0.98668 |  0:00:49s\n",
      "epoch 32 | loss: 0.16835 | val_0_auc: 0.99223 |  0:00:51s\n",
      "epoch 33 | loss: 0.16933 | val_0_auc: 0.99205 |  0:00:52s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | loss: 0.16508 | val_0_auc: 0.99162 |  0:00:54s\n",
      "epoch 35 | loss: 0.15878 | val_0_auc: 0.99151 |  0:00:55s\n",
      "epoch 36 | loss: 0.15112 | val_0_auc: 0.99129 |  0:00:57s\n",
      "epoch 37 | loss: 0.16094 | val_0_auc: 0.99036 |  0:00:59s\n",
      "epoch 38 | loss: 0.1623  | val_0_auc: 0.9928  |  0:01:00s\n",
      "epoch 39 | loss: 0.17385 | val_0_auc: 0.99021 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.9928\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99280213  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 62.6663\n",
      "Function value obtained: -0.9928\n",
      "Current minimum: -0.9928\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3572820543723683, 'lambda_sparse': 0.08939363131402198, 'n_steps': 10, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.29581 | val_0_auc: 0.77149 |  0:00:02s\n",
      "epoch 1  | loss: 0.82849 | val_0_auc: 0.832   |  0:00:04s\n",
      "epoch 2  | loss: 0.78971 | val_0_auc: 0.87235 |  0:00:06s\n",
      "epoch 3  | loss: 0.70827 | val_0_auc: 0.82691 |  0:00:09s\n",
      "epoch 4  | loss: 0.7287  | val_0_auc: 0.88534 |  0:00:11s\n",
      "epoch 5  | loss: 0.60927 | val_0_auc: 0.90398 |  0:00:13s\n",
      "epoch 6  | loss: 0.47721 | val_0_auc: 0.89624 |  0:00:16s\n",
      "epoch 7  | loss: 0.43884 | val_0_auc: 0.91195 |  0:00:18s\n",
      "epoch 8  | loss: 0.48705 | val_0_auc: 0.87738 |  0:00:20s\n",
      "epoch 9  | loss: 0.51729 | val_0_auc: 0.90508 |  0:00:23s\n",
      "epoch 10 | loss: 0.42373 | val_0_auc: 0.90632 |  0:00:25s\n",
      "epoch 11 | loss: 0.40468 | val_0_auc: 0.91508 |  0:00:27s\n",
      "epoch 12 | loss: 0.372   | val_0_auc: 0.91116 |  0:00:30s\n",
      "epoch 13 | loss: 0.39453 | val_0_auc: 0.92814 |  0:00:32s\n",
      "epoch 14 | loss: 0.37597 | val_0_auc: 0.93466 |  0:00:34s\n",
      "epoch 15 | loss: 0.37585 | val_0_auc: 0.91956 |  0:00:36s\n",
      "epoch 16 | loss: 0.40216 | val_0_auc: 0.9236  |  0:00:39s\n",
      "epoch 17 | loss: 0.37197 | val_0_auc: 0.92727 |  0:00:41s\n",
      "epoch 18 | loss: 0.3699  | val_0_auc: 0.94621 |  0:00:43s\n",
      "epoch 19 | loss: 0.36452 | val_0_auc: 0.93941 |  0:00:46s\n",
      "epoch 20 | loss: 0.35363 | val_0_auc: 0.94248 |  0:00:48s\n",
      "epoch 21 | loss: 0.35469 | val_0_auc: 0.94629 |  0:00:50s\n",
      "epoch 22 | loss: 0.35031 | val_0_auc: 0.94775 |  0:00:53s\n",
      "epoch 23 | loss: 0.35687 | val_0_auc: 0.94589 |  0:00:55s\n",
      "epoch 24 | loss: 0.32597 | val_0_auc: 0.95693 |  0:00:57s\n",
      "epoch 25 | loss: 0.31787 | val_0_auc: 0.94484 |  0:00:59s\n",
      "epoch 26 | loss: 0.3642  | val_0_auc: 0.94197 |  0:01:02s\n",
      "epoch 27 | loss: 0.3721  | val_0_auc: 0.94429 |  0:01:04s\n",
      "epoch 28 | loss: 0.3779  | val_0_auc: 0.94572 |  0:01:06s\n",
      "epoch 29 | loss: 0.32361 | val_0_auc: 0.95321 |  0:01:09s\n",
      "epoch 30 | loss: 0.33535 | val_0_auc: 0.95451 |  0:01:11s\n",
      "epoch 31 | loss: 0.3334  | val_0_auc: 0.95604 |  0:01:13s\n",
      "epoch 32 | loss: 0.33947 | val_0_auc: 0.95154 |  0:01:16s\n",
      "epoch 33 | loss: 0.45424 | val_0_auc: 0.9475  |  0:01:18s\n",
      "epoch 34 | loss: 0.56043 | val_0_auc: 0.95293 |  0:01:20s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.95693\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95693125  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 81.5891\n",
      "Function value obtained: -0.9569\n",
      "Current minimum: -0.9928\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1692073340137596, 'lambda_sparse': 0.05055603814632494, 'n_steps': 6, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.56686 | val_0_auc: 0.73475 |  0:00:01s\n",
      "epoch 1  | loss: 0.84206 | val_0_auc: 0.80139 |  0:00:03s\n",
      "epoch 2  | loss: 0.58355 | val_0_auc: 0.82832 |  0:00:05s\n",
      "epoch 3  | loss: 0.49677 | val_0_auc: 0.86137 |  0:00:07s\n",
      "epoch 4  | loss: 0.44775 | val_0_auc: 0.9037  |  0:00:09s\n",
      "epoch 5  | loss: 0.4098  | val_0_auc: 0.92297 |  0:00:11s\n",
      "epoch 6  | loss: 0.36835 | val_0_auc: 0.91873 |  0:00:13s\n",
      "epoch 7  | loss: 0.3583  | val_0_auc: 0.93438 |  0:00:15s\n",
      "epoch 8  | loss: 0.35815 | val_0_auc: 0.94264 |  0:00:17s\n",
      "epoch 9  | loss: 0.33075 | val_0_auc: 0.94377 |  0:00:19s\n",
      "epoch 10 | loss: 0.3076  | val_0_auc: 0.95354 |  0:00:21s\n",
      "epoch 11 | loss: 0.28578 | val_0_auc: 0.96036 |  0:00:23s\n",
      "epoch 12 | loss: 0.28367 | val_0_auc: 0.95339 |  0:00:25s\n",
      "epoch 13 | loss: 0.27073 | val_0_auc: 0.96328 |  0:00:27s\n",
      "epoch 14 | loss: 0.26497 | val_0_auc: 0.96463 |  0:00:29s\n",
      "epoch 15 | loss: 0.26206 | val_0_auc: 0.9706  |  0:00:31s\n",
      "epoch 16 | loss: 0.25967 | val_0_auc: 0.97216 |  0:00:33s\n",
      "epoch 17 | loss: 0.24341 | val_0_auc: 0.9726  |  0:00:34s\n",
      "epoch 18 | loss: 0.24726 | val_0_auc: 0.97197 |  0:00:36s\n",
      "epoch 19 | loss: 0.23395 | val_0_auc: 0.97953 |  0:00:38s\n",
      "epoch 20 | loss: 0.21419 | val_0_auc: 0.97489 |  0:00:40s\n",
      "epoch 21 | loss: 0.2192  | val_0_auc: 0.97889 |  0:00:42s\n",
      "epoch 22 | loss: 0.22918 | val_0_auc: 0.98209 |  0:00:44s\n",
      "epoch 23 | loss: 0.24371 | val_0_auc: 0.97915 |  0:00:46s\n",
      "epoch 24 | loss: 0.22313 | val_0_auc: 0.98623 |  0:00:48s\n",
      "epoch 25 | loss: 0.19721 | val_0_auc: 0.98808 |  0:00:50s\n",
      "epoch 26 | loss: 0.19461 | val_0_auc: 0.99003 |  0:00:52s\n",
      "epoch 27 | loss: 0.19632 | val_0_auc: 0.98422 |  0:00:54s\n",
      "epoch 28 | loss: 0.19695 | val_0_auc: 0.97793 |  0:00:56s\n",
      "epoch 29 | loss: 0.18771 | val_0_auc: 0.98524 |  0:00:58s\n",
      "epoch 30 | loss: 0.191   | val_0_auc: 0.98936 |  0:00:59s\n",
      "epoch 31 | loss: 0.18695 | val_0_auc: 0.99103 |  0:01:01s\n",
      "epoch 32 | loss: 0.17483 | val_0_auc: 0.99151 |  0:01:03s\n",
      "epoch 33 | loss: 0.18132 | val_0_auc: 0.99126 |  0:01:05s\n",
      "epoch 34 | loss: 0.19088 | val_0_auc: 0.98743 |  0:01:07s\n",
      "epoch 35 | loss: 0.17397 | val_0_auc: 0.98855 |  0:01:09s\n",
      "epoch 36 | loss: 0.16012 | val_0_auc: 0.9848  |  0:01:11s\n",
      "epoch 37 | loss: 0.17232 | val_0_auc: 0.98056 |  0:01:13s\n",
      "epoch 38 | loss: 0.16073 | val_0_auc: 0.99234 |  0:01:15s\n",
      "epoch 39 | loss: 0.15374 | val_0_auc: 0.99343 |  0:01:17s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99343\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99342956  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 78.1439\n",
      "Function value obtained: -0.9934\n",
      "Current minimum: -0.9934\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0726430938336577, 'lambda_sparse': 0.033960576396166633, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.90214 | val_0_auc: 0.80897 |  0:00:01s\n",
      "epoch 1  | loss: 0.5506  | val_0_auc: 0.86105 |  0:00:02s\n",
      "epoch 2  | loss: 0.47103 | val_0_auc: 0.88849 |  0:00:03s\n",
      "epoch 3  | loss: 0.41192 | val_0_auc: 0.91197 |  0:00:05s\n",
      "epoch 4  | loss: 0.36529 | val_0_auc: 0.9274  |  0:00:06s\n",
      "epoch 5  | loss: 0.34463 | val_0_auc: 0.94384 |  0:00:07s\n",
      "epoch 6  | loss: 0.31598 | val_0_auc: 0.95127 |  0:00:09s\n",
      "epoch 7  | loss: 0.286   | val_0_auc: 0.96631 |  0:00:10s\n",
      "epoch 8  | loss: 0.27107 | val_0_auc: 0.9712  |  0:00:11s\n",
      "epoch 9  | loss: 0.25032 | val_0_auc: 0.97181 |  0:00:12s\n",
      "epoch 10 | loss: 0.24512 | val_0_auc: 0.9743  |  0:00:14s\n",
      "epoch 11 | loss: 0.23131 | val_0_auc: 0.97892 |  0:00:15s\n",
      "epoch 12 | loss: 0.21656 | val_0_auc: 0.98405 |  0:00:16s\n",
      "epoch 13 | loss: 0.22105 | val_0_auc: 0.98567 |  0:00:18s\n",
      "epoch 14 | loss: 0.20844 | val_0_auc: 0.9843  |  0:00:19s\n",
      "epoch 15 | loss: 0.19297 | val_0_auc: 0.9875  |  0:00:20s\n",
      "epoch 16 | loss: 0.19115 | val_0_auc: 0.98622 |  0:00:21s\n",
      "epoch 17 | loss: 0.19393 | val_0_auc: 0.98916 |  0:00:23s\n",
      "epoch 18 | loss: 0.1733  | val_0_auc: 0.99167 |  0:00:24s\n",
      "epoch 19 | loss: 0.17025 | val_0_auc: 0.98892 |  0:00:25s\n",
      "epoch 20 | loss: 0.17264 | val_0_auc: 0.98894 |  0:00:27s\n",
      "epoch 21 | loss: 0.17944 | val_0_auc: 0.99076 |  0:00:28s\n",
      "epoch 22 | loss: 0.15782 | val_0_auc: 0.99056 |  0:00:29s\n",
      "epoch 23 | loss: 0.16858 | val_0_auc: 0.98627 |  0:00:31s\n",
      "epoch 24 | loss: 0.17304 | val_0_auc: 0.99134 |  0:00:32s\n",
      "epoch 25 | loss: 0.17447 | val_0_auc: 0.9884  |  0:00:33s\n",
      "epoch 26 | loss: 0.17151 | val_0_auc: 0.99484 |  0:00:34s\n",
      "epoch 27 | loss: 0.15936 | val_0_auc: 0.9917  |  0:00:36s\n",
      "epoch 28 | loss: 0.16675 | val_0_auc: 0.98947 |  0:00:37s\n",
      "epoch 29 | loss: 0.18135 | val_0_auc: 0.99258 |  0:00:38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30 | loss: 0.15625 | val_0_auc: 0.9937  |  0:00:40s\n",
      "epoch 31 | loss: 0.15132 | val_0_auc: 0.99311 |  0:00:41s\n",
      "epoch 32 | loss: 0.14902 | val_0_auc: 0.99278 |  0:00:42s\n",
      "epoch 33 | loss: 0.13718 | val_0_auc: 0.99279 |  0:00:43s\n",
      "epoch 34 | loss: 0.15554 | val_0_auc: 0.99409 |  0:00:45s\n",
      "epoch 35 | loss: 0.14858 | val_0_auc: 0.99502 |  0:00:46s\n",
      "epoch 36 | loss: 0.14419 | val_0_auc: 0.99187 |  0:00:47s\n",
      "epoch 37 | loss: 0.1611  | val_0_auc: 0.9946  |  0:00:48s\n",
      "epoch 38 | loss: 0.13055 | val_0_auc: 0.99066 |  0:00:50s\n",
      "epoch 39 | loss: 0.15201 | val_0_auc: 0.99318 |  0:00:51s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.99502\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99502498  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 51.9571\n",
      "Function value obtained: -0.9950\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.992146173225783, 'lambda_sparse': 0.022041630342532453, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.2183  | val_0_auc: 0.56366 |  0:00:01s\n",
      "epoch 1  | loss: 0.87898 | val_0_auc: 0.722   |  0:00:03s\n",
      "epoch 2  | loss: 0.77212 | val_0_auc: 0.73878 |  0:00:05s\n",
      "epoch 3  | loss: 0.59197 | val_0_auc: 0.81283 |  0:00:07s\n",
      "epoch 4  | loss: 0.52498 | val_0_auc: 0.86407 |  0:00:09s\n",
      "epoch 5  | loss: 0.48725 | val_0_auc: 0.8467  |  0:00:11s\n",
      "epoch 6  | loss: 0.49477 | val_0_auc: 0.84995 |  0:00:13s\n",
      "epoch 7  | loss: 0.51707 | val_0_auc: 0.87612 |  0:00:15s\n",
      "epoch 8  | loss: 0.45656 | val_0_auc: 0.85314 |  0:00:17s\n",
      "epoch 9  | loss: 0.4896  | val_0_auc: 0.86001 |  0:00:19s\n",
      "epoch 10 | loss: 0.50081 | val_0_auc: 0.85557 |  0:00:21s\n",
      "epoch 11 | loss: 0.45425 | val_0_auc: 0.87386 |  0:00:22s\n",
      "epoch 12 | loss: 0.46047 | val_0_auc: 0.89167 |  0:00:24s\n",
      "epoch 13 | loss: 0.54238 | val_0_auc: 0.86518 |  0:00:26s\n",
      "epoch 14 | loss: 0.4742  | val_0_auc: 0.88898 |  0:00:28s\n",
      "epoch 15 | loss: 0.42332 | val_0_auc: 0.88939 |  0:00:30s\n",
      "epoch 16 | loss: 0.40642 | val_0_auc: 0.88869 |  0:00:32s\n",
      "epoch 17 | loss: 0.41224 | val_0_auc: 0.89993 |  0:00:34s\n",
      "epoch 18 | loss: 0.43126 | val_0_auc: 0.89082 |  0:00:36s\n",
      "epoch 19 | loss: 0.43028 | val_0_auc: 0.8894  |  0:00:38s\n",
      "epoch 20 | loss: 0.42558 | val_0_auc: 0.88825 |  0:00:39s\n",
      "epoch 21 | loss: 0.42894 | val_0_auc: 0.89073 |  0:00:41s\n",
      "epoch 22 | loss: 0.42224 | val_0_auc: 0.89432 |  0:00:43s\n",
      "epoch 23 | loss: 0.40282 | val_0_auc: 0.89083 |  0:00:45s\n",
      "epoch 24 | loss: 0.38957 | val_0_auc: 0.90006 |  0:00:47s\n",
      "epoch 25 | loss: 0.39613 | val_0_auc: 0.91473 |  0:00:49s\n",
      "epoch 26 | loss: 0.38073 | val_0_auc: 0.92075 |  0:00:51s\n",
      "epoch 27 | loss: 0.37264 | val_0_auc: 0.91595 |  0:00:53s\n",
      "epoch 28 | loss: 0.38327 | val_0_auc: 0.92179 |  0:00:55s\n",
      "epoch 29 | loss: 0.36411 | val_0_auc: 0.92326 |  0:00:57s\n",
      "epoch 30 | loss: 0.36189 | val_0_auc: 0.91056 |  0:00:58s\n",
      "epoch 31 | loss: 0.37244 | val_0_auc: 0.91357 |  0:01:00s\n",
      "epoch 32 | loss: 0.40573 | val_0_auc: 0.91611 |  0:01:02s\n",
      "epoch 33 | loss: 0.41909 | val_0_auc: 0.92748 |  0:01:04s\n",
      "epoch 34 | loss: 0.36817 | val_0_auc: 0.9231  |  0:01:06s\n",
      "epoch 35 | loss: 0.36154 | val_0_auc: 0.92973 |  0:01:08s\n",
      "epoch 36 | loss: 0.35101 | val_0_auc: 0.9409  |  0:01:10s\n",
      "epoch 37 | loss: 0.34157 | val_0_auc: 0.93723 |  0:01:12s\n",
      "epoch 38 | loss: 0.33525 | val_0_auc: 0.94544 |  0:01:14s\n",
      "epoch 39 | loss: 0.31342 | val_0_auc: 0.94186 |  0:01:16s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.94544\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.94543577  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 76.8536\n",
      "Function value obtained: -0.9454\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6212075060412214, 'lambda_sparse': 0.06941364086313698, 'n_steps': 5, 'n_a': 8, 'n_d': 8, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.91861 | val_0_auc: 0.67241 |  0:00:00s\n",
      "epoch 1  | loss: 0.67673 | val_0_auc: 0.74669 |  0:00:01s\n",
      "epoch 2  | loss: 0.61718 | val_0_auc: 0.78501 |  0:00:02s\n",
      "epoch 3  | loss: 0.57695 | val_0_auc: 0.79799 |  0:00:03s\n",
      "epoch 4  | loss: 0.54368 | val_0_auc: 0.80706 |  0:00:04s\n",
      "epoch 5  | loss: 0.53835 | val_0_auc: 0.82013 |  0:00:05s\n",
      "epoch 6  | loss: 0.50002 | val_0_auc: 0.84131 |  0:00:06s\n",
      "epoch 7  | loss: 0.47864 | val_0_auc: 0.86202 |  0:00:07s\n",
      "epoch 8  | loss: 0.47551 | val_0_auc: 0.84455 |  0:00:07s\n",
      "epoch 9  | loss: 0.4939  | val_0_auc: 0.86247 |  0:00:08s\n",
      "epoch 10 | loss: 0.49079 | val_0_auc: 0.86965 |  0:00:09s\n",
      "epoch 11 | loss: 0.48628 | val_0_auc: 0.86662 |  0:00:10s\n",
      "epoch 12 | loss: 0.47091 | val_0_auc: 0.87715 |  0:00:11s\n",
      "epoch 13 | loss: 0.45958 | val_0_auc: 0.90138 |  0:00:12s\n",
      "epoch 14 | loss: 0.4599  | val_0_auc: 0.88724 |  0:00:13s\n",
      "epoch 15 | loss: 0.44947 | val_0_auc: 0.8815  |  0:00:14s\n",
      "epoch 16 | loss: 0.43953 | val_0_auc: 0.89704 |  0:00:14s\n",
      "epoch 17 | loss: 0.43834 | val_0_auc: 0.89041 |  0:00:15s\n",
      "epoch 18 | loss: 0.41911 | val_0_auc: 0.91489 |  0:00:16s\n",
      "epoch 19 | loss: 0.38962 | val_0_auc: 0.91272 |  0:00:17s\n",
      "epoch 20 | loss: 0.38539 | val_0_auc: 0.90901 |  0:00:18s\n",
      "epoch 21 | loss: 0.38564 | val_0_auc: 0.9055  |  0:00:19s\n",
      "epoch 22 | loss: 0.3753  | val_0_auc: 0.90761 |  0:00:20s\n",
      "epoch 23 | loss: 0.37147 | val_0_auc: 0.92342 |  0:00:21s\n",
      "epoch 24 | loss: 0.36479 | val_0_auc: 0.91713 |  0:00:21s\n",
      "epoch 25 | loss: 0.36395 | val_0_auc: 0.91461 |  0:00:22s\n",
      "epoch 26 | loss: 0.35049 | val_0_auc: 0.92966 |  0:00:23s\n",
      "epoch 27 | loss: 0.33341 | val_0_auc: 0.94065 |  0:00:24s\n",
      "epoch 28 | loss: 0.33339 | val_0_auc: 0.93649 |  0:00:25s\n",
      "epoch 29 | loss: 0.32566 | val_0_auc: 0.95152 |  0:00:26s\n",
      "epoch 30 | loss: 0.31843 | val_0_auc: 0.94731 |  0:00:27s\n",
      "epoch 31 | loss: 0.31238 | val_0_auc: 0.95276 |  0:00:28s\n",
      "epoch 32 | loss: 0.30285 | val_0_auc: 0.94811 |  0:00:29s\n",
      "epoch 33 | loss: 0.30278 | val_0_auc: 0.95323 |  0:00:29s\n",
      "epoch 34 | loss: 0.30553 | val_0_auc: 0.95932 |  0:00:30s\n",
      "epoch 35 | loss: 0.30147 | val_0_auc: 0.9548  |  0:00:31s\n",
      "epoch 36 | loss: 0.27892 | val_0_auc: 0.95602 |  0:00:32s\n",
      "epoch 37 | loss: 0.27898 | val_0_auc: 0.95857 |  0:00:33s\n",
      "epoch 38 | loss: 0.27822 | val_0_auc: 0.96    |  0:00:34s\n",
      "epoch 39 | loss: 0.27988 | val_0_auc: 0.9614  |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.9614\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96139915  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 35.3766\n",
      "Function value obtained: -0.9614\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1228572014540585, 'lambda_sparse': 0.05730794499467639, 'n_steps': 6, 'n_a': 8, 'n_d': 8, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.88316 | val_0_auc: 0.56043 |  0:00:00s\n",
      "epoch 1  | loss: 0.71092 | val_0_auc: 0.6964  |  0:00:02s\n",
      "epoch 2  | loss: 0.65303 | val_0_auc: 0.72771 |  0:00:03s\n",
      "epoch 3  | loss: 0.6082  | val_0_auc: 0.7898  |  0:00:04s\n",
      "epoch 4  | loss: 0.56055 | val_0_auc: 0.8411  |  0:00:05s\n",
      "epoch 5  | loss: 0.50165 | val_0_auc: 0.87054 |  0:00:06s\n",
      "epoch 6  | loss: 0.46102 | val_0_auc: 0.89428 |  0:00:07s\n",
      "epoch 7  | loss: 0.42199 | val_0_auc: 0.91375 |  0:00:08s\n",
      "epoch 8  | loss: 0.39848 | val_0_auc: 0.92822 |  0:00:09s\n",
      "epoch 9  | loss: 0.36771 | val_0_auc: 0.93165 |  0:00:10s\n",
      "epoch 10 | loss: 0.35727 | val_0_auc: 0.93708 |  0:00:11s\n",
      "epoch 11 | loss: 0.34974 | val_0_auc: 0.95414 |  0:00:12s\n",
      "epoch 12 | loss: 0.32757 | val_0_auc: 0.95722 |  0:00:13s\n",
      "epoch 13 | loss: 0.31436 | val_0_auc: 0.95515 |  0:00:14s\n",
      "epoch 14 | loss: 0.2986  | val_0_auc: 0.96077 |  0:00:15s\n",
      "epoch 15 | loss: 0.30224 | val_0_auc: 0.96187 |  0:00:16s\n",
      "epoch 16 | loss: 0.28362 | val_0_auc: 0.96389 |  0:00:17s\n",
      "epoch 17 | loss: 0.2733  | val_0_auc: 0.96281 |  0:00:18s\n",
      "epoch 18 | loss: 0.26492 | val_0_auc: 0.96988 |  0:00:19s\n",
      "epoch 19 | loss: 0.2555  | val_0_auc: 0.97767 |  0:00:20s\n",
      "epoch 20 | loss: 0.24012 | val_0_auc: 0.97823 |  0:00:21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 0.23629 | val_0_auc: 0.97261 |  0:00:22s\n",
      "epoch 22 | loss: 0.22515 | val_0_auc: 0.97717 |  0:00:23s\n",
      "epoch 23 | loss: 0.23708 | val_0_auc: 0.97228 |  0:00:24s\n",
      "epoch 24 | loss: 0.21711 | val_0_auc: 0.9799  |  0:00:25s\n",
      "epoch 25 | loss: 0.22802 | val_0_auc: 0.97924 |  0:00:26s\n",
      "epoch 26 | loss: 0.21418 | val_0_auc: 0.97314 |  0:00:27s\n",
      "epoch 27 | loss: 0.22252 | val_0_auc: 0.97315 |  0:00:28s\n",
      "epoch 28 | loss: 0.20663 | val_0_auc: 0.97721 |  0:00:29s\n",
      "epoch 29 | loss: 0.21096 | val_0_auc: 0.97982 |  0:00:30s\n",
      "epoch 30 | loss: 0.21941 | val_0_auc: 0.97909 |  0:00:31s\n",
      "epoch 31 | loss: 0.20672 | val_0_auc: 0.98211 |  0:00:32s\n",
      "epoch 32 | loss: 0.21196 | val_0_auc: 0.98249 |  0:00:33s\n",
      "epoch 33 | loss: 0.21791 | val_0_auc: 0.98594 |  0:00:34s\n",
      "epoch 34 | loss: 0.20132 | val_0_auc: 0.97649 |  0:00:35s\n",
      "epoch 35 | loss: 0.21018 | val_0_auc: 0.98793 |  0:00:36s\n",
      "epoch 36 | loss: 0.20602 | val_0_auc: 0.98507 |  0:00:37s\n",
      "epoch 37 | loss: 0.19781 | val_0_auc: 0.98575 |  0:00:38s\n",
      "epoch 38 | loss: 0.20213 | val_0_auc: 0.98576 |  0:00:39s\n",
      "epoch 39 | loss: 0.19001 | val_0_auc: 0.9878  |  0:00:40s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.98793\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98792891  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 40.4457\n",
      "Function value obtained: -0.9879\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.591540638363211, 'lambda_sparse': 0.035871400703219376, 'n_steps': 6, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.39632 | val_0_auc: 0.76387 |  0:00:01s\n",
      "epoch 1  | loss: 0.84913 | val_0_auc: 0.82131 |  0:00:03s\n",
      "epoch 2  | loss: 0.74252 | val_0_auc: 0.85622 |  0:00:05s\n",
      "epoch 3  | loss: 0.58675 | val_0_auc: 0.83912 |  0:00:07s\n",
      "epoch 4  | loss: 0.52205 | val_0_auc: 0.86476 |  0:00:09s\n",
      "epoch 5  | loss: 0.48787 | val_0_auc: 0.85192 |  0:00:11s\n",
      "epoch 6  | loss: 0.44365 | val_0_auc: 0.88306 |  0:00:13s\n",
      "epoch 7  | loss: 0.43662 | val_0_auc: 0.88736 |  0:00:15s\n",
      "epoch 8  | loss: 0.40582 | val_0_auc: 0.90061 |  0:00:17s\n",
      "epoch 9  | loss: 0.37793 | val_0_auc: 0.90416 |  0:00:19s\n",
      "epoch 10 | loss: 0.40874 | val_0_auc: 0.92274 |  0:00:21s\n",
      "epoch 11 | loss: 0.37372 | val_0_auc: 0.91407 |  0:00:23s\n",
      "epoch 12 | loss: 0.35567 | val_0_auc: 0.94097 |  0:00:25s\n",
      "epoch 13 | loss: 0.36065 | val_0_auc: 0.93566 |  0:00:27s\n",
      "epoch 14 | loss: 0.33848 | val_0_auc: 0.9379  |  0:00:29s\n",
      "epoch 15 | loss: 0.34573 | val_0_auc: 0.94055 |  0:00:30s\n",
      "epoch 16 | loss: 0.33227 | val_0_auc: 0.93707 |  0:00:32s\n",
      "epoch 17 | loss: 0.35354 | val_0_auc: 0.94402 |  0:00:34s\n",
      "epoch 18 | loss: 0.30772 | val_0_auc: 0.95576 |  0:00:36s\n",
      "epoch 19 | loss: 0.30236 | val_0_auc: 0.94685 |  0:00:38s\n",
      "epoch 20 | loss: 0.27516 | val_0_auc: 0.96029 |  0:00:40s\n",
      "epoch 21 | loss: 0.27394 | val_0_auc: 0.95832 |  0:00:42s\n",
      "epoch 22 | loss: 0.29111 | val_0_auc: 0.96353 |  0:00:44s\n",
      "epoch 23 | loss: 0.35079 | val_0_auc: 0.9426  |  0:00:46s\n",
      "epoch 24 | loss: 0.33427 | val_0_auc: 0.96402 |  0:00:48s\n",
      "epoch 25 | loss: 0.28658 | val_0_auc: 0.967   |  0:00:50s\n",
      "epoch 26 | loss: 0.28476 | val_0_auc: 0.94638 |  0:00:52s\n",
      "epoch 27 | loss: 0.28259 | val_0_auc: 0.95619 |  0:00:54s\n",
      "epoch 28 | loss: 0.27289 | val_0_auc: 0.95768 |  0:00:55s\n",
      "epoch 29 | loss: 0.25779 | val_0_auc: 0.96008 |  0:00:57s\n",
      "epoch 30 | loss: 0.25308 | val_0_auc: 0.95962 |  0:00:59s\n",
      "epoch 31 | loss: 0.24728 | val_0_auc: 0.96486 |  0:01:01s\n",
      "epoch 32 | loss: 0.23698 | val_0_auc: 0.96637 |  0:01:03s\n",
      "epoch 33 | loss: 0.25654 | val_0_auc: 0.96984 |  0:01:05s\n",
      "epoch 34 | loss: 0.26596 | val_0_auc: 0.97005 |  0:01:07s\n",
      "epoch 35 | loss: 0.25337 | val_0_auc: 0.97388 |  0:01:09s\n",
      "epoch 36 | loss: 0.24295 | val_0_auc: 0.96241 |  0:01:11s\n",
      "epoch 37 | loss: 0.23838 | val_0_auc: 0.9631  |  0:01:13s\n",
      "epoch 38 | loss: 0.24512 | val_0_auc: 0.97028 |  0:01:15s\n",
      "epoch 39 | loss: 0.23286 | val_0_auc: 0.97414 |  0:01:17s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.97414\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9741421  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 77.9695\n",
      "Function value obtained: -0.9741\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.145542977086393, 'lambda_sparse': 0.02177617041468413, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.90451 | val_0_auc: 0.79528 |  0:00:01s\n",
      "epoch 1  | loss: 0.53709 | val_0_auc: 0.83536 |  0:00:02s\n",
      "epoch 2  | loss: 0.48241 | val_0_auc: 0.8878  |  0:00:03s\n",
      "epoch 3  | loss: 0.4238  | val_0_auc: 0.89969 |  0:00:05s\n",
      "epoch 4  | loss: 0.38595 | val_0_auc: 0.91832 |  0:00:06s\n",
      "epoch 5  | loss: 0.35642 | val_0_auc: 0.93796 |  0:00:07s\n",
      "epoch 6  | loss: 0.32488 | val_0_auc: 0.94541 |  0:00:09s\n",
      "epoch 7  | loss: 0.30397 | val_0_auc: 0.95217 |  0:00:10s\n",
      "epoch 8  | loss: 0.27765 | val_0_auc: 0.96635 |  0:00:11s\n",
      "epoch 9  | loss: 0.25508 | val_0_auc: 0.96512 |  0:00:12s\n",
      "epoch 10 | loss: 0.24745 | val_0_auc: 0.97258 |  0:00:14s\n",
      "epoch 11 | loss: 0.24313 | val_0_auc: 0.98136 |  0:00:15s\n",
      "epoch 12 | loss: 0.2232  | val_0_auc: 0.97893 |  0:00:16s\n",
      "epoch 13 | loss: 0.22282 | val_0_auc: 0.98379 |  0:00:18s\n",
      "epoch 14 | loss: 0.2115  | val_0_auc: 0.98601 |  0:00:19s\n",
      "epoch 15 | loss: 0.19939 | val_0_auc: 0.98414 |  0:00:20s\n",
      "epoch 16 | loss: 0.19709 | val_0_auc: 0.98398 |  0:00:21s\n",
      "epoch 17 | loss: 0.20795 | val_0_auc: 0.98668 |  0:00:23s\n",
      "epoch 18 | loss: 0.18743 | val_0_auc: 0.98898 |  0:00:24s\n",
      "epoch 19 | loss: 0.17731 | val_0_auc: 0.98696 |  0:00:25s\n",
      "epoch 20 | loss: 0.18049 | val_0_auc: 0.98453 |  0:00:26s\n",
      "epoch 21 | loss: 0.18584 | val_0_auc: 0.98827 |  0:00:28s\n",
      "epoch 22 | loss: 0.17346 | val_0_auc: 0.9753  |  0:00:29s\n",
      "epoch 23 | loss: 0.17036 | val_0_auc: 0.99051 |  0:00:30s\n",
      "epoch 24 | loss: 0.17261 | val_0_auc: 0.98926 |  0:00:32s\n",
      "epoch 25 | loss: 0.16674 | val_0_auc: 0.99171 |  0:00:33s\n",
      "epoch 26 | loss: 0.16857 | val_0_auc: 0.99419 |  0:00:34s\n",
      "epoch 27 | loss: 0.16514 | val_0_auc: 0.98541 |  0:00:35s\n",
      "epoch 28 | loss: 0.17596 | val_0_auc: 0.98784 |  0:00:37s\n",
      "epoch 29 | loss: 0.1885  | val_0_auc: 0.98729 |  0:00:38s\n",
      "epoch 30 | loss: 0.16512 | val_0_auc: 0.98561 |  0:00:39s\n",
      "epoch 31 | loss: 0.16741 | val_0_auc: 0.98982 |  0:00:41s\n",
      "epoch 32 | loss: 0.15997 | val_0_auc: 0.98769 |  0:00:42s\n",
      "epoch 33 | loss: 0.14644 | val_0_auc: 0.99403 |  0:00:43s\n",
      "epoch 34 | loss: 0.1555  | val_0_auc: 0.99434 |  0:00:44s\n",
      "epoch 35 | loss: 0.14868 | val_0_auc: 0.99242 |  0:00:46s\n",
      "epoch 36 | loss: 0.15258 | val_0_auc: 0.98949 |  0:00:47s\n",
      "epoch 37 | loss: 0.1679  | val_0_auc: 0.99454 |  0:00:48s\n",
      "epoch 38 | loss: 0.14907 | val_0_auc: 0.9924  |  0:00:50s\n",
      "epoch 39 | loss: 0.15952 | val_0_auc: 0.99462 |  0:00:51s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99462\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99461594  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 51.7765\n",
      "Function value obtained: -0.9946\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8636795620786435, 'lambda_sparse': 0.06531026784977459, 'n_steps': 8, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.77944 | val_0_auc: 0.62985 |  0:00:01s\n",
      "epoch 1  | loss: 0.84912 | val_0_auc: 0.73881 |  0:00:03s\n",
      "epoch 2  | loss: 0.67033 | val_0_auc: 0.7912  |  0:00:05s\n",
      "epoch 3  | loss: 0.67007 | val_0_auc: 0.80795 |  0:00:06s\n",
      "epoch 4  | loss: 1.00018 | val_0_auc: 0.75457 |  0:00:08s\n",
      "epoch 5  | loss: 0.74833 | val_0_auc: 0.81601 |  0:00:10s\n",
      "epoch 6  | loss: 0.60892 | val_0_auc: 0.80167 |  0:00:12s\n",
      "epoch 7  | loss: 0.77607 | val_0_auc: 0.8565  |  0:00:13s\n",
      "epoch 8  | loss: 0.70989 | val_0_auc: 0.81399 |  0:00:15s\n",
      "epoch 9  | loss: 0.57295 | val_0_auc: 0.8496  |  0:00:17s\n",
      "epoch 10 | loss: 0.49708 | val_0_auc: 0.84195 |  0:00:19s\n",
      "epoch 11 | loss: 0.48503 | val_0_auc: 0.86435 |  0:00:20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.48047 | val_0_auc: 0.85811 |  0:00:22s\n",
      "epoch 13 | loss: 0.46011 | val_0_auc: 0.87828 |  0:00:24s\n",
      "epoch 14 | loss: 0.4661  | val_0_auc: 0.85844 |  0:00:25s\n",
      "epoch 15 | loss: 0.45036 | val_0_auc: 0.8836  |  0:00:27s\n",
      "epoch 16 | loss: 0.46154 | val_0_auc: 0.8857  |  0:00:29s\n",
      "epoch 17 | loss: 0.43154 | val_0_auc: 0.87947 |  0:00:31s\n",
      "epoch 18 | loss: 0.42673 | val_0_auc: 0.89143 |  0:00:32s\n",
      "epoch 19 | loss: 0.41995 | val_0_auc: 0.89024 |  0:00:34s\n",
      "epoch 20 | loss: 0.40732 | val_0_auc: 0.91395 |  0:00:36s\n",
      "epoch 21 | loss: 0.40598 | val_0_auc: 0.90515 |  0:00:37s\n",
      "epoch 22 | loss: 0.37449 | val_0_auc: 0.91748 |  0:00:39s\n",
      "epoch 23 | loss: 0.38604 | val_0_auc: 0.92828 |  0:00:41s\n",
      "epoch 24 | loss: 0.3663  | val_0_auc: 0.91998 |  0:00:43s\n",
      "epoch 25 | loss: 0.36864 | val_0_auc: 0.93292 |  0:00:44s\n",
      "epoch 26 | loss: 0.36642 | val_0_auc: 0.94025 |  0:00:46s\n",
      "epoch 27 | loss: 0.35414 | val_0_auc: 0.94009 |  0:00:48s\n",
      "epoch 28 | loss: 0.35457 | val_0_auc: 0.94246 |  0:00:49s\n",
      "epoch 29 | loss: 0.33706 | val_0_auc: 0.94076 |  0:00:51s\n",
      "epoch 30 | loss: 0.37699 | val_0_auc: 0.92865 |  0:00:53s\n",
      "epoch 31 | loss: 0.34214 | val_0_auc: 0.93961 |  0:00:55s\n",
      "epoch 32 | loss: 0.32725 | val_0_auc: 0.95144 |  0:00:56s\n",
      "epoch 33 | loss: 0.31044 | val_0_auc: 0.94331 |  0:00:58s\n",
      "epoch 34 | loss: 0.31392 | val_0_auc: 0.9417  |  0:01:00s\n",
      "epoch 35 | loss: 0.30743 | val_0_auc: 0.94943 |  0:01:02s\n",
      "epoch 36 | loss: 0.30139 | val_0_auc: 0.95357 |  0:01:03s\n",
      "epoch 37 | loss: 0.28515 | val_0_auc: 0.9525  |  0:01:05s\n",
      "epoch 38 | loss: 0.28108 | val_0_auc: 0.95887 |  0:01:07s\n",
      "epoch 39 | loss: 0.2726  | val_0_auc: 0.95699 |  0:01:09s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.95887\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95887277  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 69.8017\n",
      "Function value obtained: -0.9589\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7455722971463405, 'lambda_sparse': 0.030809612658016232, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.74346 | val_0_auc: 0.71098 |  0:00:00s\n",
      "epoch 1  | loss: 0.59346 | val_0_auc: 0.76579 |  0:00:01s\n",
      "epoch 2  | loss: 0.53288 | val_0_auc: 0.81328 |  0:00:02s\n",
      "epoch 3  | loss: 0.49312 | val_0_auc: 0.83288 |  0:00:03s\n",
      "epoch 4  | loss: 0.48042 | val_0_auc: 0.84539 |  0:00:04s\n",
      "epoch 5  | loss: 0.46306 | val_0_auc: 0.85543 |  0:00:05s\n",
      "epoch 6  | loss: 0.44654 | val_0_auc: 0.86899 |  0:00:06s\n",
      "epoch 7  | loss: 0.43387 | val_0_auc: 0.87064 |  0:00:07s\n",
      "epoch 8  | loss: 0.42038 | val_0_auc: 0.88837 |  0:00:07s\n",
      "epoch 9  | loss: 0.39999 | val_0_auc: 0.88848 |  0:00:08s\n",
      "epoch 10 | loss: 0.38909 | val_0_auc: 0.90607 |  0:00:09s\n",
      "epoch 11 | loss: 0.37577 | val_0_auc: 0.90771 |  0:00:10s\n",
      "epoch 12 | loss: 0.35775 | val_0_auc: 0.92184 |  0:00:11s\n",
      "epoch 13 | loss: 0.33466 | val_0_auc: 0.93274 |  0:00:12s\n",
      "epoch 14 | loss: 0.3033  | val_0_auc: 0.94639 |  0:00:13s\n",
      "epoch 15 | loss: 0.30897 | val_0_auc: 0.94741 |  0:00:14s\n",
      "epoch 16 | loss: 0.27794 | val_0_auc: 0.95273 |  0:00:15s\n",
      "epoch 17 | loss: 0.26848 | val_0_auc: 0.96003 |  0:00:15s\n",
      "epoch 18 | loss: 0.2526  | val_0_auc: 0.96245 |  0:00:16s\n",
      "epoch 19 | loss: 0.23306 | val_0_auc: 0.96937 |  0:00:17s\n",
      "epoch 20 | loss: 0.21626 | val_0_auc: 0.97295 |  0:00:18s\n",
      "epoch 21 | loss: 0.20379 | val_0_auc: 0.97911 |  0:00:19s\n",
      "epoch 22 | loss: 0.1997  | val_0_auc: 0.98243 |  0:00:20s\n",
      "epoch 23 | loss: 0.18175 | val_0_auc: 0.98494 |  0:00:21s\n",
      "epoch 24 | loss: 0.21075 | val_0_auc: 0.98846 |  0:00:22s\n",
      "epoch 25 | loss: 0.19248 | val_0_auc: 0.97754 |  0:00:23s\n",
      "epoch 26 | loss: 0.19493 | val_0_auc: 0.98526 |  0:00:24s\n",
      "epoch 27 | loss: 0.18791 | val_0_auc: 0.98066 |  0:00:24s\n",
      "epoch 28 | loss: 0.18633 | val_0_auc: 0.98399 |  0:00:25s\n",
      "epoch 29 | loss: 0.17719 | val_0_auc: 0.99102 |  0:00:26s\n",
      "epoch 30 | loss: 0.18042 | val_0_auc: 0.98717 |  0:00:27s\n",
      "epoch 31 | loss: 0.1757  | val_0_auc: 0.98733 |  0:00:28s\n",
      "epoch 32 | loss: 0.17593 | val_0_auc: 0.98736 |  0:00:29s\n",
      "epoch 33 | loss: 0.17501 | val_0_auc: 0.9878  |  0:00:30s\n",
      "epoch 34 | loss: 0.16644 | val_0_auc: 0.99089 |  0:00:31s\n",
      "epoch 35 | loss: 0.16419 | val_0_auc: 0.98949 |  0:00:31s\n",
      "epoch 36 | loss: 0.1576  | val_0_auc: 0.99139 |  0:00:32s\n",
      "epoch 37 | loss: 0.14491 | val_0_auc: 0.9877  |  0:00:33s\n",
      "epoch 38 | loss: 0.15484 | val_0_auc: 0.99118 |  0:00:34s\n",
      "epoch 39 | loss: 0.15722 | val_0_auc: 0.99058 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99139\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99138995  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 35.7403\n",
      "Function value obtained: -0.9914\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6708121649292025, 'lambda_sparse': 0.02885748295697321, 'n_steps': 6, 'n_a': 8, 'n_d': 8, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.8817  | val_0_auc: 0.53939 |  0:00:01s\n",
      "epoch 1  | loss: 0.70774 | val_0_auc: 0.64981 |  0:00:02s\n",
      "epoch 2  | loss: 0.6512  | val_0_auc: 0.66462 |  0:00:03s\n",
      "epoch 3  | loss: 0.62512 | val_0_auc: 0.70401 |  0:00:04s\n",
      "epoch 4  | loss: 0.58532 | val_0_auc: 0.76886 |  0:00:05s\n",
      "epoch 5  | loss: 0.56082 | val_0_auc: 0.75576 |  0:00:06s\n",
      "epoch 6  | loss: 0.54303 | val_0_auc: 0.78363 |  0:00:07s\n",
      "epoch 7  | loss: 0.52059 | val_0_auc: 0.82701 |  0:00:08s\n",
      "epoch 8  | loss: 0.52178 | val_0_auc: 0.81403 |  0:00:09s\n",
      "epoch 9  | loss: 0.51284 | val_0_auc: 0.81864 |  0:00:10s\n",
      "epoch 10 | loss: 0.50114 | val_0_auc: 0.83621 |  0:00:11s\n",
      "epoch 11 | loss: 0.49798 | val_0_auc: 0.83689 |  0:00:12s\n",
      "epoch 12 | loss: 0.48631 | val_0_auc: 0.84831 |  0:00:13s\n",
      "epoch 13 | loss: 0.48475 | val_0_auc: 0.85971 |  0:00:14s\n",
      "epoch 14 | loss: 0.47137 | val_0_auc: 0.865   |  0:00:15s\n",
      "epoch 15 | loss: 0.46622 | val_0_auc: 0.85936 |  0:00:16s\n",
      "epoch 16 | loss: 0.4537  | val_0_auc: 0.87404 |  0:00:17s\n",
      "epoch 17 | loss: 0.44422 | val_0_auc: 0.8755  |  0:00:18s\n",
      "epoch 18 | loss: 0.43593 | val_0_auc: 0.87254 |  0:00:19s\n",
      "epoch 19 | loss: 0.45223 | val_0_auc: 0.86481 |  0:00:20s\n",
      "epoch 20 | loss: 0.45617 | val_0_auc: 0.86599 |  0:00:21s\n",
      "epoch 21 | loss: 0.44945 | val_0_auc: 0.84639 |  0:00:22s\n",
      "epoch 22 | loss: 0.43404 | val_0_auc: 0.88252 |  0:00:23s\n",
      "epoch 23 | loss: 0.4147  | val_0_auc: 0.89841 |  0:00:24s\n",
      "epoch 24 | loss: 0.39655 | val_0_auc: 0.91157 |  0:00:25s\n",
      "epoch 25 | loss: 0.38808 | val_0_auc: 0.92644 |  0:00:26s\n",
      "epoch 26 | loss: 0.36205 | val_0_auc: 0.92498 |  0:00:27s\n",
      "epoch 27 | loss: 0.36311 | val_0_auc: 0.93056 |  0:00:28s\n",
      "epoch 28 | loss: 0.36512 | val_0_auc: 0.91146 |  0:00:29s\n",
      "epoch 29 | loss: 0.36901 | val_0_auc: 0.93879 |  0:00:30s\n",
      "epoch 30 | loss: 0.3512  | val_0_auc: 0.935   |  0:00:31s\n",
      "epoch 31 | loss: 0.33567 | val_0_auc: 0.93773 |  0:00:32s\n",
      "epoch 32 | loss: 0.33156 | val_0_auc: 0.94235 |  0:00:33s\n",
      "epoch 33 | loss: 0.31688 | val_0_auc: 0.93432 |  0:00:34s\n",
      "epoch 34 | loss: 0.30232 | val_0_auc: 0.93434 |  0:00:35s\n",
      "epoch 35 | loss: 0.31124 | val_0_auc: 0.95353 |  0:00:36s\n",
      "epoch 36 | loss: 0.29464 | val_0_auc: 0.94868 |  0:00:37s\n",
      "epoch 37 | loss: 0.28596 | val_0_auc: 0.95341 |  0:00:38s\n",
      "epoch 38 | loss: 0.28427 | val_0_auc: 0.95789 |  0:00:39s\n",
      "epoch 39 | loss: 0.27694 | val_0_auc: 0.95065 |  0:00:40s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.95789\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95789183  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 40.5259\n",
      "Function value obtained: -0.9579\n",
      "Current minimum: -0.9950\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9080284222182393, 'lambda_sparse': 0.04873545194308182, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.86998 | val_0_auc: 0.67486 |  0:00:01s\n",
      "epoch 1  | loss: 0.63242 | val_0_auc: 0.77747 |  0:00:02s\n",
      "epoch 2  | loss: 0.55408 | val_0_auc: 0.83816 |  0:00:03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.52875 | val_0_auc: 0.81462 |  0:00:05s\n",
      "epoch 4  | loss: 0.51019 | val_0_auc: 0.82798 |  0:00:06s\n",
      "epoch 5  | loss: 0.47962 | val_0_auc: 0.86807 |  0:00:07s\n",
      "epoch 6  | loss: 0.47334 | val_0_auc: 0.8661  |  0:00:08s\n",
      "epoch 7  | loss: 0.44866 | val_0_auc: 0.87325 |  0:00:09s\n",
      "epoch 8  | loss: 0.44315 | val_0_auc: 0.87699 |  0:00:11s\n",
      "epoch 9  | loss: 0.43145 | val_0_auc: 0.88164 |  0:00:12s\n",
      "epoch 10 | loss: 0.42703 | val_0_auc: 0.88542 |  0:00:13s\n",
      "epoch 11 | loss: 0.43218 | val_0_auc: 0.87888 |  0:00:14s\n",
      "epoch 12 | loss: 0.42538 | val_0_auc: 0.89817 |  0:00:16s\n",
      "epoch 13 | loss: 0.41903 | val_0_auc: 0.88727 |  0:00:17s\n",
      "epoch 14 | loss: 0.44454 | val_0_auc: 0.89837 |  0:00:18s\n",
      "epoch 15 | loss: 0.41943 | val_0_auc: 0.90158 |  0:00:19s\n",
      "epoch 16 | loss: 0.40029 | val_0_auc: 0.90047 |  0:00:20s\n",
      "epoch 17 | loss: 0.38576 | val_0_auc: 0.91906 |  0:00:22s\n",
      "epoch 18 | loss: 0.3917  | val_0_auc: 0.91767 |  0:00:23s\n",
      "epoch 19 | loss: 0.38416 | val_0_auc: 0.91904 |  0:00:24s\n",
      "epoch 20 | loss: 0.36074 | val_0_auc: 0.92349 |  0:00:25s\n",
      "epoch 21 | loss: 0.3719  | val_0_auc: 0.92455 |  0:00:27s\n",
      "epoch 22 | loss: 0.36177 | val_0_auc: 0.92994 |  0:00:28s\n",
      "epoch 23 | loss: 0.35452 | val_0_auc: 0.9381  |  0:00:29s\n",
      "epoch 24 | loss: 0.32388 | val_0_auc: 0.94143 |  0:00:30s\n",
      "epoch 25 | loss: 0.31518 | val_0_auc: 0.94482 |  0:00:32s\n",
      "epoch 26 | loss: 0.30429 | val_0_auc: 0.95182 |  0:00:33s\n",
      "epoch 27 | loss: 0.30705 | val_0_auc: 0.96178 |  0:00:34s\n",
      "epoch 28 | loss: 0.29579 | val_0_auc: 0.9569  |  0:00:35s\n",
      "epoch 29 | loss: 0.2846  | val_0_auc: 0.94911 |  0:00:36s\n",
      "epoch 30 | loss: 0.28929 | val_0_auc: 0.95665 |  0:00:38s\n",
      "epoch 31 | loss: 0.27695 | val_0_auc: 0.94974 |  0:00:39s\n",
      "epoch 32 | loss: 0.27371 | val_0_auc: 0.96752 |  0:00:40s\n",
      "epoch 33 | loss: 0.24693 | val_0_auc: 0.96309 |  0:00:41s\n",
      "epoch 34 | loss: 0.24213 | val_0_auc: 0.96006 |  0:00:43s\n",
      "epoch 35 | loss: 0.23278 | val_0_auc: 0.96538 |  0:00:44s\n",
      "epoch 36 | loss: 0.22486 | val_0_auc: 0.97479 |  0:00:45s\n",
      "epoch 37 | loss: 0.2116  | val_0_auc: 0.97995 |  0:00:46s\n",
      "epoch 38 | loss: 0.21574 | val_0_auc: 0.9819  |  0:00:47s\n",
      "epoch 39 | loss: 0.20643 | val_0_auc: 0.98307 |  0:00:49s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.98307\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98307049  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 52.0472\n",
      "Function value obtained: -0.9831\n",
      "Current minimum: -0.9950\n",
      "STARTED: syn3\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4249484862518583, 'max_depth': 14, 'n_estimators': 896}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99992174  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.1026\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3813035051640128, 'max_depth': 6, 'n_estimators': 773}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99989684  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.0680\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.26471495742462636, 'max_depth': 5, 'n_estimators': 695}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99992707  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.0771\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29094499322423073, 'max_depth': 8, 'n_estimators': 342}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99991284  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.0940\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2728418466161798, 'max_depth': 4, 'n_estimators': 716}\n",
      "AUC:  0.99991107  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.0627\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4540837973031734, 'max_depth': 3, 'n_estimators': 315}\n",
      "AUC:  0.99996798  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.0364\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4206765156618807, 'max_depth': 6, 'n_estimators': 491}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99989506  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.0849\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4457821176633403, 'max_depth': 4, 'n_estimators': 902}\n",
      "AUC:  0.99988794  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.0681\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4370652058389529, 'max_depth': 6, 'n_estimators': 909}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99981502  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.0536\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.13919846273474043, 'max_depth': 13, 'n_estimators': 596}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99991284  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.0791\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48358925634392247, 'max_depth': 12, 'n_estimators': 962}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99993775  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0455\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1531559059746968, 'max_depth': 12, 'n_estimators': 273}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99984703  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.0719\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.24677867966635736, 'max_depth': 8, 'n_estimators': 890}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99985859  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.0539\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18247997249540035, 'max_depth': 10, 'n_estimators': 792}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99984881  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0612\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2735428955168469, 'max_depth': 14, 'n_estimators': 558}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99984703  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0536\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3062124887471891, 'max_depth': 12, 'n_estimators': 932}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99992707  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0541\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4735185767560224, 'max_depth': 8, 'n_estimators': 827}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99988439  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.0899\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14474617677164653, 'max_depth': 13, 'n_estimators': 812}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9998915  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.0664\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3316027922305174, 'max_depth': 5, 'n_estimators': 641}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99993775  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.1044\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.32086830471493877, 'max_depth': 5, 'n_estimators': 121}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99990039  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.0665\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.32615808784215156, 'max_depth': 6, 'n_estimators': 264}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99979723  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.0436\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29947842739778363, 'max_depth': 13, 'n_estimators': 548}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99985504  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.0518\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.11599607482096742, 'max_depth': 8, 'n_estimators': 171}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9998915  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.0813\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.12776501015790737, 'max_depth': 6, 'n_estimators': 196}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99985593  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0770\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.21624660777982915, 'max_depth': 12, 'n_estimators': 458}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99990929  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.0679\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4345340481907255, 'max_depth': 5, 'n_estimators': 206}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99990217  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.0680\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3377386597198311, 'max_depth': 14, 'n_estimators': 259}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9998666  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.0490\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43871993147971, 'max_depth': 9, 'n_estimators': 238}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99992174  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.0672\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3018492218463864, 'max_depth': 6, 'n_estimators': 811}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99990039  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.0520\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.35219299945852955, 'max_depth': 5, 'n_estimators': 160}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9998915  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 1.1141\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5707356904063647, 'max_depth': 14, 'n_estimators': 222}\n",
      "[02:06:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99987905  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.1994\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8843817624543578, 'max_depth': 10, 'n_estimators': 923}\n",
      "[02:06:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99989328  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.1402\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.42740297253596826, 'max_depth': 15, 'n_estimators': 697}\n",
      "[02:06:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99990395  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.2466\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6828976721800313, 'max_depth': 6, 'n_estimators': 387}\n",
      "[02:06:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99988794  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.1590\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -0.9999\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8880765022899698, 'max_depth': 4, 'n_estimators': 675}\n",
      "[02:06:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99998043  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.1506\n",
      "Function value obtained: -1.0000\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7068937199226752, 'max_depth': 9, 'n_estimators': 920}\n",
      "[02:06:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99989328  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.1753\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8029005435331039, 'max_depth': 15, 'n_estimators': 455}\n",
      "[02:06:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99991818  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.1641\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4828915334252911, 'max_depth': 4, 'n_estimators': 998}\n",
      "[02:06:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99989328  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.1841\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2316543250359194, 'max_depth': 14, 'n_estimators': 356}\n",
      "[02:06:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99988439  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.3441\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43430870803912025, 'max_depth': 12, 'n_estimators': 355}\n",
      "[02:06:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99979901  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.1129\n",
      "Function value obtained: -0.9998\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14410835104029135, 'max_depth': 9, 'n_estimators': 906}\n",
      "[02:06:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99990039  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.4991\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5699591373380657, 'max_depth': 11, 'n_estimators': 171}\n",
      "[02:06:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.9998666  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1914\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5387053724455791, 'max_depth': 9, 'n_estimators': 810}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:06:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99989861  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.2175\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8611101441183366, 'max_depth': 12, 'n_estimators': 355}\n",
      "[02:06:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99990929  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.1498\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3119129918844748, 'max_depth': 6, 'n_estimators': 669}\n",
      "[02:06:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99990751  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.2515\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6189015496285607, 'max_depth': 5, 'n_estimators': 467}\n",
      "[02:06:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99986126  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.1450\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6021815190882222, 'max_depth': 9, 'n_estimators': 334}\n",
      "[02:06:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99990217  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.1917\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7961372475619464, 'max_depth': 7, 'n_estimators': 174}\n",
      "[02:06:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99988261  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.1490\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4798688052740553, 'max_depth': 11, 'n_estimators': 834}\n",
      "[02:06:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99992174  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.2644\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.817500037852028, 'max_depth': 5, 'n_estimators': 497}\n",
      "[02:06:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99991107  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.1417\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4940834713981386, 'max_depth': 8, 'n_estimators': 980}\n",
      "[02:06:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99991284  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.2268\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43946482025077405, 'max_depth': 8, 'n_estimators': 221}\n",
      "[02:06:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99985415  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.1957\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7752254009105006, 'max_depth': 14, 'n_estimators': 854}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:06:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99990395  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.1787\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9772439680731393, 'max_depth': 10, 'n_estimators': 186}\n",
      "[02:06:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99993775  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.1513\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5745344671543213, 'max_depth': 8, 'n_estimators': 703}\n",
      "[02:06:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99987193  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.2037\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4222705105523177, 'max_depth': 13, 'n_estimators': 939}\n",
      "[02:06:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99986304  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.1867\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2457939722296526, 'max_depth': 11, 'n_estimators': 619}\n",
      "[02:06:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99991284  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.4184\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.15072434276539107, 'max_depth': 5, 'n_estimators': 739}\n",
      "[02:06:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99992885  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.6046\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.11480079286754727, 'max_depth': 4, 'n_estimators': 283}\n",
      "[02:06:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99988083  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.4263\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7932043562805929, 'max_depth': 14, 'n_estimators': 906}\n",
      "[02:06:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99991107  of boosting iteration \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.6115\n",
      "Function value obtained: -0.9999\n",
      "Current minimum: -1.0000\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1351826498116349, 'lambda_sparse': 0.08599987291406029, 'n_steps': 7, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.28649 | val_0_auc: 0.95855 |  0:00:02s\n",
      "epoch 1  | loss: 0.34493 | val_0_auc: 0.98176 |  0:00:04s\n",
      "epoch 2  | loss: 0.19497 | val_0_auc: 0.99387 |  0:00:06s\n",
      "epoch 3  | loss: 0.17376 | val_0_auc: 0.99633 |  0:00:08s\n",
      "epoch 4  | loss: 0.14356 | val_0_auc: 0.9981  |  0:00:10s\n",
      "epoch 5  | loss: 0.15219 | val_0_auc: 0.99702 |  0:00:13s\n",
      "epoch 6  | loss: 0.12527 | val_0_auc: 0.99806 |  0:00:15s\n",
      "epoch 7  | loss: 0.11835 | val_0_auc: 0.99796 |  0:00:17s\n",
      "epoch 8  | loss: 0.11542 | val_0_auc: 0.9983  |  0:00:19s\n",
      "epoch 9  | loss: 0.11532 | val_0_auc: 0.99891 |  0:00:21s\n",
      "epoch 10 | loss: 0.10231 | val_0_auc: 0.99842 |  0:00:24s\n",
      "epoch 11 | loss: 0.10109 | val_0_auc: 0.99786 |  0:00:26s\n",
      "epoch 12 | loss: 0.09391 | val_0_auc: 0.99867 |  0:00:28s\n",
      "epoch 13 | loss: 0.09512 | val_0_auc: 0.99887 |  0:00:30s\n",
      "epoch 14 | loss: 0.09018 | val_0_auc: 0.99849 |  0:00:32s\n",
      "epoch 15 | loss: 0.09195 | val_0_auc: 0.99899 |  0:00:35s\n",
      "epoch 16 | loss: 0.09447 | val_0_auc: 0.99853 |  0:00:37s\n",
      "epoch 17 | loss: 0.09182 | val_0_auc: 0.99885 |  0:00:39s\n",
      "epoch 18 | loss: 0.09316 | val_0_auc: 0.99857 |  0:00:41s\n",
      "epoch 19 | loss: 0.09139 | val_0_auc: 0.99749 |  0:00:43s\n",
      "epoch 20 | loss: 0.0855  | val_0_auc: 0.99905 |  0:00:46s\n",
      "epoch 21 | loss: 0.07459 | val_0_auc: 0.99872 |  0:00:48s\n",
      "epoch 22 | loss: 0.08468 | val_0_auc: 0.99862 |  0:00:50s\n",
      "epoch 23 | loss: 0.10379 | val_0_auc: 0.9989  |  0:00:52s\n",
      "epoch 24 | loss: 0.08172 | val_0_auc: 0.99934 |  0:00:54s\n",
      "epoch 25 | loss: 0.09208 | val_0_auc: 0.99934 |  0:00:56s\n",
      "epoch 26 | loss: 0.08323 | val_0_auc: 0.99801 |  0:00:59s\n",
      "epoch 27 | loss: 0.11235 | val_0_auc: 0.99747 |  0:01:01s\n",
      "epoch 28 | loss: 0.10048 | val_0_auc: 0.99857 |  0:01:03s\n",
      "epoch 29 | loss: 0.09556 | val_0_auc: 0.99815 |  0:01:05s\n",
      "epoch 30 | loss: 0.08533 | val_0_auc: 0.99853 |  0:01:07s\n",
      "epoch 31 | loss: 0.08446 | val_0_auc: 0.99924 |  0:01:09s\n",
      "epoch 32 | loss: 0.08937 | val_0_auc: 0.99884 |  0:01:12s\n",
      "epoch 33 | loss: 0.07503 | val_0_auc: 0.99949 |  0:01:14s\n",
      "epoch 34 | loss: 0.08223 | val_0_auc: 0.99925 |  0:01:16s\n",
      "epoch 35 | loss: 0.07497 | val_0_auc: 0.99921 |  0:01:18s\n",
      "epoch 36 | loss: 0.06604 | val_0_auc: 0.9995  |  0:01:20s\n",
      "epoch 37 | loss: 0.07075 | val_0_auc: 0.99941 |  0:01:23s\n",
      "epoch 38 | loss: 0.08302 | val_0_auc: 0.99942 |  0:01:25s\n",
      "epoch 39 | loss: 0.08539 | val_0_auc: 0.99928 |  0:01:27s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.9995\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99950019  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 88.4732\n",
      "Function value obtained: -0.9995\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5219390631263732, 'lambda_sparse': 0.05943070799815816, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.72691 | val_0_auc: 0.88989 |  0:00:01s\n",
      "epoch 1  | loss: 0.34737 | val_0_auc: 0.96732 |  0:00:02s\n",
      "epoch 2  | loss: 0.19932 | val_0_auc: 0.99129 |  0:00:03s\n",
      "epoch 3  | loss: 0.15695 | val_0_auc: 0.99329 |  0:00:04s\n",
      "epoch 4  | loss: 0.13199 | val_0_auc: 0.99514 |  0:00:06s\n",
      "epoch 5  | loss: 0.11992 | val_0_auc: 0.99718 |  0:00:07s\n",
      "epoch 6  | loss: 0.11514 | val_0_auc: 0.99496 |  0:00:08s\n",
      "epoch 7  | loss: 0.09976 | val_0_auc: 0.9962  |  0:00:09s\n",
      "epoch 8  | loss: 0.09395 | val_0_auc: 0.99717 |  0:00:11s\n",
      "epoch 9  | loss: 0.10113 | val_0_auc: 0.99738 |  0:00:12s\n",
      "epoch 10 | loss: 0.09295 | val_0_auc: 0.99694 |  0:00:13s\n",
      "epoch 11 | loss: 0.10784 | val_0_auc: 0.99709 |  0:00:14s\n",
      "epoch 12 | loss: 0.08774 | val_0_auc: 0.99786 |  0:00:15s\n",
      "epoch 13 | loss: 0.0941  | val_0_auc: 0.99768 |  0:00:17s\n",
      "epoch 14 | loss: 0.08375 | val_0_auc: 0.9976  |  0:00:18s\n",
      "epoch 15 | loss: 0.07629 | val_0_auc: 0.99825 |  0:00:19s\n",
      "epoch 16 | loss: 0.07898 | val_0_auc: 0.99689 |  0:00:20s\n",
      "epoch 17 | loss: 0.09439 | val_0_auc: 0.99713 |  0:00:22s\n",
      "epoch 18 | loss: 0.08315 | val_0_auc: 0.99814 |  0:00:23s\n",
      "epoch 19 | loss: 0.07806 | val_0_auc: 0.9984  |  0:00:24s\n",
      "epoch 20 | loss: 0.07326 | val_0_auc: 0.99846 |  0:00:25s\n",
      "epoch 21 | loss: 0.09042 | val_0_auc: 0.99874 |  0:00:26s\n",
      "epoch 22 | loss: 0.08146 | val_0_auc: 0.99823 |  0:00:28s\n",
      "epoch 23 | loss: 0.07199 | val_0_auc: 0.99871 |  0:00:29s\n",
      "epoch 24 | loss: 0.07975 | val_0_auc: 0.99816 |  0:00:30s\n",
      "epoch 25 | loss: 0.08334 | val_0_auc: 0.99827 |  0:00:31s\n",
      "epoch 26 | loss: 0.08608 | val_0_auc: 0.99827 |  0:00:33s\n",
      "epoch 27 | loss: 0.09007 | val_0_auc: 0.99881 |  0:00:34s\n",
      "epoch 28 | loss: 0.11471 | val_0_auc: 0.99832 |  0:00:35s\n",
      "epoch 29 | loss: 0.07787 | val_0_auc: 0.99771 |  0:00:36s\n",
      "epoch 30 | loss: 0.07443 | val_0_auc: 0.99863 |  0:00:37s\n",
      "epoch 31 | loss: 0.07781 | val_0_auc: 0.99823 |  0:00:39s\n",
      "epoch 32 | loss: 0.07529 | val_0_auc: 0.99872 |  0:00:40s\n",
      "epoch 33 | loss: 0.0797  | val_0_auc: 0.99754 |  0:00:41s\n",
      "epoch 34 | loss: 0.07344 | val_0_auc: 0.99819 |  0:00:42s\n",
      "epoch 35 | loss: 0.07374 | val_0_auc: 0.99791 |  0:00:43s\n",
      "epoch 36 | loss: 0.0719  | val_0_auc: 0.99824 |  0:00:45s\n",
      "epoch 37 | loss: 0.07814 | val_0_auc: 0.99791 |  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.99881\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99881361  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 46.7265\n",
      "Function value obtained: -0.9988\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3414177263645095, 'lambda_sparse': 0.03812149174698964, 'n_steps': 9, 'n_a': 64, 'n_d': 64, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.37058 | val_0_auc: 0.9027  |  0:00:02s\n",
      "epoch 1  | loss: 0.38902 | val_0_auc: 0.95977 |  0:00:05s\n",
      "epoch 2  | loss: 0.23481 | val_0_auc: 0.9849  |  0:00:08s\n",
      "epoch 3  | loss: 0.18619 | val_0_auc: 0.99259 |  0:00:11s\n",
      "epoch 4  | loss: 0.2184  | val_0_auc: 0.9909  |  0:00:14s\n",
      "epoch 5  | loss: 0.21635 | val_0_auc: 0.9957  |  0:00:17s\n",
      "epoch 6  | loss: 0.15087 | val_0_auc: 0.99249 |  0:00:20s\n",
      "epoch 7  | loss: 0.23722 | val_0_auc: 0.99358 |  0:00:23s\n",
      "epoch 8  | loss: 0.1541  | val_0_auc: 0.99426 |  0:00:26s\n",
      "epoch 9  | loss: 0.10935 | val_0_auc: 0.99628 |  0:00:29s\n",
      "epoch 10 | loss: 0.11349 | val_0_auc: 0.9957  |  0:00:32s\n",
      "epoch 11 | loss: 0.11769 | val_0_auc: 0.99231 |  0:00:35s\n",
      "epoch 12 | loss: 0.10658 | val_0_auc: 0.99577 |  0:00:37s\n",
      "epoch 13 | loss: 0.14266 | val_0_auc: 0.99658 |  0:00:40s\n",
      "epoch 14 | loss: 0.11172 | val_0_auc: 0.99666 |  0:00:43s\n",
      "epoch 15 | loss: 0.10611 | val_0_auc: 0.99636 |  0:00:46s\n",
      "epoch 16 | loss: 0.08657 | val_0_auc: 0.99692 |  0:00:49s\n",
      "epoch 17 | loss: 0.10465 | val_0_auc: 0.99732 |  0:00:52s\n",
      "epoch 18 | loss: 0.18577 | val_0_auc: 0.99699 |  0:00:54s\n",
      "epoch 19 | loss: 0.1238  | val_0_auc: 0.99511 |  0:00:57s\n",
      "epoch 20 | loss: 0.14614 | val_0_auc: 0.9975  |  0:01:00s\n",
      "epoch 21 | loss: 0.15016 | val_0_auc: 0.99716 |  0:01:03s\n",
      "epoch 22 | loss: 0.10749 | val_0_auc: 0.99849 |  0:01:06s\n",
      "epoch 23 | loss: 0.07847 | val_0_auc: 0.99719 |  0:01:09s\n",
      "epoch 24 | loss: 0.08711 | val_0_auc: 0.99894 |  0:01:12s\n",
      "epoch 25 | loss: 0.07751 | val_0_auc: 0.99794 |  0:01:15s\n",
      "epoch 26 | loss: 0.09664 | val_0_auc: 0.99818 |  0:01:18s\n",
      "epoch 27 | loss: 0.08946 | val_0_auc: 0.99851 |  0:01:20s\n",
      "epoch 28 | loss: 0.07989 | val_0_auc: 0.99711 |  0:01:23s\n",
      "epoch 29 | loss: 0.06792 | val_0_auc: 0.9983  |  0:01:26s\n",
      "epoch 30 | loss: 0.08145 | val_0_auc: 0.99798 |  0:01:29s\n",
      "epoch 31 | loss: 0.07537 | val_0_auc: 0.9986  |  0:01:32s\n",
      "epoch 32 | loss: 0.08156 | val_0_auc: 0.99861 |  0:01:35s\n",
      "epoch 33 | loss: 0.06668 | val_0_auc: 0.99821 |  0:01:38s\n",
      "epoch 34 | loss: 0.06905 | val_0_auc: 0.99788 |  0:01:41s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.99894\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99893812  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 102.3018\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.209102878190361, 'lambda_sparse': 0.055335126604700885, 'n_steps': 7, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.19932 | val_0_auc: 0.90412 |  0:00:01s\n",
      "epoch 1  | loss: 0.34564 | val_0_auc: 0.97378 |  0:00:03s\n",
      "epoch 2  | loss: 0.21903 | val_0_auc: 0.99066 |  0:00:04s\n",
      "epoch 3  | loss: 0.16444 | val_0_auc: 0.99545 |  0:00:06s\n",
      "epoch 4  | loss: 0.13724 | val_0_auc: 0.99509 |  0:00:07s\n",
      "epoch 5  | loss: 0.12558 | val_0_auc: 0.99697 |  0:00:09s\n",
      "epoch 6  | loss: 0.10744 | val_0_auc: 0.99752 |  0:00:10s\n",
      "epoch 7  | loss: 0.10306 | val_0_auc: 0.99758 |  0:00:12s\n",
      "epoch 8  | loss: 0.13434 | val_0_auc: 0.99741 |  0:00:14s\n",
      "epoch 9  | loss: 0.14311 | val_0_auc: 0.99769 |  0:00:15s\n",
      "epoch 10 | loss: 0.1563  | val_0_auc: 0.99844 |  0:00:17s\n",
      "epoch 11 | loss: 0.11849 | val_0_auc: 0.99831 |  0:00:18s\n",
      "epoch 12 | loss: 0.10819 | val_0_auc: 0.99837 |  0:00:20s\n",
      "epoch 13 | loss: 0.09517 | val_0_auc: 0.99824 |  0:00:21s\n",
      "epoch 14 | loss: 0.08832 | val_0_auc: 0.99842 |  0:00:23s\n",
      "epoch 15 | loss: 0.10222 | val_0_auc: 0.99864 |  0:00:24s\n",
      "epoch 16 | loss: 0.09011 | val_0_auc: 0.99812 |  0:00:26s\n",
      "epoch 17 | loss: 0.08637 | val_0_auc: 0.99812 |  0:00:27s\n",
      "epoch 18 | loss: 0.09133 | val_0_auc: 0.99834 |  0:00:29s\n",
      "epoch 19 | loss: 0.08866 | val_0_auc: 0.99863 |  0:00:30s\n",
      "epoch 20 | loss: 0.08953 | val_0_auc: 0.99903 |  0:00:32s\n",
      "epoch 21 | loss: 0.09242 | val_0_auc: 0.99865 |  0:00:33s\n",
      "epoch 22 | loss: 0.08759 | val_0_auc: 0.99807 |  0:00:35s\n",
      "epoch 23 | loss: 0.09408 | val_0_auc: 0.99873 |  0:00:36s\n",
      "epoch 24 | loss: 0.09789 | val_0_auc: 0.99863 |  0:00:38s\n",
      "epoch 25 | loss: 0.09401 | val_0_auc: 0.99868 |  0:00:39s\n",
      "epoch 26 | loss: 0.08713 | val_0_auc: 0.99897 |  0:00:41s\n",
      "epoch 27 | loss: 0.08277 | val_0_auc: 0.99904 |  0:00:43s\n",
      "epoch 28 | loss: 0.08013 | val_0_auc: 0.99865 |  0:00:44s\n",
      "epoch 29 | loss: 0.07979 | val_0_auc: 0.99891 |  0:00:46s\n",
      "epoch 30 | loss: 0.07422 | val_0_auc: 0.99897 |  0:00:47s\n",
      "epoch 31 | loss: 0.07069 | val_0_auc: 0.99893 |  0:00:49s\n",
      "epoch 32 | loss: 0.07642 | val_0_auc: 0.99876 |  0:00:50s\n",
      "epoch 33 | loss: 0.0688  | val_0_auc: 0.99863 |  0:00:52s\n",
      "epoch 34 | loss: 0.08311 | val_0_auc: 0.99929 |  0:00:53s\n",
      "epoch 35 | loss: 0.06748 | val_0_auc: 0.99854 |  0:00:55s\n",
      "epoch 36 | loss: 0.07896 | val_0_auc: 0.99885 |  0:00:56s\n",
      "epoch 37 | loss: 0.08756 | val_0_auc: 0.99797 |  0:00:58s\n",
      "epoch 38 | loss: 0.08601 | val_0_auc: 0.99844 |  0:00:59s\n",
      "epoch 39 | loss: 0.07736 | val_0_auc: 0.99855 |  0:01:01s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.99929\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99928852  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 62.0407\n",
      "Function value obtained: -0.9993\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.820140700312006, 'lambda_sparse': 0.026569387921748164, 'n_steps': 10, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.16998 | val_0_auc: 0.84968 |  0:00:03s\n",
      "epoch 1  | loss: 0.62962 | val_0_auc: 0.88135 |  0:00:06s\n",
      "epoch 2  | loss: 1.01532 | val_0_auc: 0.92125 |  0:00:08s\n",
      "epoch 3  | loss: 0.97552 | val_0_auc: 0.95206 |  0:00:10s\n",
      "epoch 4  | loss: 0.3841  | val_0_auc: 0.96332 |  0:00:13s\n",
      "epoch 5  | loss: 0.39552 | val_0_auc: 0.97506 |  0:00:15s\n",
      "epoch 6  | loss: 0.76035 | val_0_auc: 0.97165 |  0:00:17s\n",
      "epoch 7  | loss: 0.37635 | val_0_auc: 0.97732 |  0:00:20s\n",
      "epoch 8  | loss: 0.22397 | val_0_auc: 0.98853 |  0:00:22s\n",
      "epoch 9  | loss: 0.14723 | val_0_auc: 0.98943 |  0:00:24s\n",
      "epoch 10 | loss: 0.13268 | val_0_auc: 0.99379 |  0:00:27s\n",
      "epoch 11 | loss: 0.11534 | val_0_auc: 0.99501 |  0:00:29s\n",
      "epoch 12 | loss: 0.12943 | val_0_auc: 0.98784 |  0:00:31s\n",
      "epoch 13 | loss: 0.26738 | val_0_auc: 0.99234 |  0:00:34s\n",
      "epoch 14 | loss: 0.38767 | val_0_auc: 0.9868  |  0:00:36s\n",
      "epoch 15 | loss: 0.31476 | val_0_auc: 0.99432 |  0:00:38s\n",
      "epoch 16 | loss: 0.23703 | val_0_auc: 0.99522 |  0:00:41s\n",
      "epoch 17 | loss: 0.10691 | val_0_auc: 0.99539 |  0:00:43s\n",
      "epoch 18 | loss: 0.08372 | val_0_auc: 0.9962  |  0:00:45s\n",
      "epoch 19 | loss: 0.08106 | val_0_auc: 0.99724 |  0:00:47s\n",
      "epoch 20 | loss: 0.07851 | val_0_auc: 0.99727 |  0:00:50s\n",
      "epoch 21 | loss: 0.10169 | val_0_auc: 0.99748 |  0:00:52s\n",
      "epoch 22 | loss: 0.11013 | val_0_auc: 0.99544 |  0:00:54s\n",
      "epoch 23 | loss: 0.09176 | val_0_auc: 0.99551 |  0:00:57s\n",
      "epoch 24 | loss: 0.08813 | val_0_auc: 0.99675 |  0:00:59s\n",
      "epoch 25 | loss: 0.09478 | val_0_auc: 0.99593 |  0:01:01s\n",
      "epoch 26 | loss: 0.08523 | val_0_auc: 0.99616 |  0:01:03s\n",
      "epoch 27 | loss: 0.08453 | val_0_auc: 0.99686 |  0:01:06s\n",
      "epoch 28 | loss: 0.07731 | val_0_auc: 0.99701 |  0:01:08s\n",
      "epoch 29 | loss: 0.08789 | val_0_auc: 0.99743 |  0:01:10s\n",
      "epoch 30 | loss: 0.0794  | val_0_auc: 0.9963  |  0:01:13s\n",
      "epoch 31 | loss: 0.08396 | val_0_auc: 0.99597 |  0:01:15s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.99748\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99748137  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 76.2981\n",
      "Function value obtained: -0.9975\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2678485893253433, 'lambda_sparse': 0.07697817638567792, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.58273 | val_0_auc: 0.8022  |  0:00:01s\n",
      "epoch 1  | loss: 0.5946  | val_0_auc: 0.95652 |  0:00:03s\n",
      "epoch 2  | loss: 0.3419  | val_0_auc: 0.97525 |  0:00:05s\n",
      "epoch 3  | loss: 0.25964 | val_0_auc: 0.99314 |  0:00:06s\n",
      "epoch 4  | loss: 0.20432 | val_0_auc: 0.99377 |  0:00:08s\n",
      "epoch 5  | loss: 0.15405 | val_0_auc: 0.99493 |  0:00:10s\n",
      "epoch 6  | loss: 0.13368 | val_0_auc: 0.99608 |  0:00:11s\n",
      "epoch 7  | loss: 0.10898 | val_0_auc: 0.99748 |  0:00:13s\n",
      "epoch 8  | loss: 0.09726 | val_0_auc: 0.99749 |  0:00:15s\n",
      "epoch 9  | loss: 0.10506 | val_0_auc: 0.99841 |  0:00:17s\n",
      "epoch 10 | loss: 0.10643 | val_0_auc: 0.99849 |  0:00:18s\n",
      "epoch 11 | loss: 0.1037  | val_0_auc: 0.99801 |  0:00:20s\n",
      "epoch 12 | loss: 0.09768 | val_0_auc: 0.99812 |  0:00:22s\n",
      "epoch 13 | loss: 0.10114 | val_0_auc: 0.99871 |  0:00:23s\n",
      "epoch 14 | loss: 0.11004 | val_0_auc: 0.99778 |  0:00:25s\n",
      "epoch 15 | loss: 0.10903 | val_0_auc: 0.99856 |  0:00:27s\n",
      "epoch 16 | loss: 0.10113 | val_0_auc: 0.9972  |  0:00:28s\n",
      "epoch 17 | loss: 0.10372 | val_0_auc: 0.99821 |  0:00:30s\n",
      "epoch 18 | loss: 0.09596 | val_0_auc: 0.99816 |  0:00:32s\n",
      "epoch 19 | loss: 0.10427 | val_0_auc: 0.99819 |  0:00:33s\n",
      "epoch 20 | loss: 0.09294 | val_0_auc: 0.99768 |  0:00:35s\n",
      "epoch 21 | loss: 0.08031 | val_0_auc: 0.99809 |  0:00:37s\n",
      "epoch 22 | loss: 0.07864 | val_0_auc: 0.99702 |  0:00:38s\n",
      "epoch 23 | loss: 0.0791  | val_0_auc: 0.99763 |  0:00:40s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.99871\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99871045  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 41.2386\n",
      "Function value obtained: -0.9987\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.431808338997011, 'lambda_sparse': 0.07583214952286116, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.50455 | val_0_auc: 0.87522 |  0:00:01s\n",
      "epoch 1  | loss: 0.31848 | val_0_auc: 0.98033 |  0:00:03s\n",
      "epoch 2  | loss: 0.18405 | val_0_auc: 0.99353 |  0:00:04s\n",
      "epoch 3  | loss: 0.16077 | val_0_auc: 0.995   |  0:00:06s\n",
      "epoch 4  | loss: 0.13155 | val_0_auc: 0.99619 |  0:00:08s\n",
      "epoch 5  | loss: 0.11683 | val_0_auc: 0.99618 |  0:00:10s\n",
      "epoch 6  | loss: 0.10126 | val_0_auc: 0.99659 |  0:00:11s\n",
      "epoch 7  | loss: 0.10209 | val_0_auc: 0.99733 |  0:00:13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 0.09764 | val_0_auc: 0.9975  |  0:00:15s\n",
      "epoch 9  | loss: 0.0885  | val_0_auc: 0.99684 |  0:00:16s\n",
      "epoch 10 | loss: 0.10476 | val_0_auc: 0.99721 |  0:00:18s\n",
      "epoch 11 | loss: 0.11236 | val_0_auc: 0.99652 |  0:00:20s\n",
      "epoch 12 | loss: 0.09247 | val_0_auc: 0.99712 |  0:00:21s\n",
      "epoch 13 | loss: 0.09575 | val_0_auc: 0.99732 |  0:00:23s\n",
      "epoch 14 | loss: 0.08273 | val_0_auc: 0.99807 |  0:00:24s\n",
      "epoch 15 | loss: 0.09274 | val_0_auc: 0.9976  |  0:00:26s\n",
      "epoch 16 | loss: 0.09135 | val_0_auc: 0.99773 |  0:00:28s\n",
      "epoch 17 | loss: 0.08746 | val_0_auc: 0.99703 |  0:00:30s\n",
      "epoch 18 | loss: 0.08754 | val_0_auc: 0.99763 |  0:00:31s\n",
      "epoch 19 | loss: 0.0847  | val_0_auc: 0.99715 |  0:00:33s\n",
      "epoch 20 | loss: 0.08329 | val_0_auc: 0.99788 |  0:00:35s\n",
      "epoch 21 | loss: 0.08218 | val_0_auc: 0.99815 |  0:00:36s\n",
      "epoch 22 | loss: 0.08013 | val_0_auc: 0.9974  |  0:00:38s\n",
      "epoch 23 | loss: 0.09122 | val_0_auc: 0.99737 |  0:00:40s\n",
      "epoch 24 | loss: 0.07381 | val_0_auc: 0.99819 |  0:00:41s\n",
      "epoch 25 | loss: 0.08301 | val_0_auc: 0.99831 |  0:00:43s\n",
      "epoch 26 | loss: 0.07301 | val_0_auc: 0.99842 |  0:00:45s\n",
      "epoch 27 | loss: 0.06635 | val_0_auc: 0.99831 |  0:00:46s\n",
      "epoch 28 | loss: 0.07671 | val_0_auc: 0.99781 |  0:00:48s\n",
      "epoch 29 | loss: 0.08048 | val_0_auc: 0.99798 |  0:00:50s\n",
      "epoch 30 | loss: 0.08304 | val_0_auc: 0.99826 |  0:00:51s\n",
      "epoch 31 | loss: 0.07051 | val_0_auc: 0.99781 |  0:00:53s\n",
      "epoch 32 | loss: 0.0717  | val_0_auc: 0.99835 |  0:00:55s\n",
      "epoch 33 | loss: 0.06743 | val_0_auc: 0.99834 |  0:00:56s\n",
      "epoch 34 | loss: 0.0708  | val_0_auc: 0.99842 |  0:00:58s\n",
      "epoch 35 | loss: 0.06788 | val_0_auc: 0.99794 |  0:01:00s\n",
      "epoch 36 | loss: 0.06526 | val_0_auc: 0.99708 |  0:01:01s\n",
      "epoch 37 | loss: 0.06848 | val_0_auc: 0.99703 |  0:01:03s\n",
      "epoch 38 | loss: 0.07635 | val_0_auc: 0.99824 |  0:01:05s\n",
      "epoch 39 | loss: 0.06534 | val_0_auc: 0.99731 |  0:01:06s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.99842\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99842052  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 67.4198\n",
      "Function value obtained: -0.9984\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1661126805007411, 'lambda_sparse': 0.058478856820622024, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.7404  | val_0_auc: 0.94853 |  0:00:00s\n",
      "epoch 1  | loss: 0.28764 | val_0_auc: 0.98746 |  0:00:01s\n",
      "epoch 2  | loss: 0.16124 | val_0_auc: 0.99524 |  0:00:02s\n",
      "epoch 3  | loss: 0.1281  | val_0_auc: 0.99624 |  0:00:03s\n",
      "epoch 4  | loss: 0.12253 | val_0_auc: 0.99678 |  0:00:04s\n",
      "epoch 5  | loss: 0.10355 | val_0_auc: 0.99666 |  0:00:05s\n",
      "epoch 6  | loss: 0.09937 | val_0_auc: 0.99769 |  0:00:06s\n",
      "epoch 7  | loss: 0.09844 | val_0_auc: 0.99704 |  0:00:07s\n",
      "epoch 8  | loss: 0.10104 | val_0_auc: 0.99702 |  0:00:08s\n",
      "epoch 9  | loss: 0.09409 | val_0_auc: 0.99765 |  0:00:08s\n",
      "epoch 10 | loss: 0.10138 | val_0_auc: 0.99734 |  0:00:09s\n",
      "epoch 11 | loss: 0.09608 | val_0_auc: 0.99744 |  0:00:10s\n",
      "epoch 12 | loss: 0.09785 | val_0_auc: 0.99748 |  0:00:11s\n",
      "epoch 13 | loss: 0.09664 | val_0_auc: 0.99771 |  0:00:12s\n",
      "epoch 14 | loss: 0.0907  | val_0_auc: 0.9982  |  0:00:13s\n",
      "epoch 15 | loss: 0.08706 | val_0_auc: 0.9983  |  0:00:14s\n",
      "epoch 16 | loss: 0.08575 | val_0_auc: 0.99805 |  0:00:15s\n",
      "epoch 17 | loss: 0.08223 | val_0_auc: 0.99779 |  0:00:16s\n",
      "epoch 18 | loss: 0.08957 | val_0_auc: 0.99862 |  0:00:16s\n",
      "epoch 19 | loss: 0.08721 | val_0_auc: 0.99861 |  0:00:17s\n",
      "epoch 20 | loss: 0.08055 | val_0_auc: 0.99838 |  0:00:18s\n",
      "epoch 21 | loss: 0.08563 | val_0_auc: 0.99871 |  0:00:19s\n",
      "epoch 22 | loss: 0.08406 | val_0_auc: 0.99874 |  0:00:20s\n",
      "epoch 23 | loss: 0.07956 | val_0_auc: 0.99908 |  0:00:21s\n",
      "epoch 24 | loss: 0.08449 | val_0_auc: 0.99886 |  0:00:22s\n",
      "epoch 25 | loss: 0.08236 | val_0_auc: 0.9986  |  0:00:23s\n",
      "epoch 26 | loss: 0.07567 | val_0_auc: 0.99889 |  0:00:24s\n",
      "epoch 27 | loss: 0.07643 | val_0_auc: 0.99896 |  0:00:24s\n",
      "epoch 28 | loss: 0.06968 | val_0_auc: 0.99894 |  0:00:25s\n",
      "epoch 29 | loss: 0.07192 | val_0_auc: 0.99878 |  0:00:26s\n",
      "epoch 30 | loss: 0.068   | val_0_auc: 0.99848 |  0:00:27s\n",
      "epoch 31 | loss: 0.07044 | val_0_auc: 0.99888 |  0:00:28s\n",
      "epoch 32 | loss: 0.06235 | val_0_auc: 0.99899 |  0:00:29s\n",
      "epoch 33 | loss: 0.06619 | val_0_auc: 0.99841 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.99908\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99907864  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 30.4993\n",
      "Function value obtained: -0.9991\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7491613003479358, 'lambda_sparse': 0.049568448599178534, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.2437  | val_0_auc: 0.68713 |  0:00:01s\n",
      "epoch 1  | loss: 0.72765 | val_0_auc: 0.7837  |  0:00:03s\n",
      "epoch 2  | loss: 0.56752 | val_0_auc: 0.8451  |  0:00:05s\n",
      "epoch 3  | loss: 0.48887 | val_0_auc: 0.83783 |  0:00:06s\n",
      "epoch 4  | loss: 0.42828 | val_0_auc: 0.91018 |  0:00:08s\n",
      "epoch 5  | loss: 0.43763 | val_0_auc: 0.93371 |  0:00:10s\n",
      "epoch 6  | loss: 0.39004 | val_0_auc: 0.96143 |  0:00:11s\n",
      "epoch 7  | loss: 0.29162 | val_0_auc: 0.96044 |  0:00:13s\n",
      "epoch 8  | loss: 0.32989 | val_0_auc: 0.96518 |  0:00:15s\n",
      "epoch 9  | loss: 0.27584 | val_0_auc: 0.97433 |  0:00:17s\n",
      "epoch 10 | loss: 0.22679 | val_0_auc: 0.98284 |  0:00:18s\n",
      "epoch 11 | loss: 0.17513 | val_0_auc: 0.97619 |  0:00:20s\n",
      "epoch 12 | loss: 0.17594 | val_0_auc: 0.98362 |  0:00:22s\n",
      "epoch 13 | loss: 0.17903 | val_0_auc: 0.98424 |  0:00:24s\n",
      "epoch 14 | loss: 0.17676 | val_0_auc: 0.98243 |  0:00:25s\n",
      "epoch 15 | loss: 0.17304 | val_0_auc: 0.9821  |  0:00:27s\n",
      "epoch 16 | loss: 0.15817 | val_0_auc: 0.98636 |  0:00:29s\n",
      "epoch 17 | loss: 0.1332  | val_0_auc: 0.98921 |  0:00:30s\n",
      "epoch 18 | loss: 0.12528 | val_0_auc: 0.99385 |  0:00:32s\n",
      "epoch 19 | loss: 0.11888 | val_0_auc: 0.99463 |  0:00:34s\n",
      "epoch 20 | loss: 0.10588 | val_0_auc: 0.99277 |  0:00:36s\n",
      "epoch 21 | loss: 0.1189  | val_0_auc: 0.99227 |  0:00:37s\n",
      "epoch 22 | loss: 0.11123 | val_0_auc: 0.99487 |  0:00:39s\n",
      "epoch 23 | loss: 0.11718 | val_0_auc: 0.99572 |  0:00:41s\n",
      "epoch 24 | loss: 0.0944  | val_0_auc: 0.99569 |  0:00:42s\n",
      "epoch 25 | loss: 0.10859 | val_0_auc: 0.99361 |  0:00:44s\n",
      "epoch 26 | loss: 0.10293 | val_0_auc: 0.99556 |  0:00:46s\n",
      "epoch 27 | loss: 0.11285 | val_0_auc: 0.99548 |  0:00:48s\n",
      "epoch 28 | loss: 0.10331 | val_0_auc: 0.99627 |  0:00:49s\n",
      "epoch 29 | loss: 0.11337 | val_0_auc: 0.99549 |  0:00:51s\n",
      "epoch 30 | loss: 0.11094 | val_0_auc: 0.99438 |  0:00:53s\n",
      "epoch 31 | loss: 0.10001 | val_0_auc: 0.99601 |  0:00:54s\n",
      "epoch 32 | loss: 0.10092 | val_0_auc: 0.99547 |  0:00:56s\n",
      "epoch 33 | loss: 0.11672 | val_0_auc: 0.99481 |  0:00:58s\n",
      "epoch 34 | loss: 0.11673 | val_0_auc: 0.99538 |  0:01:00s\n",
      "epoch 35 | loss: 0.09865 | val_0_auc: 0.99464 |  0:01:01s\n",
      "epoch 36 | loss: 0.11205 | val_0_auc: 0.99437 |  0:01:03s\n",
      "epoch 37 | loss: 0.10396 | val_0_auc: 0.99421 |  0:01:05s\n",
      "epoch 38 | loss: 0.11035 | val_0_auc: 0.99563 |  0:01:06s\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.99627\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99626653  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 67.3055\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9995\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5677018396290885, 'lambda_sparse': 0.04841913603075304, 'n_steps': 4, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.69288 | val_0_auc: 0.96082 |  0:00:01s\n",
      "epoch 1  | loss: 0.21783 | val_0_auc: 0.99027 |  0:00:02s\n",
      "epoch 2  | loss: 0.14306 | val_0_auc: 0.99443 |  0:00:03s\n",
      "epoch 3  | loss: 0.11869 | val_0_auc: 0.99582 |  0:00:04s\n",
      "epoch 4  | loss: 0.10594 | val_0_auc: 0.99834 |  0:00:05s\n",
      "epoch 5  | loss: 0.08834 | val_0_auc: 0.99832 |  0:00:06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6  | loss: 0.08948 | val_0_auc: 0.99852 |  0:00:07s\n",
      "epoch 7  | loss: 0.10085 | val_0_auc: 0.99823 |  0:00:08s\n",
      "epoch 8  | loss: 0.08399 | val_0_auc: 0.99845 |  0:00:09s\n",
      "epoch 9  | loss: 0.09896 | val_0_auc: 0.99865 |  0:00:10s\n",
      "epoch 10 | loss: 0.07796 | val_0_auc: 0.9985  |  0:00:11s\n",
      "epoch 11 | loss: 0.07759 | val_0_auc: 0.99897 |  0:00:13s\n",
      "epoch 12 | loss: 0.08234 | val_0_auc: 0.99896 |  0:00:14s\n",
      "epoch 13 | loss: 0.07903 | val_0_auc: 0.99893 |  0:00:15s\n",
      "epoch 14 | loss: 0.07557 | val_0_auc: 0.99922 |  0:00:16s\n",
      "epoch 15 | loss: 0.06959 | val_0_auc: 0.99899 |  0:00:17s\n",
      "epoch 16 | loss: 0.07322 | val_0_auc: 0.99924 |  0:00:18s\n",
      "epoch 17 | loss: 0.06692 | val_0_auc: 0.99938 |  0:00:19s\n",
      "epoch 18 | loss: 0.06554 | val_0_auc: 0.99928 |  0:00:20s\n",
      "epoch 19 | loss: 0.06712 | val_0_auc: 0.99896 |  0:00:21s\n",
      "epoch 20 | loss: 0.07397 | val_0_auc: 0.99892 |  0:00:22s\n",
      "epoch 21 | loss: 0.06504 | val_0_auc: 0.99935 |  0:00:23s\n",
      "epoch 22 | loss: 0.05789 | val_0_auc: 0.99933 |  0:00:24s\n",
      "epoch 23 | loss: 0.0538  | val_0_auc: 0.99947 |  0:00:25s\n",
      "epoch 24 | loss: 0.06133 | val_0_auc: 0.99943 |  0:00:27s\n",
      "epoch 25 | loss: 0.06626 | val_0_auc: 0.99935 |  0:00:28s\n",
      "epoch 26 | loss: 0.05502 | val_0_auc: 0.99957 |  0:00:29s\n",
      "epoch 27 | loss: 0.06653 | val_0_auc: 0.9993  |  0:00:30s\n",
      "epoch 28 | loss: 0.06065 | val_0_auc: 0.99928 |  0:00:31s\n",
      "epoch 29 | loss: 0.06517 | val_0_auc: 0.99924 |  0:00:32s\n",
      "epoch 30 | loss: 0.06425 | val_0_auc: 0.99919 |  0:00:33s\n",
      "epoch 31 | loss: 0.06348 | val_0_auc: 0.99887 |  0:00:34s\n",
      "epoch 32 | loss: 0.05528 | val_0_auc: 0.99908 |  0:00:35s\n",
      "epoch 33 | loss: 0.05609 | val_0_auc: 0.99942 |  0:00:36s\n",
      "epoch 34 | loss: 0.07106 | val_0_auc: 0.99879 |  0:00:37s\n",
      "epoch 35 | loss: 0.06305 | val_0_auc: 0.99935 |  0:00:38s\n",
      "epoch 36 | loss: 0.0552  | val_0_auc: 0.99948 |  0:00:40s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.99957\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99956956  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 40.4812\n",
      "Function value obtained: -0.9996\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3894647892525955, 'lambda_sparse': 0.06569477787420686, 'n_steps': 7, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.30253 | val_0_auc: 0.75976 |  0:00:01s\n",
      "epoch 1  | loss: 0.53906 | val_0_auc: 0.93847 |  0:00:03s\n",
      "epoch 2  | loss: 0.31403 | val_0_auc: 0.97295 |  0:00:04s\n",
      "epoch 3  | loss: 0.2224  | val_0_auc: 0.98528 |  0:00:06s\n",
      "epoch 4  | loss: 0.17201 | val_0_auc: 0.99211 |  0:00:07s\n",
      "epoch 5  | loss: 0.13687 | val_0_auc: 0.99451 |  0:00:09s\n",
      "epoch 6  | loss: 0.11435 | val_0_auc: 0.99636 |  0:00:10s\n",
      "epoch 7  | loss: 0.11302 | val_0_auc: 0.99796 |  0:00:12s\n",
      "epoch 8  | loss: 0.12643 | val_0_auc: 0.99583 |  0:00:13s\n",
      "epoch 9  | loss: 0.10996 | val_0_auc: 0.99715 |  0:00:15s\n",
      "epoch 10 | loss: 0.11586 | val_0_auc: 0.9966  |  0:00:16s\n",
      "epoch 11 | loss: 0.13088 | val_0_auc: 0.99787 |  0:00:18s\n",
      "epoch 12 | loss: 0.12519 | val_0_auc: 0.99559 |  0:00:19s\n",
      "epoch 13 | loss: 0.09648 | val_0_auc: 0.99774 |  0:00:21s\n",
      "epoch 14 | loss: 0.09255 | val_0_auc: 0.99715 |  0:00:22s\n",
      "epoch 15 | loss: 0.10608 | val_0_auc: 0.99724 |  0:00:24s\n",
      "epoch 16 | loss: 0.08978 | val_0_auc: 0.99629 |  0:00:26s\n",
      "epoch 17 | loss: 0.08305 | val_0_auc: 0.99803 |  0:00:27s\n",
      "epoch 18 | loss: 0.09145 | val_0_auc: 0.99724 |  0:00:29s\n",
      "epoch 19 | loss: 0.08689 | val_0_auc: 0.99861 |  0:00:30s\n",
      "epoch 20 | loss: 0.08917 | val_0_auc: 0.99774 |  0:00:32s\n",
      "epoch 21 | loss: 0.08699 | val_0_auc: 0.99753 |  0:00:33s\n",
      "epoch 22 | loss: 0.09181 | val_0_auc: 0.99734 |  0:00:35s\n",
      "epoch 23 | loss: 0.08616 | val_0_auc: 0.99786 |  0:00:36s\n",
      "epoch 24 | loss: 0.08985 | val_0_auc: 0.99798 |  0:00:38s\n",
      "epoch 25 | loss: 0.08522 | val_0_auc: 0.9981  |  0:00:39s\n",
      "epoch 26 | loss: 0.07909 | val_0_auc: 0.99791 |  0:00:41s\n",
      "epoch 27 | loss: 0.07757 | val_0_auc: 0.99833 |  0:00:42s\n",
      "epoch 28 | loss: 0.08261 | val_0_auc: 0.99825 |  0:00:44s\n",
      "epoch 29 | loss: 0.07724 | val_0_auc: 0.99834 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.99861\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99860906  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 46.4864\n",
      "Function value obtained: -0.9986\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.352041201863751, 'lambda_sparse': 0.0901313617394624, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.75932 | val_0_auc: 0.87684 |  0:00:01s\n",
      "epoch 1  | loss: 0.37033 | val_0_auc: 0.97869 |  0:00:02s\n",
      "epoch 2  | loss: 0.19908 | val_0_auc: 0.9938  |  0:00:03s\n",
      "epoch 3  | loss: 0.15573 | val_0_auc: 0.99571 |  0:00:04s\n",
      "epoch 4  | loss: 0.11628 | val_0_auc: 0.9974  |  0:00:06s\n",
      "epoch 5  | loss: 0.11375 | val_0_auc: 0.99741 |  0:00:07s\n",
      "epoch 6  | loss: 0.12658 | val_0_auc: 0.99813 |  0:00:08s\n",
      "epoch 7  | loss: 0.1113  | val_0_auc: 0.99827 |  0:00:09s\n",
      "epoch 8  | loss: 0.09647 | val_0_auc: 0.99823 |  0:00:10s\n",
      "epoch 9  | loss: 0.09176 | val_0_auc: 0.99875 |  0:00:12s\n",
      "epoch 10 | loss: 0.09414 | val_0_auc: 0.99897 |  0:00:13s\n",
      "epoch 11 | loss: 0.10231 | val_0_auc: 0.99797 |  0:00:14s\n",
      "epoch 12 | loss: 0.09089 | val_0_auc: 0.99776 |  0:00:15s\n",
      "epoch 13 | loss: 0.09428 | val_0_auc: 0.99777 |  0:00:17s\n",
      "epoch 14 | loss: 0.08912 | val_0_auc: 0.99792 |  0:00:18s\n",
      "epoch 15 | loss: 0.09054 | val_0_auc: 0.99725 |  0:00:19s\n",
      "epoch 16 | loss: 0.08709 | val_0_auc: 0.99823 |  0:00:20s\n",
      "epoch 17 | loss: 0.09982 | val_0_auc: 0.99745 |  0:00:22s\n",
      "epoch 18 | loss: 0.08982 | val_0_auc: 0.9983  |  0:00:23s\n",
      "epoch 19 | loss: 0.08429 | val_0_auc: 0.99876 |  0:00:24s\n",
      "epoch 20 | loss: 0.07809 | val_0_auc: 0.99896 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.99897\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99896658  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 26.1389\n",
      "Function value obtained: -0.9990\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1632237883043235, 'lambda_sparse': 0.023843760998888117, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.90436 | val_0_auc: 0.8079  |  0:00:01s\n",
      "epoch 1  | loss: 0.48052 | val_0_auc: 0.94242 |  0:00:03s\n",
      "epoch 2  | loss: 0.29515 | val_0_auc: 0.96809 |  0:00:04s\n",
      "epoch 3  | loss: 0.21951 | val_0_auc: 0.99025 |  0:00:06s\n",
      "epoch 4  | loss: 0.14783 | val_0_auc: 0.99391 |  0:00:07s\n",
      "epoch 5  | loss: 0.12275 | val_0_auc: 0.99658 |  0:00:09s\n",
      "epoch 6  | loss: 0.11987 | val_0_auc: 0.99655 |  0:00:10s\n",
      "epoch 7  | loss: 0.1047  | val_0_auc: 0.99692 |  0:00:12s\n",
      "epoch 8  | loss: 0.11855 | val_0_auc: 0.99739 |  0:00:13s\n",
      "epoch 9  | loss: 0.13148 | val_0_auc: 0.99784 |  0:00:15s\n",
      "epoch 10 | loss: 0.11497 | val_0_auc: 0.99753 |  0:00:17s\n",
      "epoch 11 | loss: 0.10279 | val_0_auc: 0.99809 |  0:00:18s\n",
      "epoch 12 | loss: 0.09032 | val_0_auc: 0.99747 |  0:00:20s\n",
      "epoch 13 | loss: 0.12103 | val_0_auc: 0.99629 |  0:00:21s\n",
      "epoch 14 | loss: 0.11197 | val_0_auc: 0.99734 |  0:00:23s\n",
      "epoch 15 | loss: 0.10401 | val_0_auc: 0.99778 |  0:00:24s\n",
      "epoch 16 | loss: 0.09274 | val_0_auc: 0.99771 |  0:00:26s\n",
      "epoch 17 | loss: 0.09536 | val_0_auc: 0.99668 |  0:00:27s\n",
      "epoch 18 | loss: 0.10106 | val_0_auc: 0.99781 |  0:00:29s\n",
      "epoch 19 | loss: 0.09485 | val_0_auc: 0.99777 |  0:00:31s\n",
      "epoch 20 | loss: 0.08596 | val_0_auc: 0.99756 |  0:00:32s\n",
      "epoch 21 | loss: 0.09774 | val_0_auc: 0.99742 |  0:00:34s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.99809\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99808969  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 34.6023\n",
      "Function value obtained: -0.9981\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6336396835818139, 'lambda_sparse': 0.055238338662125734, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.98}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.87819 | val_0_auc: 0.81372 |  0:00:01s\n",
      "epoch 1  | loss: 0.43691 | val_0_auc: 0.94466 |  0:00:02s\n",
      "epoch 2  | loss: 0.28859 | val_0_auc: 0.96976 |  0:00:04s\n",
      "epoch 3  | loss: 0.21328 | val_0_auc: 0.98309 |  0:00:05s\n",
      "epoch 4  | loss: 0.2462  | val_0_auc: 0.98227 |  0:00:06s\n",
      "epoch 5  | loss: 0.21211 | val_0_auc: 0.98393 |  0:00:08s\n",
      "epoch 6  | loss: 0.21277 | val_0_auc: 0.99208 |  0:00:09s\n",
      "epoch 7  | loss: 0.13963 | val_0_auc: 0.99396 |  0:00:10s\n",
      "epoch 8  | loss: 0.12127 | val_0_auc: 0.99543 |  0:00:12s\n",
      "epoch 9  | loss: 0.1002  | val_0_auc: 0.99561 |  0:00:13s\n",
      "epoch 10 | loss: 0.10142 | val_0_auc: 0.99503 |  0:00:14s\n",
      "epoch 11 | loss: 0.11085 | val_0_auc: 0.99484 |  0:00:16s\n",
      "epoch 12 | loss: 0.10972 | val_0_auc: 0.9952  |  0:00:17s\n",
      "epoch 13 | loss: 0.09539 | val_0_auc: 0.99484 |  0:00:19s\n",
      "epoch 14 | loss: 0.10297 | val_0_auc: 0.99461 |  0:00:20s\n",
      "epoch 15 | loss: 0.0953  | val_0_auc: 0.99637 |  0:00:21s\n",
      "epoch 16 | loss: 0.10067 | val_0_auc: 0.99641 |  0:00:23s\n",
      "epoch 17 | loss: 0.08845 | val_0_auc: 0.99708 |  0:00:24s\n",
      "epoch 18 | loss: 0.08553 | val_0_auc: 0.99709 |  0:00:25s\n",
      "epoch 19 | loss: 0.08133 | val_0_auc: 0.99751 |  0:00:27s\n",
      "epoch 20 | loss: 0.08061 | val_0_auc: 0.99856 |  0:00:28s\n",
      "epoch 21 | loss: 0.07712 | val_0_auc: 0.99699 |  0:00:29s\n",
      "epoch 22 | loss: 0.08937 | val_0_auc: 0.99743 |  0:00:31s\n",
      "epoch 23 | loss: 0.07917 | val_0_auc: 0.99751 |  0:00:32s\n",
      "epoch 24 | loss: 0.0784  | val_0_auc: 0.99807 |  0:00:33s\n",
      "epoch 25 | loss: 0.077   | val_0_auc: 0.99705 |  0:00:35s\n",
      "epoch 26 | loss: 0.06675 | val_0_auc: 0.99819 |  0:00:36s\n",
      "epoch 27 | loss: 0.06724 | val_0_auc: 0.99868 |  0:00:38s\n",
      "epoch 28 | loss: 0.07428 | val_0_auc: 0.99842 |  0:00:39s\n",
      "epoch 29 | loss: 0.09503 | val_0_auc: 0.99739 |  0:00:40s\n",
      "epoch 30 | loss: 0.10957 | val_0_auc: 0.99789 |  0:00:42s\n",
      "epoch 31 | loss: 0.09505 | val_0_auc: 0.99764 |  0:00:43s\n",
      "epoch 32 | loss: 0.09341 | val_0_auc: 0.99783 |  0:00:44s\n",
      "epoch 33 | loss: 0.07132 | val_0_auc: 0.99805 |  0:00:46s\n",
      "epoch 34 | loss: 0.09009 | val_0_auc: 0.99745 |  0:00:47s\n",
      "epoch 35 | loss: 0.08156 | val_0_auc: 0.9979  |  0:00:48s\n",
      "epoch 36 | loss: 0.06623 | val_0_auc: 0.99847 |  0:00:50s\n",
      "epoch 37 | loss: 0.07116 | val_0_auc: 0.99859 |  0:00:51s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.99868\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99868021  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 51.9452\n",
      "Function value obtained: -0.9987\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1126249965883332, 'lambda_sparse': 0.01065905860176036, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.77697 | val_0_auc: 0.90472 |  0:00:01s\n",
      "epoch 1  | loss: 0.32215 | val_0_auc: 0.97891 |  0:00:02s\n",
      "epoch 2  | loss: 0.18004 | val_0_auc: 0.98972 |  0:00:04s\n",
      "epoch 3  | loss: 0.12263 | val_0_auc: 0.99449 |  0:00:05s\n",
      "epoch 4  | loss: 0.10108 | val_0_auc: 0.99729 |  0:00:06s\n",
      "epoch 5  | loss: 0.10052 | val_0_auc: 0.99664 |  0:00:08s\n",
      "epoch 6  | loss: 0.0801  | val_0_auc: 0.99697 |  0:00:09s\n",
      "epoch 7  | loss: 0.1233  | val_0_auc: 0.99758 |  0:00:10s\n",
      "epoch 8  | loss: 0.12613 | val_0_auc: 0.99724 |  0:00:12s\n",
      "epoch 9  | loss: 0.10398 | val_0_auc: 0.99875 |  0:00:13s\n",
      "epoch 10 | loss: 0.09224 | val_0_auc: 0.99881 |  0:00:14s\n",
      "epoch 11 | loss: 0.08748 | val_0_auc: 0.99822 |  0:00:16s\n",
      "epoch 12 | loss: 0.07253 | val_0_auc: 0.99793 |  0:00:17s\n",
      "epoch 13 | loss: 0.06799 | val_0_auc: 0.99832 |  0:00:19s\n",
      "epoch 14 | loss: 0.07506 | val_0_auc: 0.99843 |  0:00:20s\n",
      "epoch 15 | loss: 0.06937 | val_0_auc: 0.99799 |  0:00:21s\n",
      "epoch 16 | loss: 0.0638  | val_0_auc: 0.99812 |  0:00:23s\n",
      "epoch 17 | loss: 0.06811 | val_0_auc: 0.99828 |  0:00:24s\n",
      "epoch 18 | loss: 0.06785 | val_0_auc: 0.99842 |  0:00:25s\n",
      "epoch 19 | loss: 0.06292 | val_0_auc: 0.99863 |  0:00:27s\n",
      "epoch 20 | loss: 0.06828 | val_0_auc: 0.99801 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_val_0_auc = 0.99881\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99880828  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 29.1337\n",
      "Function value obtained: -0.9988\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7854030180997567, 'lambda_sparse': 0.04183911440311463, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.11607 | val_0_auc: 0.70577 |  0:00:01s\n",
      "epoch 1  | loss: 0.61809 | val_0_auc: 0.85085 |  0:00:03s\n",
      "epoch 2  | loss: 0.44562 | val_0_auc: 0.92591 |  0:00:05s\n",
      "epoch 3  | loss: 0.316   | val_0_auc: 0.96424 |  0:00:06s\n",
      "epoch 4  | loss: 0.26414 | val_0_auc: 0.97423 |  0:00:08s\n",
      "epoch 5  | loss: 0.25108 | val_0_auc: 0.98596 |  0:00:10s\n",
      "epoch 6  | loss: 0.24713 | val_0_auc: 0.97812 |  0:00:12s\n",
      "epoch 7  | loss: 0.19522 | val_0_auc: 0.98368 |  0:00:13s\n",
      "epoch 8  | loss: 0.18874 | val_0_auc: 0.9879  |  0:00:15s\n",
      "epoch 9  | loss: 0.18696 | val_0_auc: 0.99147 |  0:00:17s\n",
      "epoch 10 | loss: 0.16956 | val_0_auc: 0.9887  |  0:00:18s\n",
      "epoch 11 | loss: 0.13859 | val_0_auc: 0.99014 |  0:00:20s\n",
      "epoch 12 | loss: 0.16945 | val_0_auc: 0.99275 |  0:00:22s\n",
      "epoch 13 | loss: 0.13134 | val_0_auc: 0.99317 |  0:00:24s\n",
      "epoch 14 | loss: 0.17657 | val_0_auc: 0.99255 |  0:00:25s\n",
      "epoch 15 | loss: 0.22375 | val_0_auc: 0.98921 |  0:00:27s\n",
      "epoch 16 | loss: 0.25701 | val_0_auc: 0.99199 |  0:00:29s\n",
      "epoch 17 | loss: 0.18207 | val_0_auc: 0.99111 |  0:00:31s\n",
      "epoch 18 | loss: 0.17991 | val_0_auc: 0.98678 |  0:00:32s\n",
      "epoch 19 | loss: 0.20253 | val_0_auc: 0.99182 |  0:00:34s\n",
      "epoch 20 | loss: 0.14316 | val_0_auc: 0.99249 |  0:00:36s\n",
      "epoch 21 | loss: 0.12564 | val_0_auc: 0.99391 |  0:00:37s\n",
      "epoch 22 | loss: 0.12487 | val_0_auc: 0.99365 |  0:00:39s\n",
      "epoch 23 | loss: 0.12022 | val_0_auc: 0.99548 |  0:00:41s\n",
      "epoch 24 | loss: 0.11546 | val_0_auc: 0.99534 |  0:00:43s\n",
      "epoch 25 | loss: 0.11752 | val_0_auc: 0.99544 |  0:00:44s\n",
      "epoch 26 | loss: 0.10484 | val_0_auc: 0.99416 |  0:00:46s\n",
      "epoch 27 | loss: 0.11474 | val_0_auc: 0.99585 |  0:00:48s\n",
      "epoch 28 | loss: 0.09839 | val_0_auc: 0.99549 |  0:00:49s\n",
      "epoch 29 | loss: 0.10382 | val_0_auc: 0.99467 |  0:00:51s\n",
      "epoch 30 | loss: 0.10156 | val_0_auc: 0.99621 |  0:00:53s\n",
      "epoch 31 | loss: 0.10107 | val_0_auc: 0.99278 |  0:00:55s\n",
      "epoch 32 | loss: 0.1024  | val_0_auc: 0.99464 |  0:00:56s\n",
      "epoch 33 | loss: 0.09519 | val_0_auc: 0.99507 |  0:00:58s\n",
      "epoch 34 | loss: 0.09819 | val_0_auc: 0.99639 |  0:01:00s\n",
      "epoch 35 | loss: 0.08821 | val_0_auc: 0.99497 |  0:01:01s\n",
      "epoch 36 | loss: 0.09386 | val_0_auc: 0.99561 |  0:01:03s\n",
      "epoch 37 | loss: 0.08273 | val_0_auc: 0.99641 |  0:01:05s\n",
      "epoch 38 | loss: 0.09424 | val_0_auc: 0.99519 |  0:01:06s\n",
      "epoch 39 | loss: 0.08823 | val_0_auc: 0.99582 |  0:01:08s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99641\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99641416  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 69.1949\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.190448477049906, 'lambda_sparse': 0.043062813949967665, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.14518 | val_0_auc: 0.7276  |  0:00:01s\n",
      "epoch 1  | loss: 0.54028 | val_0_auc: 0.9298  |  0:00:03s\n",
      "epoch 2  | loss: 0.30017 | val_0_auc: 0.9809  |  0:00:05s\n",
      "epoch 3  | loss: 0.19638 | val_0_auc: 0.98753 |  0:00:07s\n",
      "epoch 4  | loss: 0.16205 | val_0_auc: 0.99043 |  0:00:08s\n",
      "epoch 5  | loss: 0.15278 | val_0_auc: 0.99359 |  0:00:10s\n",
      "epoch 6  | loss: 0.1257  | val_0_auc: 0.99444 |  0:00:12s\n",
      "epoch 7  | loss: 0.11961 | val_0_auc: 0.99603 |  0:00:13s\n",
      "epoch 8  | loss: 0.106   | val_0_auc: 0.99649 |  0:00:15s\n",
      "epoch 9  | loss: 0.10883 | val_0_auc: 0.99616 |  0:00:17s\n",
      "epoch 10 | loss: 0.10322 | val_0_auc: 0.99663 |  0:00:19s\n",
      "epoch 11 | loss: 0.11115 | val_0_auc: 0.99723 |  0:00:20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.09646 | val_0_auc: 0.99817 |  0:00:22s\n",
      "epoch 13 | loss: 0.12436 | val_0_auc: 0.99758 |  0:00:24s\n",
      "epoch 14 | loss: 0.13521 | val_0_auc: 0.99665 |  0:00:25s\n",
      "epoch 15 | loss: 0.13155 | val_0_auc: 0.99685 |  0:00:27s\n",
      "epoch 16 | loss: 0.11436 | val_0_auc: 0.99784 |  0:00:29s\n",
      "epoch 17 | loss: 0.09479 | val_0_auc: 0.99745 |  0:00:31s\n",
      "epoch 18 | loss: 0.09676 | val_0_auc: 0.99753 |  0:00:32s\n",
      "epoch 19 | loss: 0.09299 | val_0_auc: 0.99788 |  0:00:34s\n",
      "epoch 20 | loss: 0.08452 | val_0_auc: 0.99729 |  0:00:36s\n",
      "epoch 21 | loss: 0.10729 | val_0_auc: 0.99746 |  0:00:37s\n",
      "epoch 22 | loss: 0.09884 | val_0_auc: 0.99767 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.99817\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99817151  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 40.1391\n",
      "Function value obtained: -0.9982\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0004508742805982, 'lambda_sparse': 0.05671540571650534, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.74009 | val_0_auc: 0.945   |  0:00:01s\n",
      "epoch 1  | loss: 0.27308 | val_0_auc: 0.98758 |  0:00:02s\n",
      "epoch 2  | loss: 0.16252 | val_0_auc: 0.99185 |  0:00:04s\n",
      "epoch 3  | loss: 0.14485 | val_0_auc: 0.99539 |  0:00:05s\n",
      "epoch 4  | loss: 0.14559 | val_0_auc: 0.99587 |  0:00:07s\n",
      "epoch 5  | loss: 0.14321 | val_0_auc: 0.99682 |  0:00:08s\n",
      "epoch 6  | loss: 0.11885 | val_0_auc: 0.99752 |  0:00:09s\n",
      "epoch 7  | loss: 0.1484  | val_0_auc: 0.99768 |  0:00:11s\n",
      "epoch 8  | loss: 0.1452  | val_0_auc: 0.99762 |  0:00:12s\n",
      "epoch 9  | loss: 0.12477 | val_0_auc: 0.99865 |  0:00:13s\n",
      "epoch 10 | loss: 0.11917 | val_0_auc: 0.99818 |  0:00:15s\n",
      "epoch 11 | loss: 0.1171  | val_0_auc: 0.99811 |  0:00:16s\n",
      "epoch 12 | loss: 0.09439 | val_0_auc: 0.99792 |  0:00:17s\n",
      "epoch 13 | loss: 0.09179 | val_0_auc: 0.9984  |  0:00:19s\n",
      "epoch 14 | loss: 0.09833 | val_0_auc: 0.99803 |  0:00:20s\n",
      "epoch 15 | loss: 0.09317 | val_0_auc: 0.99827 |  0:00:21s\n",
      "epoch 16 | loss: 0.08766 | val_0_auc: 0.99809 |  0:00:23s\n",
      "epoch 17 | loss: 0.08696 | val_0_auc: 0.99771 |  0:00:24s\n",
      "epoch 18 | loss: 0.09497 | val_0_auc: 0.99884 |  0:00:25s\n",
      "epoch 19 | loss: 0.08562 | val_0_auc: 0.99878 |  0:00:27s\n",
      "epoch 20 | loss: 0.08405 | val_0_auc: 0.99877 |  0:00:28s\n",
      "epoch 21 | loss: 0.08375 | val_0_auc: 0.99887 |  0:00:30s\n",
      "epoch 22 | loss: 0.09556 | val_0_auc: 0.99856 |  0:00:31s\n",
      "epoch 23 | loss: 0.08588 | val_0_auc: 0.99843 |  0:00:32s\n",
      "epoch 24 | loss: 0.08079 | val_0_auc: 0.99891 |  0:00:34s\n",
      "epoch 25 | loss: 0.08671 | val_0_auc: 0.99893 |  0:00:35s\n",
      "epoch 26 | loss: 0.07857 | val_0_auc: 0.99939 |  0:00:36s\n",
      "epoch 27 | loss: 0.08191 | val_0_auc: 0.99895 |  0:00:38s\n",
      "epoch 28 | loss: 0.08711 | val_0_auc: 0.9992  |  0:00:39s\n",
      "epoch 29 | loss: 0.09693 | val_0_auc: 0.99903 |  0:00:40s\n",
      "epoch 30 | loss: 0.09337 | val_0_auc: 0.99925 |  0:00:42s\n",
      "epoch 31 | loss: 0.08103 | val_0_auc: 0.99892 |  0:00:43s\n",
      "epoch 32 | loss: 0.07151 | val_0_auc: 0.99935 |  0:00:44s\n",
      "epoch 33 | loss: 0.06371 | val_0_auc: 0.9994  |  0:00:46s\n",
      "epoch 34 | loss: 0.07209 | val_0_auc: 0.99944 |  0:00:47s\n",
      "epoch 35 | loss: 0.06878 | val_0_auc: 0.99876 |  0:00:48s\n",
      "epoch 36 | loss: 0.0608  | val_0_auc: 0.99952 |  0:00:50s\n",
      "epoch 37 | loss: 0.06276 | val_0_auc: 0.99931 |  0:00:51s\n",
      "epoch 38 | loss: 0.07301 | val_0_auc: 0.99916 |  0:00:52s\n",
      "epoch 39 | loss: 0.05595 | val_0_auc: 0.99951 |  0:00:54s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99952\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99952331  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 54.8612\n",
      "Function value obtained: -0.9995\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.385407218881786, 'lambda_sparse': 0.0171457867213915, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.88001 | val_0_auc: 0.9257  |  0:00:01s\n",
      "epoch 1  | loss: 0.31613 | val_0_auc: 0.97656 |  0:00:02s\n",
      "epoch 2  | loss: 0.18287 | val_0_auc: 0.9931  |  0:00:03s\n",
      "epoch 3  | loss: 0.11832 | val_0_auc: 0.99531 |  0:00:05s\n",
      "epoch 4  | loss: 0.10104 | val_0_auc: 0.99677 |  0:00:06s\n",
      "epoch 5  | loss: 0.09591 | val_0_auc: 0.99695 |  0:00:07s\n",
      "epoch 6  | loss: 0.09057 | val_0_auc: 0.99704 |  0:00:09s\n",
      "epoch 7  | loss: 0.09154 | val_0_auc: 0.99762 |  0:00:10s\n",
      "epoch 8  | loss: 0.08109 | val_0_auc: 0.99757 |  0:00:11s\n",
      "epoch 9  | loss: 0.07584 | val_0_auc: 0.99811 |  0:00:12s\n",
      "epoch 10 | loss: 0.06817 | val_0_auc: 0.99858 |  0:00:14s\n",
      "epoch 11 | loss: 0.06875 | val_0_auc: 0.99866 |  0:00:15s\n",
      "epoch 12 | loss: 0.08084 | val_0_auc: 0.99745 |  0:00:16s\n",
      "epoch 13 | loss: 0.07632 | val_0_auc: 0.99794 |  0:00:17s\n",
      "epoch 14 | loss: 0.07113 | val_0_auc: 0.99745 |  0:00:19s\n",
      "epoch 15 | loss: 0.07699 | val_0_auc: 0.99799 |  0:00:20s\n",
      "epoch 16 | loss: 0.07691 | val_0_auc: 0.99788 |  0:00:21s\n",
      "epoch 17 | loss: 0.06451 | val_0_auc: 0.99859 |  0:00:23s\n",
      "epoch 18 | loss: 0.05918 | val_0_auc: 0.99826 |  0:00:24s\n",
      "epoch 19 | loss: 0.06771 | val_0_auc: 0.99812 |  0:00:25s\n",
      "epoch 20 | loss: 0.05751 | val_0_auc: 0.9985  |  0:00:26s\n",
      "epoch 21 | loss: 0.05509 | val_0_auc: 0.99843 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.99866\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99865709  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 28.8413\n",
      "Function value obtained: -0.9987\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1662743025300173, 'lambda_sparse': 0.07472893578921683, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.51297 | val_0_auc: 0.94274 |  0:00:01s\n",
      "epoch 1  | loss: 0.27605 | val_0_auc: 0.98196 |  0:00:03s\n",
      "epoch 2  | loss: 0.20604 | val_0_auc: 0.99208 |  0:00:05s\n",
      "epoch 3  | loss: 0.18668 | val_0_auc: 0.99334 |  0:00:06s\n",
      "epoch 4  | loss: 0.14759 | val_0_auc: 0.99572 |  0:00:08s\n",
      "epoch 5  | loss: 0.13291 | val_0_auc: 0.99733 |  0:00:10s\n",
      "epoch 6  | loss: 0.12423 | val_0_auc: 0.99781 |  0:00:11s\n",
      "epoch 7  | loss: 0.10861 | val_0_auc: 0.99875 |  0:00:13s\n",
      "epoch 8  | loss: 0.1072  | val_0_auc: 0.99715 |  0:00:15s\n",
      "epoch 9  | loss: 0.10009 | val_0_auc: 0.99777 |  0:00:16s\n",
      "epoch 10 | loss: 0.11725 | val_0_auc: 0.99768 |  0:00:18s\n",
      "epoch 11 | loss: 0.10946 | val_0_auc: 0.99758 |  0:00:20s\n",
      "epoch 12 | loss: 0.09465 | val_0_auc: 0.99898 |  0:00:21s\n",
      "epoch 13 | loss: 0.09661 | val_0_auc: 0.99936 |  0:00:23s\n",
      "epoch 14 | loss: 0.08088 | val_0_auc: 0.99949 |  0:00:25s\n",
      "epoch 15 | loss: 0.08368 | val_0_auc: 0.99935 |  0:00:26s\n",
      "epoch 16 | loss: 0.08915 | val_0_auc: 0.99938 |  0:00:28s\n",
      "epoch 17 | loss: 0.09284 | val_0_auc: 0.99859 |  0:00:29s\n",
      "epoch 18 | loss: 0.09525 | val_0_auc: 0.999   |  0:00:31s\n",
      "epoch 19 | loss: 0.08423 | val_0_auc: 0.99869 |  0:00:33s\n",
      "epoch 20 | loss: 0.08254 | val_0_auc: 0.9992  |  0:00:34s\n",
      "epoch 21 | loss: 0.07589 | val_0_auc: 0.99933 |  0:00:36s\n",
      "epoch 22 | loss: 0.06926 | val_0_auc: 0.99908 |  0:00:38s\n",
      "epoch 23 | loss: 0.07566 | val_0_auc: 0.99938 |  0:00:39s\n",
      "epoch 24 | loss: 0.06488 | val_0_auc: 0.99934 |  0:00:41s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.99949\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99948596  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 42.3349\n",
      "Function value obtained: -0.9995\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1028778846348775, 'lambda_sparse': 0.0002742660631434305, 'n_steps': 3, 'n_a': 64, 'n_d': 64, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.60954 | val_0_auc: 0.96813 |  0:00:01s\n",
      "epoch 1  | loss: 0.17295 | val_0_auc: 0.99178 |  0:00:02s\n",
      "epoch 2  | loss: 0.10887 | val_0_auc: 0.99504 |  0:00:03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.0765  | val_0_auc: 0.99713 |  0:00:04s\n",
      "epoch 4  | loss: 0.06807 | val_0_auc: 0.99841 |  0:00:05s\n",
      "epoch 5  | loss: 0.07442 | val_0_auc: 0.99811 |  0:00:06s\n",
      "epoch 6  | loss: 0.06235 | val_0_auc: 0.99902 |  0:00:08s\n",
      "epoch 7  | loss: 0.06346 | val_0_auc: 0.99863 |  0:00:09s\n",
      "epoch 8  | loss: 0.05945 | val_0_auc: 0.99914 |  0:00:10s\n",
      "epoch 9  | loss: 0.054   | val_0_auc: 0.99865 |  0:00:11s\n",
      "epoch 10 | loss: 0.05658 | val_0_auc: 0.9991  |  0:00:12s\n",
      "epoch 11 | loss: 0.05827 | val_0_auc: 0.99921 |  0:00:13s\n",
      "epoch 12 | loss: 0.05829 | val_0_auc: 0.99941 |  0:00:14s\n",
      "epoch 13 | loss: 0.0513  | val_0_auc: 0.9994  |  0:00:15s\n",
      "epoch 14 | loss: 0.05362 | val_0_auc: 0.99944 |  0:00:17s\n",
      "epoch 15 | loss: 0.04174 | val_0_auc: 0.99915 |  0:00:18s\n",
      "epoch 16 | loss: 0.05483 | val_0_auc: 0.99932 |  0:00:19s\n",
      "epoch 17 | loss: 0.05989 | val_0_auc: 0.99923 |  0:00:20s\n",
      "epoch 18 | loss: 0.0582  | val_0_auc: 0.99902 |  0:00:21s\n",
      "epoch 19 | loss: 0.05446 | val_0_auc: 0.99937 |  0:00:22s\n",
      "epoch 20 | loss: 0.04525 | val_0_auc: 0.99935 |  0:00:23s\n",
      "epoch 21 | loss: 0.05106 | val_0_auc: 0.99913 |  0:00:25s\n",
      "epoch 22 | loss: 0.05589 | val_0_auc: 0.99928 |  0:00:26s\n",
      "epoch 23 | loss: 0.06181 | val_0_auc: 0.9989  |  0:00:27s\n",
      "epoch 24 | loss: 0.06081 | val_0_auc: 0.99923 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.99944\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99943793  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 28.8118\n",
      "Function value obtained: -0.9994\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6456342095856673, 'lambda_sparse': 0.010471975498290707, 'n_steps': 6, 'n_a': 8, 'n_d': 8, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.83468 | val_0_auc: 0.55464 |  0:00:01s\n",
      "epoch 1  | loss: 0.68003 | val_0_auc: 0.72825 |  0:00:02s\n",
      "epoch 2  | loss: 0.56782 | val_0_auc: 0.84455 |  0:00:03s\n",
      "epoch 3  | loss: 0.44478 | val_0_auc: 0.89665 |  0:00:04s\n",
      "epoch 4  | loss: 0.34874 | val_0_auc: 0.94398 |  0:00:05s\n",
      "epoch 5  | loss: 0.28037 | val_0_auc: 0.96627 |  0:00:06s\n",
      "epoch 6  | loss: 0.23682 | val_0_auc: 0.96002 |  0:00:07s\n",
      "epoch 7  | loss: 0.20795 | val_0_auc: 0.96792 |  0:00:08s\n",
      "epoch 8  | loss: 0.19868 | val_0_auc: 0.97918 |  0:00:09s\n",
      "epoch 9  | loss: 0.19541 | val_0_auc: 0.98353 |  0:00:10s\n",
      "epoch 10 | loss: 0.18088 | val_0_auc: 0.98344 |  0:00:11s\n",
      "epoch 11 | loss: 0.1687  | val_0_auc: 0.9938  |  0:00:12s\n",
      "epoch 12 | loss: 0.14055 | val_0_auc: 0.99377 |  0:00:13s\n",
      "epoch 13 | loss: 0.14053 | val_0_auc: 0.99345 |  0:00:14s\n",
      "epoch 14 | loss: 0.12684 | val_0_auc: 0.99247 |  0:00:15s\n",
      "epoch 15 | loss: 0.12177 | val_0_auc: 0.99234 |  0:00:16s\n",
      "epoch 16 | loss: 0.12358 | val_0_auc: 0.99394 |  0:00:17s\n",
      "epoch 17 | loss: 0.13268 | val_0_auc: 0.99321 |  0:00:18s\n",
      "epoch 18 | loss: 0.1218  | val_0_auc: 0.99426 |  0:00:19s\n",
      "epoch 19 | loss: 0.10786 | val_0_auc: 0.9956  |  0:00:20s\n",
      "epoch 20 | loss: 0.09976 | val_0_auc: 0.99634 |  0:00:21s\n",
      "epoch 21 | loss: 0.1078  | val_0_auc: 0.99512 |  0:00:22s\n",
      "epoch 22 | loss: 0.10712 | val_0_auc: 0.98677 |  0:00:23s\n",
      "epoch 23 | loss: 0.12341 | val_0_auc: 0.99676 |  0:00:24s\n",
      "epoch 24 | loss: 0.10997 | val_0_auc: 0.99682 |  0:00:25s\n",
      "epoch 25 | loss: 0.09692 | val_0_auc: 0.99772 |  0:00:26s\n",
      "epoch 26 | loss: 0.10686 | val_0_auc: 0.99781 |  0:00:27s\n",
      "epoch 27 | loss: 0.09903 | val_0_auc: 0.99755 |  0:00:28s\n",
      "epoch 28 | loss: 0.0938  | val_0_auc: 0.99751 |  0:00:29s\n",
      "epoch 29 | loss: 0.10545 | val_0_auc: 0.99663 |  0:00:30s\n",
      "epoch 30 | loss: 0.10619 | val_0_auc: 0.99576 |  0:00:31s\n",
      "epoch 31 | loss: 0.09838 | val_0_auc: 0.99755 |  0:00:32s\n",
      "epoch 32 | loss: 0.09939 | val_0_auc: 0.99881 |  0:00:33s\n",
      "epoch 33 | loss: 0.08878 | val_0_auc: 0.99836 |  0:00:34s\n",
      "epoch 34 | loss: 0.07962 | val_0_auc: 0.99851 |  0:00:35s\n",
      "epoch 35 | loss: 0.09694 | val_0_auc: 0.99844 |  0:00:36s\n",
      "epoch 36 | loss: 0.08272 | val_0_auc: 0.99817 |  0:00:37s\n",
      "epoch 37 | loss: 0.08927 | val_0_auc: 0.99702 |  0:00:38s\n",
      "epoch 38 | loss: 0.09502 | val_0_auc: 0.99817 |  0:00:39s\n",
      "epoch 39 | loss: 0.08835 | val_0_auc: 0.99816 |  0:00:40s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 32 and best_val_0_auc = 0.99881\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99881183  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 40.6564\n",
      "Function value obtained: -0.9988\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8343332855266667, 'lambda_sparse': 0.05527412137324475, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.49896 | val_0_auc: 0.8194  |  0:00:01s\n",
      "epoch 1  | loss: 0.50071 | val_0_auc: 0.93197 |  0:00:03s\n",
      "epoch 2  | loss: 0.2836  | val_0_auc: 0.98649 |  0:00:05s\n",
      "epoch 3  | loss: 0.19333 | val_0_auc: 0.99027 |  0:00:06s\n",
      "epoch 4  | loss: 0.14574 | val_0_auc: 0.9921  |  0:00:08s\n",
      "epoch 5  | loss: 0.1237  | val_0_auc: 0.99575 |  0:00:09s\n",
      "epoch 6  | loss: 0.11115 | val_0_auc: 0.99562 |  0:00:11s\n",
      "epoch 7  | loss: 0.09497 | val_0_auc: 0.9967  |  0:00:13s\n",
      "epoch 8  | loss: 0.09728 | val_0_auc: 0.99616 |  0:00:14s\n",
      "epoch 9  | loss: 0.08937 | val_0_auc: 0.99657 |  0:00:16s\n",
      "epoch 10 | loss: 0.10222 | val_0_auc: 0.99623 |  0:00:18s\n",
      "epoch 11 | loss: 0.10686 | val_0_auc: 0.99695 |  0:00:19s\n",
      "epoch 12 | loss: 0.09855 | val_0_auc: 0.99612 |  0:00:21s\n",
      "epoch 13 | loss: 0.08915 | val_0_auc: 0.99704 |  0:00:23s\n",
      "epoch 14 | loss: 0.07862 | val_0_auc: 0.9988  |  0:00:24s\n",
      "epoch 15 | loss: 0.08723 | val_0_auc: 0.9983  |  0:00:26s\n",
      "epoch 16 | loss: 0.09325 | val_0_auc: 0.99823 |  0:00:28s\n",
      "epoch 17 | loss: 0.093   | val_0_auc: 0.99706 |  0:00:29s\n",
      "epoch 18 | loss: 0.09441 | val_0_auc: 0.99682 |  0:00:31s\n",
      "epoch 19 | loss: 0.08762 | val_0_auc: 0.99716 |  0:00:33s\n",
      "epoch 20 | loss: 0.08181 | val_0_auc: 0.99665 |  0:00:34s\n",
      "epoch 21 | loss: 0.07749 | val_0_auc: 0.99699 |  0:00:36s\n",
      "epoch 22 | loss: 0.06818 | val_0_auc: 0.9976  |  0:00:38s\n",
      "epoch 23 | loss: 0.07704 | val_0_auc: 0.99758 |  0:00:39s\n",
      "epoch 24 | loss: 0.0643  | val_0_auc: 0.99695 |  0:00:41s\n",
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_val_0_auc = 0.9988\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99879938  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 42.2314\n",
      "Function value obtained: -0.9988\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3209474623488588, 'lambda_sparse': 0.01821335707305702, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.20715 | val_0_auc: 0.81068 |  0:00:01s\n",
      "epoch 1  | loss: 0.53334 | val_0_auc: 0.93707 |  0:00:03s\n",
      "epoch 2  | loss: 0.3041  | val_0_auc: 0.98056 |  0:00:05s\n",
      "epoch 3  | loss: 0.18739 | val_0_auc: 0.99073 |  0:00:07s\n",
      "epoch 4  | loss: 0.16201 | val_0_auc: 0.9914  |  0:00:09s\n",
      "epoch 5  | loss: 0.11975 | val_0_auc: 0.99305 |  0:00:11s\n",
      "epoch 6  | loss: 0.11967 | val_0_auc: 0.99562 |  0:00:13s\n",
      "epoch 7  | loss: 0.11019 | val_0_auc: 0.99599 |  0:00:15s\n",
      "epoch 8  | loss: 0.10676 | val_0_auc: 0.99602 |  0:00:17s\n",
      "epoch 9  | loss: 0.11705 | val_0_auc: 0.996   |  0:00:19s\n",
      "epoch 10 | loss: 0.12281 | val_0_auc: 0.9959  |  0:00:21s\n",
      "epoch 11 | loss: 0.10108 | val_0_auc: 0.99755 |  0:00:22s\n",
      "epoch 12 | loss: 0.15971 | val_0_auc: 0.99698 |  0:00:24s\n",
      "epoch 13 | loss: 0.18754 | val_0_auc: 0.99726 |  0:00:26s\n",
      "epoch 14 | loss: 0.08635 | val_0_auc: 0.99755 |  0:00:28s\n",
      "epoch 15 | loss: 0.08472 | val_0_auc: 0.99787 |  0:00:30s\n",
      "epoch 16 | loss: 0.08364 | val_0_auc: 0.99881 |  0:00:32s\n",
      "epoch 17 | loss: 0.07526 | val_0_auc: 0.99709 |  0:00:34s\n",
      "epoch 18 | loss: 0.08234 | val_0_auc: 0.99805 |  0:00:36s\n",
      "epoch 19 | loss: 0.09985 | val_0_auc: 0.99887 |  0:00:38s\n",
      "epoch 20 | loss: 0.07198 | val_0_auc: 0.99851 |  0:00:40s\n",
      "epoch 21 | loss: 0.07333 | val_0_auc: 0.99804 |  0:00:42s\n",
      "epoch 22 | loss: 0.07352 | val_0_auc: 0.99824 |  0:00:44s\n",
      "epoch 23 | loss: 0.0791  | val_0_auc: 0.99781 |  0:00:46s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24 | loss: 0.07347 | val_0_auc: 0.99886 |  0:00:47s\n",
      "epoch 25 | loss: 0.06948 | val_0_auc: 0.99878 |  0:00:49s\n",
      "epoch 26 | loss: 0.06373 | val_0_auc: 0.99852 |  0:00:51s\n",
      "epoch 27 | loss: 0.06726 | val_0_auc: 0.99861 |  0:00:53s\n",
      "epoch 28 | loss: 0.06921 | val_0_auc: 0.99838 |  0:00:55s\n",
      "epoch 29 | loss: 0.06541 | val_0_auc: 0.99862 |  0:00:57s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.99887\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99886519  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 58.1163\n",
      "Function value obtained: -0.9989\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0965473019288485, 'lambda_sparse': 0.06054354302699259, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.75134 | val_0_auc: 0.86783 |  0:00:01s\n",
      "epoch 1  | loss: 0.38603 | val_0_auc: 0.97027 |  0:00:02s\n",
      "epoch 2  | loss: 0.24029 | val_0_auc: 0.98839 |  0:00:03s\n",
      "epoch 3  | loss: 0.17163 | val_0_auc: 0.99288 |  0:00:04s\n",
      "epoch 4  | loss: 0.14908 | val_0_auc: 0.99429 |  0:00:06s\n",
      "epoch 5  | loss: 0.13271 | val_0_auc: 0.99609 |  0:00:07s\n",
      "epoch 6  | loss: 0.1231  | val_0_auc: 0.99659 |  0:00:08s\n",
      "epoch 7  | loss: 0.12871 | val_0_auc: 0.99754 |  0:00:09s\n",
      "epoch 8  | loss: 0.10039 | val_0_auc: 0.99753 |  0:00:11s\n",
      "epoch 9  | loss: 0.11238 | val_0_auc: 0.99841 |  0:00:12s\n",
      "epoch 10 | loss: 0.11559 | val_0_auc: 0.99816 |  0:00:13s\n",
      "epoch 11 | loss: 0.1264  | val_0_auc: 0.99773 |  0:00:14s\n",
      "epoch 12 | loss: 0.1018  | val_0_auc: 0.99823 |  0:00:16s\n",
      "epoch 13 | loss: 0.09864 | val_0_auc: 0.99736 |  0:00:17s\n",
      "epoch 14 | loss: 0.09396 | val_0_auc: 0.9975  |  0:00:18s\n",
      "epoch 15 | loss: 0.08969 | val_0_auc: 0.99885 |  0:00:19s\n",
      "epoch 16 | loss: 0.08942 | val_0_auc: 0.99899 |  0:00:21s\n",
      "epoch 17 | loss: 0.09651 | val_0_auc: 0.99884 |  0:00:22s\n",
      "epoch 18 | loss: 0.09523 | val_0_auc: 0.99873 |  0:00:23s\n",
      "epoch 19 | loss: 0.08728 | val_0_auc: 0.99903 |  0:00:24s\n",
      "epoch 20 | loss: 0.0792  | val_0_auc: 0.99843 |  0:00:25s\n",
      "epoch 21 | loss: 0.08766 | val_0_auc: 0.99844 |  0:00:27s\n",
      "epoch 22 | loss: 0.08784 | val_0_auc: 0.9985  |  0:00:28s\n",
      "epoch 23 | loss: 0.07512 | val_0_auc: 0.99882 |  0:00:29s\n",
      "epoch 24 | loss: 0.09167 | val_0_auc: 0.99817 |  0:00:30s\n",
      "epoch 25 | loss: 0.08761 | val_0_auc: 0.99865 |  0:00:31s\n",
      "epoch 26 | loss: 0.09132 | val_0_auc: 0.99894 |  0:00:33s\n",
      "epoch 27 | loss: 0.08424 | val_0_auc: 0.99894 |  0:00:34s\n",
      "epoch 28 | loss: 0.10345 | val_0_auc: 0.99846 |  0:00:35s\n",
      "epoch 29 | loss: 0.07375 | val_0_auc: 0.99866 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.99903\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99902528  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 37.2168\n",
      "Function value obtained: -0.9990\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5104597268907485, 'lambda_sparse': 0.09447434688007225, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.78937 | val_0_auc: 0.85415 |  0:00:00s\n",
      "epoch 1  | loss: 0.36171 | val_0_auc: 0.98203 |  0:00:01s\n",
      "epoch 2  | loss: 0.19042 | val_0_auc: 0.9912  |  0:00:02s\n",
      "epoch 3  | loss: 0.15213 | val_0_auc: 0.99594 |  0:00:03s\n",
      "epoch 4  | loss: 0.14644 | val_0_auc: 0.99589 |  0:00:04s\n",
      "epoch 5  | loss: 0.11925 | val_0_auc: 0.99578 |  0:00:05s\n",
      "epoch 6  | loss: 0.11754 | val_0_auc: 0.99598 |  0:00:06s\n",
      "epoch 7  | loss: 0.11142 | val_0_auc: 0.99676 |  0:00:07s\n",
      "epoch 8  | loss: 0.10763 | val_0_auc: 0.9967  |  0:00:08s\n",
      "epoch 9  | loss: 0.10725 | val_0_auc: 0.99723 |  0:00:08s\n",
      "epoch 10 | loss: 0.10832 | val_0_auc: 0.99691 |  0:00:09s\n",
      "epoch 11 | loss: 0.10154 | val_0_auc: 0.99722 |  0:00:10s\n",
      "epoch 12 | loss: 0.10467 | val_0_auc: 0.99627 |  0:00:11s\n",
      "epoch 13 | loss: 0.10087 | val_0_auc: 0.99714 |  0:00:12s\n",
      "epoch 14 | loss: 0.09228 | val_0_auc: 0.99764 |  0:00:13s\n",
      "epoch 15 | loss: 0.09686 | val_0_auc: 0.99707 |  0:00:14s\n",
      "epoch 16 | loss: 0.09068 | val_0_auc: 0.99835 |  0:00:15s\n",
      "epoch 17 | loss: 0.08493 | val_0_auc: 0.99765 |  0:00:16s\n",
      "epoch 18 | loss: 0.09708 | val_0_auc: 0.99713 |  0:00:17s\n",
      "epoch 19 | loss: 0.08965 | val_0_auc: 0.99597 |  0:00:17s\n",
      "epoch 20 | loss: 0.08198 | val_0_auc: 0.99643 |  0:00:18s\n",
      "epoch 21 | loss: 0.09455 | val_0_auc: 0.99616 |  0:00:19s\n",
      "epoch 22 | loss: 0.08958 | val_0_auc: 0.99613 |  0:00:20s\n",
      "epoch 23 | loss: 0.09238 | val_0_auc: 0.99706 |  0:00:21s\n",
      "epoch 24 | loss: 0.09379 | val_0_auc: 0.9973  |  0:00:22s\n",
      "epoch 25 | loss: 0.08813 | val_0_auc: 0.99682 |  0:00:23s\n",
      "epoch 26 | loss: 0.08177 | val_0_auc: 0.99684 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.99835\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99835115  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 24.4212\n",
      "Function value obtained: -0.9984\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4954828779900153, 'lambda_sparse': 0.0854072330392144, 'n_steps': 5, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.82283 | val_0_auc: 0.85835 |  0:00:01s\n",
      "epoch 1  | loss: 0.45154 | val_0_auc: 0.95588 |  0:00:02s\n",
      "epoch 2  | loss: 0.25416 | val_0_auc: 0.98461 |  0:00:03s\n",
      "epoch 3  | loss: 0.17627 | val_0_auc: 0.99161 |  0:00:04s\n",
      "epoch 4  | loss: 0.1483  | val_0_auc: 0.99438 |  0:00:05s\n",
      "epoch 5  | loss: 0.12624 | val_0_auc: 0.99572 |  0:00:06s\n",
      "epoch 6  | loss: 0.11935 | val_0_auc: 0.9952  |  0:00:07s\n",
      "epoch 7  | loss: 0.11808 | val_0_auc: 0.99673 |  0:00:08s\n",
      "epoch 8  | loss: 0.10854 | val_0_auc: 0.99739 |  0:00:09s\n",
      "epoch 9  | loss: 0.10647 | val_0_auc: 0.99685 |  0:00:10s\n",
      "epoch 10 | loss: 0.10752 | val_0_auc: 0.99621 |  0:00:11s\n",
      "epoch 11 | loss: 0.10475 | val_0_auc: 0.99656 |  0:00:12s\n",
      "epoch 12 | loss: 0.12342 | val_0_auc: 0.99807 |  0:00:13s\n",
      "epoch 13 | loss: 0.10983 | val_0_auc: 0.99777 |  0:00:14s\n",
      "epoch 14 | loss: 0.10437 | val_0_auc: 0.9984  |  0:00:16s\n",
      "epoch 15 | loss: 0.09942 | val_0_auc: 0.99855 |  0:00:17s\n",
      "epoch 16 | loss: 0.09042 | val_0_auc: 0.99711 |  0:00:18s\n",
      "epoch 17 | loss: 0.08372 | val_0_auc: 0.99812 |  0:00:19s\n",
      "epoch 18 | loss: 0.07857 | val_0_auc: 0.99883 |  0:00:20s\n",
      "epoch 19 | loss: 0.09166 | val_0_auc: 0.99887 |  0:00:21s\n",
      "epoch 20 | loss: 0.08768 | val_0_auc: 0.99883 |  0:00:22s\n",
      "epoch 21 | loss: 0.08439 | val_0_auc: 0.99879 |  0:00:23s\n",
      "epoch 22 | loss: 0.08597 | val_0_auc: 0.99837 |  0:00:24s\n",
      "epoch 23 | loss: 0.07858 | val_0_auc: 0.9986  |  0:00:25s\n",
      "epoch 24 | loss: 0.0783  | val_0_auc: 0.99918 |  0:00:26s\n",
      "epoch 25 | loss: 0.07612 | val_0_auc: 0.99896 |  0:00:27s\n",
      "epoch 26 | loss: 0.0805  | val_0_auc: 0.99856 |  0:00:28s\n",
      "epoch 27 | loss: 0.07068 | val_0_auc: 0.99897 |  0:00:29s\n",
      "epoch 28 | loss: 0.07328 | val_0_auc: 0.99886 |  0:00:30s\n",
      "epoch 29 | loss: 0.07982 | val_0_auc: 0.99885 |  0:00:31s\n",
      "epoch 30 | loss: 0.0797  | val_0_auc: 0.99866 |  0:00:32s\n",
      "epoch 31 | loss: 0.07881 | val_0_auc: 0.99863 |  0:00:34s\n",
      "epoch 32 | loss: 0.07744 | val_0_auc: 0.99913 |  0:00:35s\n",
      "epoch 33 | loss: 0.07103 | val_0_auc: 0.99876 |  0:00:36s\n",
      "epoch 34 | loss: 0.08136 | val_0_auc: 0.99877 |  0:00:37s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.99918\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99917647  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 37.5630\n",
      "Function value obtained: -0.9992\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7238371014151372, 'lambda_sparse': 0.046149767520849, 'n_steps': 8, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.85267 | val_0_auc: 0.68301 |  0:00:01s\n",
      "epoch 1  | loss: 0.60594 | val_0_auc: 0.83935 |  0:00:02s\n",
      "epoch 2  | loss: 0.50357 | val_0_auc: 0.8835  |  0:00:03s\n",
      "epoch 3  | loss: 0.40762 | val_0_auc: 0.92038 |  0:00:05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 0.34033 | val_0_auc: 0.94511 |  0:00:06s\n",
      "epoch 5  | loss: 0.29876 | val_0_auc: 0.9654  |  0:00:07s\n",
      "epoch 6  | loss: 0.2568  | val_0_auc: 0.97523 |  0:00:09s\n",
      "epoch 7  | loss: 0.23663 | val_0_auc: 0.97779 |  0:00:10s\n",
      "epoch 8  | loss: 0.21048 | val_0_auc: 0.98077 |  0:00:11s\n",
      "epoch 9  | loss: 0.19202 | val_0_auc: 0.98901 |  0:00:13s\n",
      "epoch 10 | loss: 0.18233 | val_0_auc: 0.98861 |  0:00:14s\n",
      "epoch 11 | loss: 0.18749 | val_0_auc: 0.97358 |  0:00:15s\n",
      "epoch 12 | loss: 0.20347 | val_0_auc: 0.98458 |  0:00:16s\n",
      "epoch 13 | loss: 0.18569 | val_0_auc: 0.98249 |  0:00:18s\n",
      "epoch 14 | loss: 0.16075 | val_0_auc: 0.98848 |  0:00:19s\n",
      "epoch 15 | loss: 0.13095 | val_0_auc: 0.99463 |  0:00:20s\n",
      "epoch 16 | loss: 0.11227 | val_0_auc: 0.99606 |  0:00:21s\n",
      "epoch 17 | loss: 0.10885 | val_0_auc: 0.99621 |  0:00:23s\n",
      "epoch 18 | loss: 0.10951 | val_0_auc: 0.99472 |  0:00:24s\n",
      "epoch 19 | loss: 0.11351 | val_0_auc: 0.99446 |  0:00:25s\n",
      "epoch 20 | loss: 0.10466 | val_0_auc: 0.99535 |  0:00:27s\n",
      "epoch 21 | loss: 0.11715 | val_0_auc: 0.99675 |  0:00:28s\n",
      "epoch 22 | loss: 0.10171 | val_0_auc: 0.99589 |  0:00:29s\n",
      "epoch 23 | loss: 0.10163 | val_0_auc: 0.99682 |  0:00:30s\n",
      "epoch 24 | loss: 0.09665 | val_0_auc: 0.99584 |  0:00:32s\n",
      "epoch 25 | loss: 0.09491 | val_0_auc: 0.99621 |  0:00:33s\n",
      "epoch 26 | loss: 0.09961 | val_0_auc: 0.99626 |  0:00:34s\n",
      "epoch 27 | loss: 0.10142 | val_0_auc: 0.99682 |  0:00:35s\n",
      "epoch 28 | loss: 0.09347 | val_0_auc: 0.99629 |  0:00:37s\n",
      "epoch 29 | loss: 0.10629 | val_0_auc: 0.99727 |  0:00:38s\n",
      "epoch 30 | loss: 0.10248 | val_0_auc: 0.99731 |  0:00:39s\n",
      "epoch 31 | loss: 0.0923  | val_0_auc: 0.99743 |  0:00:41s\n",
      "epoch 32 | loss: 0.07782 | val_0_auc: 0.9973  |  0:00:42s\n",
      "epoch 33 | loss: 0.08898 | val_0_auc: 0.99808 |  0:00:43s\n",
      "epoch 34 | loss: 0.07804 | val_0_auc: 0.99852 |  0:00:44s\n",
      "epoch 35 | loss: 0.094   | val_0_auc: 0.99777 |  0:00:46s\n",
      "epoch 36 | loss: 0.08776 | val_0_auc: 0.99824 |  0:00:47s\n",
      "epoch 37 | loss: 0.0854  | val_0_auc: 0.99755 |  0:00:48s\n",
      "epoch 38 | loss: 0.07259 | val_0_auc: 0.99713 |  0:00:50s\n",
      "epoch 39 | loss: 0.08588 | val_0_auc: 0.99739 |  0:00:51s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.99852\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99851835  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 51.8818\n",
      "Function value obtained: -0.9985\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6954345569898157, 'lambda_sparse': 0.016342964413632637, 'n_steps': 4, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.68815 | val_0_auc: 0.95588 |  0:00:01s\n",
      "epoch 1  | loss: 0.23478 | val_0_auc: 0.98805 |  0:00:02s\n",
      "epoch 2  | loss: 0.14019 | val_0_auc: 0.99527 |  0:00:03s\n",
      "epoch 3  | loss: 0.10487 | val_0_auc: 0.99553 |  0:00:04s\n",
      "epoch 4  | loss: 0.09188 | val_0_auc: 0.99587 |  0:00:05s\n",
      "epoch 5  | loss: 0.08711 | val_0_auc: 0.99672 |  0:00:06s\n",
      "epoch 6  | loss: 0.09128 | val_0_auc: 0.99703 |  0:00:07s\n",
      "epoch 7  | loss: 0.09665 | val_0_auc: 0.99706 |  0:00:08s\n",
      "epoch 8  | loss: 0.0826  | val_0_auc: 0.99639 |  0:00:09s\n",
      "epoch 9  | loss: 0.09226 | val_0_auc: 0.99757 |  0:00:10s\n",
      "epoch 10 | loss: 0.06682 | val_0_auc: 0.99782 |  0:00:12s\n",
      "epoch 11 | loss: 0.06918 | val_0_auc: 0.99878 |  0:00:13s\n",
      "epoch 12 | loss: 0.0708  | val_0_auc: 0.99797 |  0:00:14s\n",
      "epoch 13 | loss: 0.07186 | val_0_auc: 0.99684 |  0:00:15s\n",
      "epoch 14 | loss: 0.06933 | val_0_auc: 0.9965  |  0:00:16s\n",
      "epoch 15 | loss: 0.07339 | val_0_auc: 0.99742 |  0:00:17s\n",
      "epoch 16 | loss: 0.06869 | val_0_auc: 0.99731 |  0:00:18s\n",
      "epoch 17 | loss: 0.06533 | val_0_auc: 0.99812 |  0:00:19s\n",
      "epoch 18 | loss: 0.05707 | val_0_auc: 0.9981  |  0:00:20s\n",
      "epoch 19 | loss: 0.0626  | val_0_auc: 0.99865 |  0:00:21s\n",
      "epoch 20 | loss: 0.07425 | val_0_auc: 0.99893 |  0:00:22s\n",
      "epoch 21 | loss: 0.06203 | val_0_auc: 0.99919 |  0:00:24s\n",
      "epoch 22 | loss: 0.05229 | val_0_auc: 0.99927 |  0:00:25s\n",
      "epoch 23 | loss: 0.05008 | val_0_auc: 0.9993  |  0:00:26s\n",
      "epoch 24 | loss: 0.05778 | val_0_auc: 0.9991  |  0:00:27s\n",
      "epoch 25 | loss: 0.05933 | val_0_auc: 0.99922 |  0:00:28s\n",
      "epoch 26 | loss: 0.04696 | val_0_auc: 0.99913 |  0:00:29s\n",
      "epoch 27 | loss: 0.0585  | val_0_auc: 0.99904 |  0:00:30s\n",
      "epoch 28 | loss: 0.05385 | val_0_auc: 0.99862 |  0:00:31s\n",
      "epoch 29 | loss: 0.06234 | val_0_auc: 0.99901 |  0:00:32s\n",
      "epoch 30 | loss: 0.0584  | val_0_auc: 0.99881 |  0:00:34s\n",
      "epoch 31 | loss: 0.06029 | val_0_auc: 0.99897 |  0:00:35s\n",
      "epoch 32 | loss: 0.05147 | val_0_auc: 0.99906 |  0:00:36s\n",
      "epoch 33 | loss: 0.05275 | val_0_auc: 0.99925 |  0:00:37s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_auc = 0.9993\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99930275  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 37.6723\n",
      "Function value obtained: -0.9993\n",
      "Current minimum: -0.9996\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7715273906807025, 'lambda_sparse': 0.06039977558447745, 'n_steps': 3, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.69599 | val_0_auc: 0.84815 |  0:00:00s\n",
      "epoch 1  | loss: 0.38484 | val_0_auc: 0.96957 |  0:00:01s\n",
      "epoch 2  | loss: 0.21395 | val_0_auc: 0.98854 |  0:00:02s\n",
      "epoch 3  | loss: 0.13082 | val_0_auc: 0.99631 |  0:00:02s\n",
      "epoch 4  | loss: 0.10383 | val_0_auc: 0.99625 |  0:00:03s\n",
      "epoch 5  | loss: 0.09604 | val_0_auc: 0.9968  |  0:00:04s\n",
      "epoch 6  | loss: 0.08721 | val_0_auc: 0.99631 |  0:00:05s\n",
      "epoch 7  | loss: 0.08642 | val_0_auc: 0.99709 |  0:00:05s\n",
      "epoch 8  | loss: 0.08369 | val_0_auc: 0.99664 |  0:00:06s\n",
      "epoch 9  | loss: 0.08133 | val_0_auc: 0.99739 |  0:00:07s\n",
      "epoch 10 | loss: 0.07639 | val_0_auc: 0.99762 |  0:00:08s\n",
      "epoch 11 | loss: 0.08206 | val_0_auc: 0.99703 |  0:00:08s\n",
      "epoch 12 | loss: 0.0756  | val_0_auc: 0.99774 |  0:00:09s\n",
      "epoch 13 | loss: 0.07406 | val_0_auc: 0.99772 |  0:00:10s\n",
      "epoch 14 | loss: 0.06789 | val_0_auc: 0.99808 |  0:00:10s\n",
      "epoch 15 | loss: 0.06598 | val_0_auc: 0.99821 |  0:00:11s\n",
      "epoch 16 | loss: 0.07233 | val_0_auc: 0.99813 |  0:00:12s\n",
      "epoch 17 | loss: 0.07445 | val_0_auc: 0.99809 |  0:00:13s\n",
      "epoch 18 | loss: 0.06472 | val_0_auc: 0.99832 |  0:00:13s\n",
      "epoch 19 | loss: 0.06733 | val_0_auc: 0.99831 |  0:00:14s\n",
      "epoch 20 | loss: 0.07189 | val_0_auc: 0.99853 |  0:00:15s\n",
      "epoch 21 | loss: 0.07229 | val_0_auc: 0.99817 |  0:00:16s\n",
      "epoch 22 | loss: 0.06322 | val_0_auc: 0.99803 |  0:00:16s\n",
      "epoch 23 | loss: 0.07057 | val_0_auc: 0.99764 |  0:00:17s\n",
      "epoch 24 | loss: 0.0608  | val_0_auc: 0.99809 |  0:00:18s\n",
      "epoch 25 | loss: 0.07102 | val_0_auc: 0.99676 |  0:00:19s\n",
      "epoch 26 | loss: 0.07337 | val_0_auc: 0.99772 |  0:00:19s\n",
      "epoch 27 | loss: 0.07042 | val_0_auc: 0.99808 |  0:00:20s\n",
      "epoch 28 | loss: 0.0649  | val_0_auc: 0.99839 |  0:00:21s\n",
      "epoch 29 | loss: 0.06689 | val_0_auc: 0.99872 |  0:00:22s\n",
      "epoch 30 | loss: 0.07485 | val_0_auc: 0.99774 |  0:00:22s\n",
      "epoch 31 | loss: 0.07208 | val_0_auc: 0.99769 |  0:00:23s\n",
      "epoch 32 | loss: 0.06252 | val_0_auc: 0.99754 |  0:00:24s\n",
      "epoch 33 | loss: 0.06885 | val_0_auc: 0.99793 |  0:00:24s\n",
      "epoch 34 | loss: 0.0621  | val_0_auc: 0.99804 |  0:00:25s\n",
      "epoch 35 | loss: 0.06217 | val_0_auc: 0.99807 |  0:00:26s\n",
      "epoch 36 | loss: 0.0592  | val_0_auc: 0.9971  |  0:00:27s\n",
      "epoch 37 | loss: 0.06214 | val_0_auc: 0.99783 |  0:00:27s\n",
      "epoch 38 | loss: 0.05771 | val_0_auc: 0.99811 |  0:00:28s\n",
      "epoch 39 | loss: 0.05773 | val_0_auc: 0.9984  |  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.99872\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99871934  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 32.2248\n",
      "Function value obtained: -0.9987\n",
      "Current minimum: -0.9996\n",
      "STARTED: syn4\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2618422323665017, 'max_depth': 11, 'n_estimators': 965}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99647772  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.0590\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.35928516953244405, 'max_depth': 10, 'n_estimators': 795}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9962975  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.0516\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1637636260209585, 'max_depth': 3, 'n_estimators': 662}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99545009  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.0967\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.15741455556204803, 'max_depth': 14, 'n_estimators': 636}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99622056  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.0738\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.45689542352533163, 'max_depth': 5, 'n_estimators': 378}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99497222  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.0636\n",
      "Function value obtained: -0.9950\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1681301409348099, 'max_depth': 8, 'n_estimators': 888}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99575686  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.0608\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.20449764072513119, 'max_depth': 15, 'n_estimators': 132}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99602414  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.0651\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19972532267575213, 'max_depth': 7, 'n_estimators': 801}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9959148  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.0700\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29426537134882524, 'max_depth': 15, 'n_estimators': 144}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99617702  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.0633\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.20042320594586907, 'max_depth': 6, 'n_estimators': 936}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99494994  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.0494\n",
      "Function value obtained: -0.9949\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.222238759023084, 'max_depth': 9, 'n_estimators': 612}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99584089  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0520\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3980769172790416, 'max_depth': 9, 'n_estimators': 790}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99566777  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.0480\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10635982140841849, 'max_depth': 6, 'n_estimators': 274}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99542782  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.0859\n",
      "Function value obtained: -0.9954\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4762601941798785, 'max_depth': 11, 'n_estimators': 757}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99571333  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0429\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.21613749498260457, 'max_depth': 14, 'n_estimators': 172}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99571333  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0581\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3217711519070413, 'max_depth': 14, 'n_estimators': 714}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99610109  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0552\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.42883330082778537, 'max_depth': 7, 'n_estimators': 503}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99551185  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.0535\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.21300586849563352, 'max_depth': 4, 'n_estimators': 690}\n",
      "AUC:  0.99561006  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.0844\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.17825829237258362, 'max_depth': 7, 'n_estimators': 102}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99601098  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.0645\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.39929111510454096, 'max_depth': 11, 'n_estimators': 184}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99496209  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.0727\n",
      "Function value obtained: -0.9950\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.22352612365728863, 'max_depth': 5, 'n_estimators': 634}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99621347  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.0832\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19356797741009146, 'max_depth': 10, 'n_estimators': 767}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99613551  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.0591\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4214403298510767, 'max_depth': 12, 'n_estimators': 132}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99477176  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.0471\n",
      "Function value obtained: -0.9948\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.47490937447151327, 'max_depth': 7, 'n_estimators': 613}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99609198  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0472\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.34895930492748517, 'max_depth': 5, 'n_estimators': 639}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99523647  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.0580\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29464696289973835, 'max_depth': 7, 'n_estimators': 264}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99612033  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.0653\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3850079119248977, 'max_depth': 3, 'n_estimators': 590}\n",
      "AUC:  0.99559588  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.0482\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14599606174226476, 'max_depth': 5, 'n_estimators': 283}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99634104  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.1194\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18390412383800683, 'max_depth': 12, 'n_estimators': 698}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99595631  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.0879\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2943539525326253, 'max_depth': 12, 'n_estimators': 857}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99610109  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.8998\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9692931102160587, 'max_depth': 7, 'n_estimators': 689}\n",
      "[02:30:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99482339  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.1017\n",
      "Function value obtained: -0.9948\n",
      "Current minimum: -0.9948\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.375761252778935, 'max_depth': 7, 'n_estimators': 808}\n",
      "[02:30:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99571535  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.1536\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9957\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2728811118818988, 'max_depth': 14, 'n_estimators': 102}\n",
      "[02:30:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9957842  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.2234\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9958\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8066845578222654, 'max_depth': 8, 'n_estimators': 621}\n",
      "[02:30:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99546629  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.1569\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9958\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3631097684028518, 'max_depth': 8, 'n_estimators': 146}\n",
      "[02:30:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99563942  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.1583\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9958\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16891609682368403, 'max_depth': 12, 'n_estimators': 515}\n",
      "[02:30:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99592999  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.2836\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9959\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3535422004334621, 'max_depth': 7, 'n_estimators': 969}\n",
      "[02:30:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99624182  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.1648\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3932816497232916, 'max_depth': 10, 'n_estimators': 318}\n",
      "[02:30:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9957842  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.2036\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10195069480442868, 'max_depth': 11, 'n_estimators': 743}\n",
      "[02:30:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9960859  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.5249\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5683716940584912, 'max_depth': 9, 'n_estimators': 811}\n",
      "[02:30:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.9957761  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.1430\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4147717693156282, 'max_depth': 4, 'n_estimators': 222}\n",
      "[02:30:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99581963  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.1523\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5772121237921234, 'max_depth': 8, 'n_estimators': 368}\n",
      "[02:30:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99585203  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1357\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.222080535072046, 'max_depth': 11, 'n_estimators': 664}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99568093  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.2377\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9040253577658248, 'max_depth': 4, 'n_estimators': 187}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99518584  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0999\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7827978406978323, 'max_depth': 13, 'n_estimators': 683}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99417846  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.1239\n",
      "Function value obtained: -0.9942\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2484391299555594, 'max_depth': 9, 'n_estimators': 576}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99584494  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.1833\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8009889126022082, 'max_depth': 7, 'n_estimators': 908}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99509776  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.1389\n",
      "Function value obtained: -0.9951\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9919972176514944, 'max_depth': 11, 'n_estimators': 749}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99500867  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.1523\n",
      "Function value obtained: -0.9950\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7840122827828321, 'max_depth': 5, 'n_estimators': 277}\n",
      "[02:30:30] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99475151  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.1188\n",
      "Function value obtained: -0.9948\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6685521691110888, 'max_depth': 12, 'n_estimators': 144}\n",
      "[02:30:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99561208  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.1753\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.454641091155373, 'max_depth': 13, 'n_estimators': 651}\n",
      "[02:30:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99580242  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.1628\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43446397851417085, 'max_depth': 13, 'n_estimators': 325}\n",
      "[02:30:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99586519  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.2992\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.47045349492133826, 'max_depth': 11, 'n_estimators': 645}\n",
      "[02:30:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9956627  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.2594\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.12390184441566901, 'max_depth': 10, 'n_estimators': 722}\n",
      "[02:30:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99568498  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.3847\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4751629046143415, 'max_depth': 7, 'n_estimators': 297}\n",
      "[02:30:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.9959148  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.1445\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19737783776466022, 'max_depth': 14, 'n_estimators': 167}\n",
      "[02:30:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99600086  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.2689\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.536455600874859, 'max_depth': 10, 'n_estimators': 195}\n",
      "[02:30:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99562626  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.1407\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6659250536237906, 'max_depth': 13, 'n_estimators': 687}\n",
      "[02:30:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99524963  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.1773\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6547090900980251, 'max_depth': 14, 'n_estimators': 596}\n",
      "[02:30:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99471303  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.1833\n",
      "Function value obtained: -0.9947\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14385962297943533, 'max_depth': 9, 'n_estimators': 947}\n",
      "[02:30:33] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99608185  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.7605\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9119985194567728, 'lambda_sparse': 0.04557921291651004, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.75132 | val_0_auc: 0.84747 |  0:00:01s\n",
      "epoch 1  | loss: 0.42977 | val_0_auc: 0.88509 |  0:00:02s\n",
      "epoch 2  | loss: 0.3379  | val_0_auc: 0.90652 |  0:00:03s\n",
      "epoch 3  | loss: 0.31038 | val_0_auc: 0.90023 |  0:00:04s\n",
      "epoch 4  | loss: 0.27225 | val_0_auc: 0.91888 |  0:00:06s\n",
      "epoch 5  | loss: 0.26931 | val_0_auc: 0.92738 |  0:00:07s\n",
      "epoch 6  | loss: 0.25133 | val_0_auc: 0.92079 |  0:00:08s\n",
      "epoch 7  | loss: 0.24024 | val_0_auc: 0.94027 |  0:00:09s\n",
      "epoch 8  | loss: 0.2114  | val_0_auc: 0.94492 |  0:00:10s\n",
      "epoch 9  | loss: 0.21168 | val_0_auc: 0.95262 |  0:00:12s\n",
      "epoch 10 | loss: 0.19531 | val_0_auc: 0.95192 |  0:00:13s\n",
      "epoch 11 | loss: 0.19217 | val_0_auc: 0.95526 |  0:00:14s\n",
      "epoch 12 | loss: 0.18959 | val_0_auc: 0.95537 |  0:00:15s\n",
      "epoch 13 | loss: 0.18004 | val_0_auc: 0.96078 |  0:00:16s\n",
      "epoch 14 | loss: 0.19521 | val_0_auc: 0.95129 |  0:00:18s\n",
      "epoch 15 | loss: 0.19397 | val_0_auc: 0.95055 |  0:00:19s\n",
      "epoch 16 | loss: 0.18957 | val_0_auc: 0.95892 |  0:00:20s\n",
      "epoch 17 | loss: 0.1928  | val_0_auc: 0.95922 |  0:00:21s\n",
      "epoch 18 | loss: 0.18534 | val_0_auc: 0.95943 |  0:00:22s\n",
      "epoch 19 | loss: 0.17186 | val_0_auc: 0.96279 |  0:00:23s\n",
      "epoch 20 | loss: 0.17622 | val_0_auc: 0.95317 |  0:00:25s\n",
      "epoch 21 | loss: 0.17318 | val_0_auc: 0.96514 |  0:00:26s\n",
      "epoch 22 | loss: 0.16948 | val_0_auc: 0.96079 |  0:00:27s\n",
      "epoch 23 | loss: 0.17447 | val_0_auc: 0.96686 |  0:00:28s\n",
      "epoch 24 | loss: 0.17266 | val_0_auc: 0.96382 |  0:00:29s\n",
      "epoch 25 | loss: 0.17162 | val_0_auc: 0.96913 |  0:00:31s\n",
      "epoch 26 | loss: 0.16188 | val_0_auc: 0.96846 |  0:00:32s\n",
      "epoch 27 | loss: 0.16518 | val_0_auc: 0.9677  |  0:00:33s\n",
      "epoch 28 | loss: 0.1587  | val_0_auc: 0.97021 |  0:00:34s\n",
      "epoch 29 | loss: 0.16607 | val_0_auc: 0.96603 |  0:00:35s\n",
      "epoch 30 | loss: 0.16334 | val_0_auc: 0.96528 |  0:00:37s\n",
      "epoch 31 | loss: 0.16159 | val_0_auc: 0.95888 |  0:00:38s\n",
      "epoch 32 | loss: 0.16801 | val_0_auc: 0.96024 |  0:00:39s\n",
      "epoch 33 | loss: 0.16581 | val_0_auc: 0.96619 |  0:00:40s\n",
      "epoch 34 | loss: 0.16753 | val_0_auc: 0.96453 |  0:00:41s\n",
      "epoch 35 | loss: 0.17609 | val_0_auc: 0.97124 |  0:00:42s\n",
      "epoch 36 | loss: 0.15622 | val_0_auc: 0.97308 |  0:00:44s\n",
      "epoch 37 | loss: 0.15596 | val_0_auc: 0.96849 |  0:00:45s\n",
      "epoch 38 | loss: 0.16003 | val_0_auc: 0.96748 |  0:00:46s\n",
      "epoch 39 | loss: 0.17686 | val_0_auc: 0.96529 |  0:00:47s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.97308\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9730792  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 48.1134\n",
      "Function value obtained: -0.9731\n",
      "Current minimum: -0.9731\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9574748069861343, 'lambda_sparse': 0.08059520086914775, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.29297 | val_0_auc: 0.87484 |  0:00:01s\n",
      "epoch 1  | loss: 0.52748 | val_0_auc: 0.86855 |  0:00:03s\n",
      "epoch 2  | loss: 0.49906 | val_0_auc: 0.89192 |  0:00:05s\n",
      "epoch 3  | loss: 0.3611  | val_0_auc: 0.89576 |  0:00:06s\n",
      "epoch 4  | loss: 0.34803 | val_0_auc: 0.923   |  0:00:08s\n",
      "epoch 5  | loss: 0.30574 | val_0_auc: 0.94188 |  0:00:10s\n",
      "epoch 6  | loss: 0.27428 | val_0_auc: 0.93338 |  0:00:11s\n",
      "epoch 7  | loss: 0.28765 | val_0_auc: 0.94104 |  0:00:13s\n",
      "epoch 8  | loss: 0.27135 | val_0_auc: 0.93962 |  0:00:15s\n",
      "epoch 9  | loss: 0.26248 | val_0_auc: 0.95536 |  0:00:16s\n",
      "epoch 10 | loss: 0.24547 | val_0_auc: 0.95238 |  0:00:18s\n",
      "epoch 11 | loss: 0.2313  | val_0_auc: 0.95216 |  0:00:20s\n",
      "epoch 12 | loss: 0.24659 | val_0_auc: 0.9549  |  0:00:21s\n",
      "epoch 13 | loss: 0.23726 | val_0_auc: 0.9599  |  0:00:23s\n",
      "epoch 14 | loss: 0.22436 | val_0_auc: 0.9615  |  0:00:25s\n",
      "epoch 15 | loss: 0.22735 | val_0_auc: 0.96486 |  0:00:26s\n",
      "epoch 16 | loss: 0.21002 | val_0_auc: 0.97103 |  0:00:28s\n",
      "epoch 17 | loss: 0.20777 | val_0_auc: 0.96642 |  0:00:30s\n",
      "epoch 18 | loss: 0.20651 | val_0_auc: 0.9632  |  0:00:31s\n",
      "epoch 19 | loss: 0.19828 | val_0_auc: 0.96453 |  0:00:33s\n",
      "epoch 20 | loss: 0.19803 | val_0_auc: 0.9603  |  0:00:35s\n",
      "epoch 21 | loss: 0.1932  | val_0_auc: 0.9713  |  0:00:36s\n",
      "epoch 22 | loss: 0.1846  | val_0_auc: 0.97284 |  0:00:38s\n",
      "epoch 23 | loss: 0.16772 | val_0_auc: 0.97669 |  0:00:40s\n",
      "epoch 24 | loss: 0.17389 | val_0_auc: 0.9759  |  0:00:42s\n",
      "epoch 25 | loss: 0.17185 | val_0_auc: 0.97594 |  0:00:43s\n",
      "epoch 26 | loss: 0.15847 | val_0_auc: 0.97272 |  0:00:45s\n",
      "epoch 27 | loss: 0.15965 | val_0_auc: 0.97476 |  0:00:47s\n",
      "epoch 28 | loss: 0.18247 | val_0_auc: 0.97441 |  0:00:48s\n",
      "epoch 29 | loss: 0.16669 | val_0_auc: 0.97539 |  0:00:50s\n",
      "epoch 30 | loss: 0.16015 | val_0_auc: 0.97182 |  0:00:52s\n",
      "epoch 31 | loss: 0.15775 | val_0_auc: 0.98135 |  0:00:53s\n",
      "epoch 32 | loss: 0.15834 | val_0_auc: 0.96905 |  0:00:55s\n",
      "epoch 33 | loss: 0.14743 | val_0_auc: 0.98234 |  0:00:57s\n",
      "epoch 34 | loss: 0.14785 | val_0_auc: 0.98053 |  0:00:58s\n",
      "epoch 35 | loss: 0.14958 | val_0_auc: 0.97657 |  0:01:00s\n",
      "epoch 36 | loss: 0.14566 | val_0_auc: 0.97391 |  0:01:02s\n",
      "epoch 37 | loss: 0.14173 | val_0_auc: 0.97685 |  0:01:03s\n",
      "epoch 38 | loss: 0.14402 | val_0_auc: 0.98192 |  0:01:05s\n",
      "epoch 39 | loss: 0.15166 | val_0_auc: 0.97478 |  0:01:07s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.98234\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98234303  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 67.7987\n",
      "Function value obtained: -0.9823\n",
      "Current minimum: -0.9823\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2682304128052804, 'lambda_sparse': 0.06472189950447386, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.38479 | val_0_auc: 0.6394  |  0:00:01s\n",
      "epoch 1  | loss: 0.60221 | val_0_auc: 0.80649 |  0:00:03s\n",
      "epoch 2  | loss: 0.44987 | val_0_auc: 0.86355 |  0:00:05s\n",
      "epoch 3  | loss: 0.38912 | val_0_auc: 0.88114 |  0:00:07s\n",
      "epoch 4  | loss: 0.33245 | val_0_auc: 0.89983 |  0:00:08s\n",
      "epoch 5  | loss: 0.32259 | val_0_auc: 0.91134 |  0:00:10s\n",
      "epoch 6  | loss: 0.3508  | val_0_auc: 0.90804 |  0:00:12s\n",
      "epoch 7  | loss: 0.30078 | val_0_auc: 0.92069 |  0:00:14s\n",
      "epoch 8  | loss: 0.2909  | val_0_auc: 0.93037 |  0:00:15s\n",
      "epoch 9  | loss: 0.25849 | val_0_auc: 0.95046 |  0:00:17s\n",
      "epoch 10 | loss: 0.2516  | val_0_auc: 0.95588 |  0:00:19s\n",
      "epoch 11 | loss: 0.25872 | val_0_auc: 0.95683 |  0:00:20s\n",
      "epoch 12 | loss: 0.26241 | val_0_auc: 0.95906 |  0:00:22s\n",
      "epoch 13 | loss: 0.25919 | val_0_auc: 0.93852 |  0:00:24s\n",
      "epoch 14 | loss: 0.2742  | val_0_auc: 0.9385  |  0:00:26s\n",
      "epoch 15 | loss: 0.34836 | val_0_auc: 0.94545 |  0:00:27s\n",
      "epoch 16 | loss: 0.30873 | val_0_auc: 0.93907 |  0:00:29s\n",
      "epoch 17 | loss: 0.2913  | val_0_auc: 0.95685 |  0:00:31s\n",
      "epoch 18 | loss: 0.24747 | val_0_auc: 0.95596 |  0:00:33s\n",
      "epoch 19 | loss: 0.23527 | val_0_auc: 0.95589 |  0:00:34s\n",
      "epoch 20 | loss: 0.2156  | val_0_auc: 0.96401 |  0:00:36s\n",
      "epoch 21 | loss: 0.20913 | val_0_auc: 0.96576 |  0:00:38s\n",
      "epoch 22 | loss: 0.20441 | val_0_auc: 0.9635  |  0:00:40s\n",
      "epoch 23 | loss: 0.19702 | val_0_auc: 0.96671 |  0:00:41s\n",
      "epoch 24 | loss: 0.20022 | val_0_auc: 0.96745 |  0:00:43s\n",
      "epoch 25 | loss: 0.18359 | val_0_auc: 0.97537 |  0:00:45s\n",
      "epoch 26 | loss: 0.18366 | val_0_auc: 0.975   |  0:00:46s\n",
      "epoch 27 | loss: 0.16673 | val_0_auc: 0.97061 |  0:00:48s\n",
      "epoch 28 | loss: 0.17314 | val_0_auc: 0.97802 |  0:00:50s\n",
      "epoch 29 | loss: 0.16718 | val_0_auc: 0.97804 |  0:00:52s\n",
      "epoch 30 | loss: 0.16898 | val_0_auc: 0.97958 |  0:00:53s\n",
      "epoch 31 | loss: 0.14976 | val_0_auc: 0.97774 |  0:00:55s\n",
      "epoch 32 | loss: 0.1513  | val_0_auc: 0.98254 |  0:00:57s\n",
      "epoch 33 | loss: 0.1536  | val_0_auc: 0.98165 |  0:00:59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | loss: 0.15325 | val_0_auc: 0.98038 |  0:01:00s\n",
      "epoch 35 | loss: 0.15345 | val_0_auc: 0.97904 |  0:01:02s\n",
      "epoch 36 | loss: 0.14806 | val_0_auc: 0.98199 |  0:01:04s\n",
      "epoch 37 | loss: 0.14682 | val_0_auc: 0.98059 |  0:01:05s\n",
      "epoch 38 | loss: 0.14913 | val_0_auc: 0.98122 |  0:01:07s\n",
      "epoch 39 | loss: 0.14961 | val_0_auc: 0.9847  |  0:01:09s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.9847\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98470202  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 69.9149\n",
      "Function value obtained: -0.9847\n",
      "Current minimum: -0.9847\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0063685631168235, 'lambda_sparse': 0.017689368582224155, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.72889 | val_0_auc: 0.74512 |  0:00:00s\n",
      "epoch 1  | loss: 0.41015 | val_0_auc: 0.83259 |  0:00:01s\n",
      "epoch 2  | loss: 0.32134 | val_0_auc: 0.91051 |  0:00:02s\n",
      "epoch 3  | loss: 0.27475 | val_0_auc: 0.92043 |  0:00:03s\n",
      "epoch 4  | loss: 0.25701 | val_0_auc: 0.92741 |  0:00:04s\n",
      "epoch 5  | loss: 0.23637 | val_0_auc: 0.93961 |  0:00:05s\n",
      "epoch 6  | loss: 0.22223 | val_0_auc: 0.94188 |  0:00:06s\n",
      "epoch 7  | loss: 0.21057 | val_0_auc: 0.94715 |  0:00:07s\n",
      "epoch 8  | loss: 0.20418 | val_0_auc: 0.94822 |  0:00:08s\n",
      "epoch 9  | loss: 0.19716 | val_0_auc: 0.95652 |  0:00:09s\n",
      "epoch 10 | loss: 0.18494 | val_0_auc: 0.95218 |  0:00:09s\n",
      "epoch 11 | loss: 0.18282 | val_0_auc: 0.95945 |  0:00:10s\n",
      "epoch 12 | loss: 0.16889 | val_0_auc: 0.96101 |  0:00:11s\n",
      "epoch 13 | loss: 0.16978 | val_0_auc: 0.96544 |  0:00:12s\n",
      "epoch 14 | loss: 0.17129 | val_0_auc: 0.96217 |  0:00:13s\n",
      "epoch 15 | loss: 0.16824 | val_0_auc: 0.96731 |  0:00:14s\n",
      "epoch 16 | loss: 0.1588  | val_0_auc: 0.9635  |  0:00:15s\n",
      "epoch 17 | loss: 0.15502 | val_0_auc: 0.97327 |  0:00:16s\n",
      "epoch 18 | loss: 0.1492  | val_0_auc: 0.97815 |  0:00:17s\n",
      "epoch 19 | loss: 0.13883 | val_0_auc: 0.9757  |  0:00:18s\n",
      "epoch 20 | loss: 0.13123 | val_0_auc: 0.97396 |  0:00:18s\n",
      "epoch 21 | loss: 0.13446 | val_0_auc: 0.9747  |  0:00:19s\n",
      "epoch 22 | loss: 0.12891 | val_0_auc: 0.98089 |  0:00:20s\n",
      "epoch 23 | loss: 0.13177 | val_0_auc: 0.98314 |  0:00:21s\n",
      "epoch 24 | loss: 0.13067 | val_0_auc: 0.98458 |  0:00:22s\n",
      "epoch 25 | loss: 0.12636 | val_0_auc: 0.98425 |  0:00:23s\n",
      "epoch 26 | loss: 0.12193 | val_0_auc: 0.9865  |  0:00:24s\n",
      "epoch 27 | loss: 0.1178  | val_0_auc: 0.98724 |  0:00:25s\n",
      "epoch 28 | loss: 0.1202  | val_0_auc: 0.9818  |  0:00:26s\n",
      "epoch 29 | loss: 0.12985 | val_0_auc: 0.98453 |  0:00:26s\n",
      "epoch 30 | loss: 0.12649 | val_0_auc: 0.98585 |  0:00:27s\n",
      "epoch 31 | loss: 0.12137 | val_0_auc: 0.99049 |  0:00:28s\n",
      "epoch 32 | loss: 0.11704 | val_0_auc: 0.9905  |  0:00:29s\n",
      "epoch 33 | loss: 0.11137 | val_0_auc: 0.98832 |  0:00:30s\n",
      "epoch 34 | loss: 0.11004 | val_0_auc: 0.9919  |  0:00:31s\n",
      "epoch 35 | loss: 0.11189 | val_0_auc: 0.98929 |  0:00:32s\n",
      "epoch 36 | loss: 0.11058 | val_0_auc: 0.99144 |  0:00:33s\n",
      "epoch 37 | loss: 0.10248 | val_0_auc: 0.98953 |  0:00:34s\n",
      "epoch 38 | loss: 0.10358 | val_0_auc: 0.99107 |  0:00:34s\n",
      "epoch 39 | loss: 0.10566 | val_0_auc: 0.99141 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.9919\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99189642  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 36.1385\n",
      "Function value obtained: -0.9919\n",
      "Current minimum: -0.9919\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0840374047235146, 'lambda_sparse': 0.07756898575730785, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.00065 | val_0_auc: 0.76321 |  0:00:01s\n",
      "epoch 1  | loss: 0.55624 | val_0_auc: 0.83676 |  0:00:03s\n",
      "epoch 2  | loss: 0.43164 | val_0_auc: 0.887   |  0:00:04s\n",
      "epoch 3  | loss: 0.38433 | val_0_auc: 0.91193 |  0:00:06s\n",
      "epoch 4  | loss: 0.32707 | val_0_auc: 0.92547 |  0:00:07s\n",
      "epoch 5  | loss: 0.28892 | val_0_auc: 0.95271 |  0:00:09s\n",
      "epoch 6  | loss: 0.2696  | val_0_auc: 0.95184 |  0:00:11s\n",
      "epoch 7  | loss: 0.25928 | val_0_auc: 0.96043 |  0:00:12s\n",
      "epoch 8  | loss: 0.24562 | val_0_auc: 0.9629  |  0:00:14s\n",
      "epoch 9  | loss: 0.23115 | val_0_auc: 0.96565 |  0:00:15s\n",
      "epoch 10 | loss: 0.233   | val_0_auc: 0.9679  |  0:00:17s\n",
      "epoch 11 | loss: 0.21937 | val_0_auc: 0.9727  |  0:00:18s\n",
      "epoch 12 | loss: 0.22445 | val_0_auc: 0.97217 |  0:00:20s\n",
      "epoch 13 | loss: 0.19942 | val_0_auc: 0.98061 |  0:00:21s\n",
      "epoch 14 | loss: 0.20012 | val_0_auc: 0.97994 |  0:00:23s\n",
      "epoch 15 | loss: 0.19006 | val_0_auc: 0.98561 |  0:00:25s\n",
      "epoch 16 | loss: 0.17273 | val_0_auc: 0.98649 |  0:00:26s\n",
      "epoch 17 | loss: 0.17453 | val_0_auc: 0.98562 |  0:00:28s\n",
      "epoch 18 | loss: 0.18154 | val_0_auc: 0.98885 |  0:00:29s\n",
      "epoch 19 | loss: 0.16707 | val_0_auc: 0.98608 |  0:00:31s\n",
      "epoch 20 | loss: 0.15734 | val_0_auc: 0.98559 |  0:00:32s\n",
      "epoch 21 | loss: 0.16147 | val_0_auc: 0.98645 |  0:00:34s\n",
      "epoch 22 | loss: 0.15603 | val_0_auc: 0.98872 |  0:00:35s\n",
      "epoch 23 | loss: 0.14856 | val_0_auc: 0.9892  |  0:00:37s\n",
      "epoch 24 | loss: 0.15546 | val_0_auc: 0.98777 |  0:00:39s\n",
      "epoch 25 | loss: 0.14531 | val_0_auc: 0.98941 |  0:00:40s\n",
      "epoch 26 | loss: 0.15425 | val_0_auc: 0.98947 |  0:00:42s\n",
      "epoch 27 | loss: 0.15168 | val_0_auc: 0.98865 |  0:00:43s\n",
      "epoch 28 | loss: 0.14432 | val_0_auc: 0.9917  |  0:00:45s\n",
      "epoch 29 | loss: 0.14263 | val_0_auc: 0.99207 |  0:00:46s\n",
      "epoch 30 | loss: 0.1416  | val_0_auc: 0.9876  |  0:00:48s\n",
      "epoch 31 | loss: 0.14508 | val_0_auc: 0.99322 |  0:00:50s\n",
      "epoch 32 | loss: 0.13843 | val_0_auc: 0.98996 |  0:00:51s\n",
      "epoch 33 | loss: 0.13607 | val_0_auc: 0.99246 |  0:00:53s\n",
      "epoch 34 | loss: 0.13213 | val_0_auc: 0.99315 |  0:00:54s\n",
      "epoch 35 | loss: 0.13463 | val_0_auc: 0.99196 |  0:00:56s\n",
      "epoch 36 | loss: 0.13261 | val_0_auc: 0.99365 |  0:00:57s\n",
      "epoch 37 | loss: 0.12581 | val_0_auc: 0.99278 |  0:00:59s\n",
      "epoch 38 | loss: 0.12707 | val_0_auc: 0.98917 |  0:01:00s\n",
      "epoch 39 | loss: 0.12716 | val_0_auc: 0.99258 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99365\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99365402  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 62.9168\n",
      "Function value obtained: -0.9937\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4824557781281862, 'lambda_sparse': 0.047107858039314596, 'n_steps': 8, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.80162 | val_0_auc: 0.62911 |  0:00:01s\n",
      "epoch 1  | loss: 0.70849 | val_0_auc: 0.78491 |  0:00:03s\n",
      "epoch 2  | loss: 0.53999 | val_0_auc: 0.82681 |  0:00:05s\n",
      "epoch 3  | loss: 0.45596 | val_0_auc: 0.88224 |  0:00:06s\n",
      "epoch 4  | loss: 0.41019 | val_0_auc: 0.89595 |  0:00:08s\n",
      "epoch 5  | loss: 0.37415 | val_0_auc: 0.88959 |  0:00:10s\n",
      "epoch 6  | loss: 0.34707 | val_0_auc: 0.88723 |  0:00:12s\n",
      "epoch 7  | loss: 0.34611 | val_0_auc: 0.90002 |  0:00:14s\n",
      "epoch 8  | loss: 0.33666 | val_0_auc: 0.90236 |  0:00:16s\n",
      "epoch 9  | loss: 0.33592 | val_0_auc: 0.92201 |  0:00:19s\n",
      "epoch 10 | loss: 0.33308 | val_0_auc: 0.90676 |  0:00:20s\n",
      "epoch 11 | loss: 0.34011 | val_0_auc: 0.92279 |  0:00:22s\n",
      "epoch 12 | loss: 0.30365 | val_0_auc: 0.92896 |  0:00:24s\n",
      "epoch 13 | loss: 0.30226 | val_0_auc: 0.91979 |  0:00:25s\n",
      "epoch 14 | loss: 0.30556 | val_0_auc: 0.92397 |  0:00:27s\n",
      "epoch 15 | loss: 0.30197 | val_0_auc: 0.92252 |  0:00:29s\n",
      "epoch 16 | loss: 0.29757 | val_0_auc: 0.92257 |  0:00:31s\n",
      "epoch 17 | loss: 0.28814 | val_0_auc: 0.93266 |  0:00:32s\n",
      "epoch 18 | loss: 0.28166 | val_0_auc: 0.92457 |  0:00:34s\n",
      "epoch 19 | loss: 0.28333 | val_0_auc: 0.94121 |  0:00:36s\n",
      "epoch 20 | loss: 0.29087 | val_0_auc: 0.93343 |  0:00:38s\n",
      "epoch 21 | loss: 0.28061 | val_0_auc: 0.93724 |  0:00:39s\n",
      "epoch 22 | loss: 0.30747 | val_0_auc: 0.94559 |  0:00:41s\n",
      "epoch 23 | loss: 0.2851  | val_0_auc: 0.94307 |  0:00:43s\n",
      "epoch 24 | loss: 0.26726 | val_0_auc: 0.94579 |  0:00:44s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25 | loss: 0.25181 | val_0_auc: 0.94961 |  0:00:46s\n",
      "epoch 26 | loss: 0.25441 | val_0_auc: 0.94    |  0:00:48s\n",
      "epoch 27 | loss: 0.25002 | val_0_auc: 0.95647 |  0:00:50s\n",
      "epoch 28 | loss: 0.24138 | val_0_auc: 0.94767 |  0:00:51s\n",
      "epoch 29 | loss: 0.25766 | val_0_auc: 0.93458 |  0:00:53s\n",
      "epoch 30 | loss: 0.28448 | val_0_auc: 0.93926 |  0:00:55s\n",
      "epoch 31 | loss: 0.26795 | val_0_auc: 0.93693 |  0:00:57s\n",
      "epoch 32 | loss: 0.26118 | val_0_auc: 0.94304 |  0:00:58s\n",
      "epoch 33 | loss: 0.24508 | val_0_auc: 0.94807 |  0:01:00s\n",
      "epoch 34 | loss: 0.24762 | val_0_auc: 0.95111 |  0:01:02s\n",
      "epoch 35 | loss: 0.21946 | val_0_auc: 0.96123 |  0:01:04s\n",
      "epoch 36 | loss: 0.20894 | val_0_auc: 0.97147 |  0:01:05s\n",
      "epoch 37 | loss: 0.217   | val_0_auc: 0.97338 |  0:01:07s\n",
      "epoch 38 | loss: 0.19762 | val_0_auc: 0.96837 |  0:01:09s\n",
      "epoch 39 | loss: 0.19074 | val_0_auc: 0.97355 |  0:01:10s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.97355\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97354998  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 71.6552\n",
      "Function value obtained: -0.9735\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5901821115982884, 'lambda_sparse': 0.04646399201085555, 'n_steps': 7, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.451   | val_0_auc: 0.76257 |  0:00:01s\n",
      "epoch 1  | loss: 0.57198 | val_0_auc: 0.85868 |  0:00:03s\n",
      "epoch 2  | loss: 0.44496 | val_0_auc: 0.87824 |  0:00:04s\n",
      "epoch 3  | loss: 0.43426 | val_0_auc: 0.88442 |  0:00:06s\n",
      "epoch 4  | loss: 0.40577 | val_0_auc: 0.87416 |  0:00:07s\n",
      "epoch 5  | loss: 0.38653 | val_0_auc: 0.89619 |  0:00:09s\n",
      "epoch 6  | loss: 0.33067 | val_0_auc: 0.92174 |  0:00:10s\n",
      "epoch 7  | loss: 0.27665 | val_0_auc: 0.92627 |  0:00:12s\n",
      "epoch 8  | loss: 0.26222 | val_0_auc: 0.93863 |  0:00:14s\n",
      "epoch 9  | loss: 0.23633 | val_0_auc: 0.94965 |  0:00:15s\n",
      "epoch 10 | loss: 0.23159 | val_0_auc: 0.94865 |  0:00:17s\n",
      "epoch 11 | loss: 0.23366 | val_0_auc: 0.94879 |  0:00:18s\n",
      "epoch 12 | loss: 0.23488 | val_0_auc: 0.95814 |  0:00:20s\n",
      "epoch 13 | loss: 0.22808 | val_0_auc: 0.96309 |  0:00:21s\n",
      "epoch 14 | loss: 0.23093 | val_0_auc: 0.96681 |  0:00:23s\n",
      "epoch 15 | loss: 0.21646 | val_0_auc: 0.95912 |  0:00:24s\n",
      "epoch 16 | loss: 0.23114 | val_0_auc: 0.95563 |  0:00:26s\n",
      "epoch 17 | loss: 0.21195 | val_0_auc: 0.95892 |  0:00:27s\n",
      "epoch 18 | loss: 0.21581 | val_0_auc: 0.95521 |  0:00:29s\n",
      "epoch 19 | loss: 0.19939 | val_0_auc: 0.96613 |  0:00:30s\n",
      "epoch 20 | loss: 0.18539 | val_0_auc: 0.96721 |  0:00:32s\n",
      "epoch 21 | loss: 0.1793  | val_0_auc: 0.9733  |  0:00:34s\n",
      "epoch 22 | loss: 0.17384 | val_0_auc: 0.97031 |  0:00:35s\n",
      "epoch 23 | loss: 0.16928 | val_0_auc: 0.9666  |  0:00:37s\n",
      "epoch 24 | loss: 0.1586  | val_0_auc: 0.96775 |  0:00:38s\n",
      "epoch 25 | loss: 0.17136 | val_0_auc: 0.97089 |  0:00:40s\n",
      "epoch 26 | loss: 0.16682 | val_0_auc: 0.96358 |  0:00:41s\n",
      "epoch 27 | loss: 0.16325 | val_0_auc: 0.97013 |  0:00:43s\n",
      "epoch 28 | loss: 0.16344 | val_0_auc: 0.96897 |  0:00:44s\n",
      "epoch 29 | loss: 0.16794 | val_0_auc: 0.96884 |  0:00:46s\n",
      "epoch 30 | loss: 0.16005 | val_0_auc: 0.97685 |  0:00:47s\n",
      "epoch 31 | loss: 0.14909 | val_0_auc: 0.97097 |  0:00:49s\n",
      "epoch 32 | loss: 0.13699 | val_0_auc: 0.97312 |  0:00:50s\n",
      "epoch 33 | loss: 0.14694 | val_0_auc: 0.97168 |  0:00:52s\n",
      "epoch 34 | loss: 0.13989 | val_0_auc: 0.97634 |  0:00:54s\n",
      "epoch 35 | loss: 0.14211 | val_0_auc: 0.97176 |  0:00:55s\n",
      "epoch 36 | loss: 0.15237 | val_0_auc: 0.97685 |  0:00:57s\n",
      "epoch 37 | loss: 0.15146 | val_0_auc: 0.97567 |  0:00:58s\n",
      "epoch 38 | loss: 0.13907 | val_0_auc: 0.97393 |  0:01:00s\n",
      "epoch 39 | loss: 0.13869 | val_0_auc: 0.9794  |  0:01:01s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.9794\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97939683  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 62.2055\n",
      "Function value obtained: -0.9794\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6490555694779006, 'lambda_sparse': 0.015195801826058194, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.7763  | val_0_auc: 0.82003 |  0:00:01s\n",
      "epoch 1  | loss: 0.47886 | val_0_auc: 0.8449  |  0:00:03s\n",
      "epoch 2  | loss: 0.4145  | val_0_auc: 0.89252 |  0:00:04s\n",
      "epoch 3  | loss: 0.36361 | val_0_auc: 0.88574 |  0:00:06s\n",
      "epoch 4  | loss: 0.34511 | val_0_auc: 0.89987 |  0:00:07s\n",
      "epoch 5  | loss: 0.31812 | val_0_auc: 0.91273 |  0:00:09s\n",
      "epoch 6  | loss: 0.28691 | val_0_auc: 0.92582 |  0:00:10s\n",
      "epoch 7  | loss: 0.26704 | val_0_auc: 0.92413 |  0:00:12s\n",
      "epoch 8  | loss: 0.27028 | val_0_auc: 0.92844 |  0:00:13s\n",
      "epoch 9  | loss: 0.26142 | val_0_auc: 0.92568 |  0:00:15s\n",
      "epoch 10 | loss: 0.23722 | val_0_auc: 0.9508  |  0:00:16s\n",
      "epoch 11 | loss: 0.21416 | val_0_auc: 0.95651 |  0:00:18s\n",
      "epoch 12 | loss: 0.20894 | val_0_auc: 0.96082 |  0:00:19s\n",
      "epoch 13 | loss: 0.19942 | val_0_auc: 0.96011 |  0:00:21s\n",
      "epoch 14 | loss: 0.19212 | val_0_auc: 0.96357 |  0:00:22s\n",
      "epoch 15 | loss: 0.17983 | val_0_auc: 0.96588 |  0:00:24s\n",
      "epoch 16 | loss: 0.18571 | val_0_auc: 0.97025 |  0:00:25s\n",
      "epoch 17 | loss: 0.16963 | val_0_auc: 0.96646 |  0:00:27s\n",
      "epoch 18 | loss: 0.16402 | val_0_auc: 0.96763 |  0:00:28s\n",
      "epoch 19 | loss: 0.18169 | val_0_auc: 0.96863 |  0:00:30s\n",
      "epoch 20 | loss: 0.1846  | val_0_auc: 0.96573 |  0:00:31s\n",
      "epoch 21 | loss: 0.17569 | val_0_auc: 0.96797 |  0:00:33s\n",
      "epoch 22 | loss: 0.17406 | val_0_auc: 0.96282 |  0:00:34s\n",
      "epoch 23 | loss: 0.17811 | val_0_auc: 0.95676 |  0:00:36s\n",
      "epoch 24 | loss: 0.18717 | val_0_auc: 0.96958 |  0:00:37s\n",
      "epoch 25 | loss: 0.18977 | val_0_auc: 0.97094 |  0:00:39s\n",
      "epoch 26 | loss: 0.17726 | val_0_auc: 0.9706  |  0:00:40s\n",
      "epoch 27 | loss: 0.16298 | val_0_auc: 0.96475 |  0:00:42s\n",
      "epoch 28 | loss: 0.16145 | val_0_auc: 0.96783 |  0:00:43s\n",
      "epoch 29 | loss: 0.15319 | val_0_auc: 0.96593 |  0:00:45s\n",
      "epoch 30 | loss: 0.17757 | val_0_auc: 0.9743  |  0:00:46s\n",
      "epoch 31 | loss: 0.15148 | val_0_auc: 0.98148 |  0:00:48s\n",
      "epoch 32 | loss: 0.16016 | val_0_auc: 0.97674 |  0:00:49s\n",
      "epoch 33 | loss: 0.15872 | val_0_auc: 0.97927 |  0:00:51s\n",
      "epoch 34 | loss: 0.15432 | val_0_auc: 0.97844 |  0:00:52s\n",
      "epoch 35 | loss: 0.1407  | val_0_auc: 0.98147 |  0:00:53s\n",
      "epoch 36 | loss: 0.13265 | val_0_auc: 0.98263 |  0:00:55s\n",
      "epoch 37 | loss: 0.13856 | val_0_auc: 0.98452 |  0:00:56s\n",
      "epoch 38 | loss: 0.14528 | val_0_auc: 0.98105 |  0:00:58s\n",
      "epoch 39 | loss: 0.14826 | val_0_auc: 0.98323 |  0:00:59s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.98452\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98451978  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 60.4973\n",
      "Function value obtained: -0.9845\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.698590416710506, 'lambda_sparse': 0.012208331631283697, 'n_steps': 10, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.98113 | val_0_auc: 0.822   |  0:00:02s\n",
      "epoch 1  | loss: 0.65774 | val_0_auc: 0.83252 |  0:00:04s\n",
      "epoch 2  | loss: 0.59263 | val_0_auc: 0.85863 |  0:00:06s\n",
      "epoch 3  | loss: 0.50184 | val_0_auc: 0.82893 |  0:00:09s\n",
      "epoch 4  | loss: 0.67451 | val_0_auc: 0.84031 |  0:00:11s\n",
      "epoch 5  | loss: 0.67443 | val_0_auc: 0.84566 |  0:00:14s\n",
      "epoch 6  | loss: 0.54882 | val_0_auc: 0.85218 |  0:00:16s\n",
      "epoch 7  | loss: 0.57232 | val_0_auc: 0.84904 |  0:00:18s\n",
      "epoch 8  | loss: 0.46801 | val_0_auc: 0.87485 |  0:00:21s\n",
      "epoch 9  | loss: 0.44447 | val_0_auc: 0.87016 |  0:00:23s\n",
      "epoch 10 | loss: 0.62034 | val_0_auc: 0.84765 |  0:00:25s\n",
      "epoch 11 | loss: 0.59878 | val_0_auc: 0.86687 |  0:00:28s\n",
      "epoch 12 | loss: 0.6498  | val_0_auc: 0.84878 |  0:00:30s\n",
      "epoch 13 | loss: 0.60404 | val_0_auc: 0.87277 |  0:00:32s\n",
      "epoch 14 | loss: 0.41543 | val_0_auc: 0.88829 |  0:00:35s\n",
      "epoch 15 | loss: 0.36222 | val_0_auc: 0.87793 |  0:00:38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16 | loss: 0.34261 | val_0_auc: 0.90029 |  0:00:40s\n",
      "epoch 17 | loss: 0.3564  | val_0_auc: 0.89786 |  0:00:42s\n",
      "epoch 18 | loss: 0.36956 | val_0_auc: 0.90335 |  0:00:45s\n",
      "epoch 19 | loss: 0.39098 | val_0_auc: 0.89798 |  0:00:47s\n",
      "epoch 20 | loss: 0.36695 | val_0_auc: 0.91802 |  0:00:50s\n",
      "epoch 21 | loss: 0.30773 | val_0_auc: 0.92152 |  0:00:52s\n",
      "epoch 22 | loss: 0.312   | val_0_auc: 0.92106 |  0:00:54s\n",
      "epoch 23 | loss: 0.30968 | val_0_auc: 0.91434 |  0:00:57s\n",
      "epoch 24 | loss: 0.33173 | val_0_auc: 0.90528 |  0:00:59s\n",
      "epoch 25 | loss: 0.31306 | val_0_auc: 0.92779 |  0:01:02s\n",
      "epoch 26 | loss: 0.29458 | val_0_auc: 0.93228 |  0:01:04s\n",
      "epoch 27 | loss: 0.28604 | val_0_auc: 0.9283  |  0:01:06s\n",
      "epoch 28 | loss: 0.2815  | val_0_auc: 0.93397 |  0:01:09s\n",
      "epoch 29 | loss: 0.27398 | val_0_auc: 0.93857 |  0:01:11s\n",
      "epoch 30 | loss: 0.26492 | val_0_auc: 0.92378 |  0:01:14s\n",
      "epoch 31 | loss: 0.28328 | val_0_auc: 0.93202 |  0:01:16s\n",
      "epoch 32 | loss: 0.28564 | val_0_auc: 0.94958 |  0:01:18s\n",
      "epoch 33 | loss: 0.26748 | val_0_auc: 0.93822 |  0:01:21s\n",
      "epoch 34 | loss: 0.30208 | val_0_auc: 0.9521  |  0:01:23s\n",
      "epoch 35 | loss: 0.24588 | val_0_auc: 0.95195 |  0:01:26s\n",
      "epoch 36 | loss: 0.25033 | val_0_auc: 0.95417 |  0:01:28s\n",
      "epoch 37 | loss: 0.24156 | val_0_auc: 0.94892 |  0:01:30s\n",
      "epoch 38 | loss: 0.25497 | val_0_auc: 0.9396  |  0:01:33s\n",
      "epoch 39 | loss: 0.25428 | val_0_auc: 0.93459 |  0:01:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.95417\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95417085  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 96.5585\n",
      "Function value obtained: -0.9542\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0149563922345277, 'lambda_sparse': 0.026551936740226774, 'n_steps': 5, 'n_a': 8, 'n_d': 8, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.93113 | val_0_auc: 0.68667 |  0:00:00s\n",
      "epoch 1  | loss: 0.57174 | val_0_auc: 0.79837 |  0:00:01s\n",
      "epoch 2  | loss: 0.45929 | val_0_auc: 0.83862 |  0:00:02s\n",
      "epoch 3  | loss: 0.3709  | val_0_auc: 0.85315 |  0:00:03s\n",
      "epoch 4  | loss: 0.31175 | val_0_auc: 0.90593 |  0:00:04s\n",
      "epoch 5  | loss: 0.27519 | val_0_auc: 0.92055 |  0:00:05s\n",
      "epoch 6  | loss: 0.26253 | val_0_auc: 0.92834 |  0:00:06s\n",
      "epoch 7  | loss: 0.2405  | val_0_auc: 0.94934 |  0:00:07s\n",
      "epoch 8  | loss: 0.22419 | val_0_auc: 0.95447 |  0:00:08s\n",
      "epoch 9  | loss: 0.21534 | val_0_auc: 0.96308 |  0:00:09s\n",
      "epoch 10 | loss: 0.21025 | val_0_auc: 0.96079 |  0:00:10s\n",
      "epoch 11 | loss: 0.20069 | val_0_auc: 0.96717 |  0:00:10s\n",
      "epoch 12 | loss: 0.18932 | val_0_auc: 0.97061 |  0:00:11s\n",
      "epoch 13 | loss: 0.17924 | val_0_auc: 0.97606 |  0:00:12s\n",
      "epoch 14 | loss: 0.17593 | val_0_auc: 0.97447 |  0:00:13s\n",
      "epoch 15 | loss: 0.16665 | val_0_auc: 0.97786 |  0:00:14s\n",
      "epoch 16 | loss: 0.15435 | val_0_auc: 0.98108 |  0:00:15s\n",
      "epoch 17 | loss: 0.15098 | val_0_auc: 0.98206 |  0:00:16s\n",
      "epoch 18 | loss: 0.14317 | val_0_auc: 0.98292 |  0:00:17s\n",
      "epoch 19 | loss: 0.1392  | val_0_auc: 0.98115 |  0:00:18s\n",
      "epoch 20 | loss: 0.13094 | val_0_auc: 0.98311 |  0:00:19s\n",
      "epoch 21 | loss: 0.12692 | val_0_auc: 0.98531 |  0:00:20s\n",
      "epoch 22 | loss: 0.12643 | val_0_auc: 0.98394 |  0:00:21s\n",
      "epoch 23 | loss: 0.12942 | val_0_auc: 0.98357 |  0:00:21s\n",
      "epoch 24 | loss: 0.13767 | val_0_auc: 0.98331 |  0:00:22s\n",
      "epoch 25 | loss: 0.14007 | val_0_auc: 0.98232 |  0:00:23s\n",
      "epoch 26 | loss: 0.13204 | val_0_auc: 0.98105 |  0:00:24s\n",
      "epoch 27 | loss: 0.13093 | val_0_auc: 0.9813  |  0:00:25s\n",
      "epoch 28 | loss: 0.13085 | val_0_auc: 0.98731 |  0:00:26s\n",
      "epoch 29 | loss: 0.11716 | val_0_auc: 0.98616 |  0:00:27s\n",
      "epoch 30 | loss: 0.12063 | val_0_auc: 0.98666 |  0:00:28s\n",
      "epoch 31 | loss: 0.11695 | val_0_auc: 0.98908 |  0:00:29s\n",
      "epoch 32 | loss: 0.11467 | val_0_auc: 0.99025 |  0:00:30s\n",
      "epoch 33 | loss: 0.11453 | val_0_auc: 0.98847 |  0:00:31s\n",
      "epoch 34 | loss: 0.1047  | val_0_auc: 0.98642 |  0:00:31s\n",
      "epoch 35 | loss: 0.10525 | val_0_auc: 0.9853  |  0:00:32s\n",
      "epoch 36 | loss: 0.10309 | val_0_auc: 0.98782 |  0:00:33s\n",
      "epoch 37 | loss: 0.10801 | val_0_auc: 0.98834 |  0:00:34s\n",
      "epoch 38 | loss: 0.1012  | val_0_auc: 0.98953 |  0:00:35s\n",
      "epoch 39 | loss: 0.0996  | val_0_auc: 0.98876 |  0:00:36s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 32 and best_val_0_auc = 0.99025\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99025019  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 36.6998\n",
      "Function value obtained: -0.9903\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4327410462138235, 'lambda_sparse': 0.034102437274319296, 'n_steps': 8, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.80932 | val_0_auc: 0.67087 |  0:00:01s\n",
      "epoch 1  | loss: 0.55622 | val_0_auc: 0.77675 |  0:00:02s\n",
      "epoch 2  | loss: 0.48135 | val_0_auc: 0.80662 |  0:00:04s\n",
      "epoch 3  | loss: 0.4415  | val_0_auc: 0.84129 |  0:00:05s\n",
      "epoch 4  | loss: 0.43869 | val_0_auc: 0.82171 |  0:00:06s\n",
      "epoch 5  | loss: 0.4326  | val_0_auc: 0.84568 |  0:00:08s\n",
      "epoch 6  | loss: 0.39371 | val_0_auc: 0.86832 |  0:00:09s\n",
      "epoch 7  | loss: 0.35804 | val_0_auc: 0.88322 |  0:00:10s\n",
      "epoch 8  | loss: 0.3288  | val_0_auc: 0.90272 |  0:00:12s\n",
      "epoch 9  | loss: 0.33218 | val_0_auc: 0.92166 |  0:00:13s\n",
      "epoch 10 | loss: 0.40786 | val_0_auc: 0.93892 |  0:00:14s\n",
      "epoch 11 | loss: 0.29478 | val_0_auc: 0.94813 |  0:00:16s\n",
      "epoch 12 | loss: 0.28427 | val_0_auc: 0.94116 |  0:00:17s\n",
      "epoch 13 | loss: 0.28132 | val_0_auc: 0.93156 |  0:00:18s\n",
      "epoch 14 | loss: 0.2648  | val_0_auc: 0.95051 |  0:00:20s\n",
      "epoch 15 | loss: 0.26556 | val_0_auc: 0.95159 |  0:00:21s\n",
      "epoch 16 | loss: 0.25579 | val_0_auc: 0.94411 |  0:00:22s\n",
      "epoch 17 | loss: 0.25412 | val_0_auc: 0.94888 |  0:00:24s\n",
      "epoch 18 | loss: 0.26018 | val_0_auc: 0.95543 |  0:00:25s\n",
      "epoch 19 | loss: 0.24837 | val_0_auc: 0.95302 |  0:00:26s\n",
      "epoch 20 | loss: 0.24367 | val_0_auc: 0.94653 |  0:00:28s\n",
      "epoch 21 | loss: 0.23331 | val_0_auc: 0.96326 |  0:00:29s\n",
      "epoch 22 | loss: 0.2028  | val_0_auc: 0.96512 |  0:00:30s\n",
      "epoch 23 | loss: 0.2031  | val_0_auc: 0.96488 |  0:00:32s\n",
      "epoch 24 | loss: 0.20357 | val_0_auc: 0.96949 |  0:00:33s\n",
      "epoch 25 | loss: 0.19282 | val_0_auc: 0.95483 |  0:00:34s\n",
      "epoch 26 | loss: 0.2156  | val_0_auc: 0.95627 |  0:00:36s\n",
      "epoch 27 | loss: 0.20962 | val_0_auc: 0.9567  |  0:00:37s\n",
      "epoch 28 | loss: 0.20689 | val_0_auc: 0.97465 |  0:00:38s\n",
      "epoch 29 | loss: 0.18412 | val_0_auc: 0.9737  |  0:00:40s\n",
      "epoch 30 | loss: 0.18068 | val_0_auc: 0.97536 |  0:00:41s\n",
      "epoch 31 | loss: 0.1617  | val_0_auc: 0.97556 |  0:00:43s\n",
      "epoch 32 | loss: 0.16538 | val_0_auc: 0.97475 |  0:00:44s\n",
      "epoch 33 | loss: 0.15582 | val_0_auc: 0.97535 |  0:00:45s\n",
      "epoch 34 | loss: 0.15463 | val_0_auc: 0.97252 |  0:00:47s\n",
      "epoch 35 | loss: 0.17331 | val_0_auc: 0.97179 |  0:00:48s\n",
      "epoch 36 | loss: 0.15424 | val_0_auc: 0.98035 |  0:00:49s\n",
      "epoch 37 | loss: 0.14942 | val_0_auc: 0.98237 |  0:00:51s\n",
      "epoch 38 | loss: 0.14402 | val_0_auc: 0.9821  |  0:00:52s\n",
      "epoch 39 | loss: 0.14014 | val_0_auc: 0.98164 |  0:00:53s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.98237\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9823653  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 54.1494\n",
      "Function value obtained: -0.9824\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6064908404028442, 'lambda_sparse': 0.09802752601792125, 'n_steps': 8, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.05586 | val_0_auc: 0.73662 |  0:00:01s\n",
      "epoch 1  | loss: 0.65907 | val_0_auc: 0.80299 |  0:00:03s\n",
      "epoch 2  | loss: 0.71405 | val_0_auc: 0.83967 |  0:00:05s\n",
      "epoch 3  | loss: 0.51724 | val_0_auc: 0.85143 |  0:00:07s\n",
      "epoch 4  | loss: 0.57348 | val_0_auc: 0.86121 |  0:00:09s\n",
      "epoch 5  | loss: 0.41655 | val_0_auc: 0.89714 |  0:00:11s\n",
      "epoch 6  | loss: 0.39875 | val_0_auc: 0.88388 |  0:00:13s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7  | loss: 0.39013 | val_0_auc: 0.90152 |  0:00:15s\n",
      "epoch 8  | loss: 0.36236 | val_0_auc: 0.89792 |  0:00:17s\n",
      "epoch 9  | loss: 0.33919 | val_0_auc: 0.91688 |  0:00:19s\n",
      "epoch 10 | loss: 0.33409 | val_0_auc: 0.91964 |  0:00:21s\n",
      "epoch 11 | loss: 0.33303 | val_0_auc: 0.91357 |  0:00:23s\n",
      "epoch 12 | loss: 0.322   | val_0_auc: 0.9241  |  0:00:25s\n",
      "epoch 13 | loss: 0.30788 | val_0_auc: 0.93068 |  0:00:27s\n",
      "epoch 14 | loss: 0.35372 | val_0_auc: 0.9224  |  0:00:29s\n",
      "epoch 15 | loss: 0.34355 | val_0_auc: 0.92832 |  0:00:31s\n",
      "epoch 16 | loss: 0.35848 | val_0_auc: 0.91993 |  0:00:33s\n",
      "epoch 17 | loss: 0.33172 | val_0_auc: 0.91845 |  0:00:35s\n",
      "epoch 18 | loss: 0.31066 | val_0_auc: 0.91978 |  0:00:37s\n",
      "epoch 19 | loss: 0.31724 | val_0_auc: 0.91238 |  0:00:39s\n",
      "epoch 20 | loss: 0.3226  | val_0_auc: 0.9084  |  0:00:41s\n",
      "epoch 21 | loss: 0.31222 | val_0_auc: 0.91552 |  0:00:43s\n",
      "epoch 22 | loss: 0.30155 | val_0_auc: 0.92064 |  0:00:45s\n",
      "epoch 23 | loss: 0.30664 | val_0_auc: 0.9256  |  0:00:47s\n",
      "\n",
      "Early stopping occurred at epoch 23 with best_epoch = 13 and best_val_0_auc = 0.93068\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9306802  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 48.3942\n",
      "Function value obtained: -0.9307\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1909692455222336, 'lambda_sparse': 0.04191148987808326, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.87317 | val_0_auc: 0.82915 |  0:00:01s\n",
      "epoch 1  | loss: 0.469   | val_0_auc: 0.88148 |  0:00:03s\n",
      "epoch 2  | loss: 0.34141 | val_0_auc: 0.91882 |  0:00:04s\n",
      "epoch 3  | loss: 0.27986 | val_0_auc: 0.92916 |  0:00:06s\n",
      "epoch 4  | loss: 0.26482 | val_0_auc: 0.94258 |  0:00:07s\n",
      "epoch 5  | loss: 0.2501  | val_0_auc: 0.94625 |  0:00:09s\n",
      "epoch 6  | loss: 0.24316 | val_0_auc: 0.94439 |  0:00:10s\n",
      "epoch 7  | loss: 0.2358  | val_0_auc: 0.95429 |  0:00:12s\n",
      "epoch 8  | loss: 0.21303 | val_0_auc: 0.96449 |  0:00:14s\n",
      "epoch 9  | loss: 0.1946  | val_0_auc: 0.97335 |  0:00:15s\n",
      "epoch 10 | loss: 0.1872  | val_0_auc: 0.97499 |  0:00:17s\n",
      "epoch 11 | loss: 0.18336 | val_0_auc: 0.96034 |  0:00:18s\n",
      "epoch 12 | loss: 0.17857 | val_0_auc: 0.97615 |  0:00:20s\n",
      "epoch 13 | loss: 0.19006 | val_0_auc: 0.98    |  0:00:21s\n",
      "epoch 14 | loss: 0.16649 | val_0_auc: 0.9785  |  0:00:23s\n",
      "epoch 15 | loss: 0.17184 | val_0_auc: 0.97957 |  0:00:24s\n",
      "epoch 16 | loss: 0.16458 | val_0_auc: 0.97882 |  0:00:26s\n",
      "epoch 17 | loss: 0.15904 | val_0_auc: 0.98376 |  0:00:28s\n",
      "epoch 18 | loss: 0.15236 | val_0_auc: 0.98583 |  0:00:29s\n",
      "epoch 19 | loss: 0.13489 | val_0_auc: 0.98592 |  0:00:31s\n",
      "epoch 20 | loss: 0.13691 | val_0_auc: 0.99053 |  0:00:32s\n",
      "epoch 21 | loss: 0.12835 | val_0_auc: 0.98922 |  0:00:34s\n",
      "epoch 22 | loss: 0.12799 | val_0_auc: 0.98864 |  0:00:35s\n",
      "epoch 23 | loss: 0.12241 | val_0_auc: 0.98758 |  0:00:37s\n",
      "epoch 24 | loss: 0.12889 | val_0_auc: 0.98843 |  0:00:39s\n",
      "epoch 25 | loss: 0.13333 | val_0_auc: 0.98979 |  0:00:40s\n",
      "epoch 26 | loss: 0.13374 | val_0_auc: 0.98644 |  0:00:42s\n",
      "epoch 27 | loss: 0.12767 | val_0_auc: 0.98905 |  0:00:43s\n",
      "epoch 28 | loss: 0.12001 | val_0_auc: 0.99039 |  0:00:45s\n",
      "epoch 29 | loss: 0.11582 | val_0_auc: 0.98998 |  0:00:46s\n",
      "epoch 30 | loss: 0.12333 | val_0_auc: 0.98733 |  0:00:48s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_auc = 0.99053\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99053368  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 48.8705\n",
      "Function value obtained: -0.9905\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.244081893895663, 'lambda_sparse': 0.06798147486794054, 'n_steps': 8, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.07032 | val_0_auc: 0.8133  |  0:00:01s\n",
      "epoch 1  | loss: 0.56946 | val_0_auc: 0.85462 |  0:00:03s\n",
      "epoch 2  | loss: 0.41847 | val_0_auc: 0.89467 |  0:00:05s\n",
      "epoch 3  | loss: 0.35668 | val_0_auc: 0.91523 |  0:00:07s\n",
      "epoch 4  | loss: 0.32077 | val_0_auc: 0.91397 |  0:00:09s\n",
      "epoch 5  | loss: 0.30102 | val_0_auc: 0.92765 |  0:00:11s\n",
      "epoch 6  | loss: 0.29433 | val_0_auc: 0.93909 |  0:00:13s\n",
      "epoch 7  | loss: 0.26345 | val_0_auc: 0.94113 |  0:00:15s\n",
      "epoch 8  | loss: 0.26677 | val_0_auc: 0.9479  |  0:00:17s\n",
      "epoch 9  | loss: 0.24761 | val_0_auc: 0.94826 |  0:00:19s\n",
      "epoch 10 | loss: 0.25229 | val_0_auc: 0.94908 |  0:00:21s\n",
      "epoch 11 | loss: 0.24411 | val_0_auc: 0.94048 |  0:00:23s\n",
      "epoch 12 | loss: 0.26685 | val_0_auc: 0.95246 |  0:00:25s\n",
      "epoch 13 | loss: 0.24516 | val_0_auc: 0.95232 |  0:00:27s\n",
      "epoch 14 | loss: 0.23237 | val_0_auc: 0.95807 |  0:00:29s\n",
      "epoch 15 | loss: 0.21136 | val_0_auc: 0.96156 |  0:00:31s\n",
      "epoch 16 | loss: 0.21322 | val_0_auc: 0.96809 |  0:00:33s\n",
      "epoch 17 | loss: 0.21036 | val_0_auc: 0.96636 |  0:00:35s\n",
      "epoch 18 | loss: 0.20813 | val_0_auc: 0.96361 |  0:00:37s\n",
      "epoch 19 | loss: 0.21219 | val_0_auc: 0.96346 |  0:00:39s\n",
      "epoch 20 | loss: 0.2249  | val_0_auc: 0.97201 |  0:00:41s\n",
      "epoch 21 | loss: 0.20144 | val_0_auc: 0.96201 |  0:00:43s\n",
      "epoch 22 | loss: 0.1888  | val_0_auc: 0.96832 |  0:00:45s\n",
      "epoch 23 | loss: 0.18011 | val_0_auc: 0.97092 |  0:00:47s\n",
      "epoch 24 | loss: 0.18306 | val_0_auc: 0.97036 |  0:00:49s\n",
      "epoch 25 | loss: 0.17547 | val_0_auc: 0.97874 |  0:00:51s\n",
      "epoch 26 | loss: 0.1665  | val_0_auc: 0.97858 |  0:00:53s\n",
      "epoch 27 | loss: 0.1682  | val_0_auc: 0.97223 |  0:00:55s\n",
      "epoch 28 | loss: 0.16266 | val_0_auc: 0.97989 |  0:00:57s\n",
      "epoch 29 | loss: 0.15542 | val_0_auc: 0.98058 |  0:00:59s\n",
      "epoch 30 | loss: 0.15636 | val_0_auc: 0.97973 |  0:01:01s\n",
      "epoch 31 | loss: 0.14965 | val_0_auc: 0.97738 |  0:01:03s\n",
      "epoch 32 | loss: 0.15234 | val_0_auc: 0.98238 |  0:01:05s\n",
      "epoch 33 | loss: 0.15089 | val_0_auc: 0.98553 |  0:01:07s\n",
      "epoch 34 | loss: 0.14975 | val_0_auc: 0.98286 |  0:01:09s\n",
      "epoch 35 | loss: 0.15278 | val_0_auc: 0.97806 |  0:01:11s\n",
      "epoch 36 | loss: 0.15589 | val_0_auc: 0.97833 |  0:01:13s\n",
      "epoch 37 | loss: 0.16162 | val_0_auc: 0.97905 |  0:01:15s\n",
      "epoch 38 | loss: 0.14704 | val_0_auc: 0.97974 |  0:01:17s\n",
      "epoch 39 | loss: 0.15012 | val_0_auc: 0.97728 |  0:01:19s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.98553\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9855302  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 79.8647\n",
      "Function value obtained: -0.9855\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0591120078667848, 'lambda_sparse': 0.04166902061679894, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.97069 | val_0_auc: 0.84503 |  0:00:01s\n",
      "epoch 1  | loss: 0.38045 | val_0_auc: 0.90092 |  0:00:02s\n",
      "epoch 2  | loss: 0.27858 | val_0_auc: 0.93613 |  0:00:04s\n",
      "epoch 3  | loss: 0.24057 | val_0_auc: 0.94697 |  0:00:05s\n",
      "epoch 4  | loss: 0.2214  | val_0_auc: 0.95134 |  0:00:06s\n",
      "epoch 5  | loss: 0.22125 | val_0_auc: 0.96589 |  0:00:08s\n",
      "epoch 6  | loss: 0.19499 | val_0_auc: 0.9726  |  0:00:09s\n",
      "epoch 7  | loss: 0.17152 | val_0_auc: 0.97855 |  0:00:10s\n",
      "epoch 8  | loss: 0.16368 | val_0_auc: 0.97541 |  0:00:12s\n",
      "epoch 9  | loss: 0.14788 | val_0_auc: 0.98523 |  0:00:13s\n",
      "epoch 10 | loss: 0.14922 | val_0_auc: 0.98756 |  0:00:14s\n",
      "epoch 11 | loss: 0.14607 | val_0_auc: 0.98875 |  0:00:16s\n",
      "epoch 12 | loss: 0.13207 | val_0_auc: 0.98969 |  0:00:17s\n",
      "epoch 13 | loss: 0.12307 | val_0_auc: 0.98994 |  0:00:19s\n",
      "epoch 14 | loss: 0.12827 | val_0_auc: 0.98869 |  0:00:20s\n",
      "epoch 15 | loss: 0.12361 | val_0_auc: 0.99027 |  0:00:21s\n",
      "epoch 16 | loss: 0.11851 | val_0_auc: 0.98853 |  0:00:22s\n",
      "epoch 17 | loss: 0.11646 | val_0_auc: 0.9905  |  0:00:24s\n",
      "epoch 18 | loss: 0.1126  | val_0_auc: 0.99249 |  0:00:25s\n",
      "epoch 19 | loss: 0.11659 | val_0_auc: 0.98988 |  0:00:27s\n",
      "epoch 20 | loss: 0.10703 | val_0_auc: 0.99012 |  0:00:28s\n",
      "epoch 21 | loss: 0.10212 | val_0_auc: 0.98824 |  0:00:29s\n",
      "epoch 22 | loss: 0.10294 | val_0_auc: 0.98657 |  0:00:31s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 0.10943 | val_0_auc: 0.99111 |  0:00:32s\n",
      "epoch 24 | loss: 0.12885 | val_0_auc: 0.99054 |  0:00:33s\n",
      "epoch 25 | loss: 0.12467 | val_0_auc: 0.98841 |  0:00:35s\n",
      "epoch 26 | loss: 0.12268 | val_0_auc: 0.9923  |  0:00:36s\n",
      "epoch 27 | loss: 0.1172  | val_0_auc: 0.98467 |  0:00:37s\n",
      "epoch 28 | loss: 0.12318 | val_0_auc: 0.98765 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_val_0_auc = 0.99249\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99248566  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 39.5444\n",
      "Function value obtained: -0.9925\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7357029343816024, 'lambda_sparse': 0.09430889143838982, 'n_steps': 4, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.66696 | val_0_auc: 0.87438 |  0:00:01s\n",
      "epoch 1  | loss: 0.42746 | val_0_auc: 0.88707 |  0:00:02s\n",
      "epoch 2  | loss: 0.35324 | val_0_auc: 0.92057 |  0:00:03s\n",
      "epoch 3  | loss: 0.30372 | val_0_auc: 0.92742 |  0:00:04s\n",
      "epoch 4  | loss: 0.25314 | val_0_auc: 0.94978 |  0:00:05s\n",
      "epoch 5  | loss: 0.23171 | val_0_auc: 0.94554 |  0:00:06s\n",
      "epoch 6  | loss: 0.23115 | val_0_auc: 0.95121 |  0:00:08s\n",
      "epoch 7  | loss: 0.20626 | val_0_auc: 0.95947 |  0:00:09s\n",
      "epoch 8  | loss: 0.1978  | val_0_auc: 0.96248 |  0:00:10s\n",
      "epoch 9  | loss: 0.20132 | val_0_auc: 0.95739 |  0:00:11s\n",
      "epoch 10 | loss: 0.1928  | val_0_auc: 0.9679  |  0:00:12s\n",
      "epoch 11 | loss: 0.19206 | val_0_auc: 0.97523 |  0:00:13s\n",
      "epoch 12 | loss: 0.1777  | val_0_auc: 0.97942 |  0:00:14s\n",
      "epoch 13 | loss: 0.17428 | val_0_auc: 0.97904 |  0:00:15s\n",
      "epoch 14 | loss: 0.15539 | val_0_auc: 0.98044 |  0:00:17s\n",
      "epoch 15 | loss: 0.1535  | val_0_auc: 0.98444 |  0:00:18s\n",
      "epoch 16 | loss: 0.14287 | val_0_auc: 0.98491 |  0:00:19s\n",
      "epoch 17 | loss: 0.13989 | val_0_auc: 0.98392 |  0:00:20s\n",
      "epoch 18 | loss: 0.13417 | val_0_auc: 0.98995 |  0:00:21s\n",
      "epoch 19 | loss: 0.13716 | val_0_auc: 0.98908 |  0:00:22s\n",
      "epoch 20 | loss: 0.13861 | val_0_auc: 0.98765 |  0:00:23s\n",
      "epoch 21 | loss: 0.13405 | val_0_auc: 0.98534 |  0:00:25s\n",
      "epoch 22 | loss: 0.13386 | val_0_auc: 0.98988 |  0:00:26s\n",
      "epoch 23 | loss: 0.12082 | val_0_auc: 0.99221 |  0:00:27s\n",
      "epoch 24 | loss: 0.12543 | val_0_auc: 0.99153 |  0:00:28s\n",
      "epoch 25 | loss: 0.10857 | val_0_auc: 0.99283 |  0:00:29s\n",
      "epoch 26 | loss: 0.11087 | val_0_auc: 0.99332 |  0:00:30s\n",
      "epoch 27 | loss: 0.10335 | val_0_auc: 0.99318 |  0:00:31s\n",
      "epoch 28 | loss: 0.10499 | val_0_auc: 0.99252 |  0:00:32s\n",
      "epoch 29 | loss: 0.10676 | val_0_auc: 0.99233 |  0:00:34s\n",
      "epoch 30 | loss: 0.11162 | val_0_auc: 0.99241 |  0:00:35s\n",
      "epoch 31 | loss: 0.11107 | val_0_auc: 0.99282 |  0:00:36s\n",
      "epoch 32 | loss: 0.1079  | val_0_auc: 0.99254 |  0:00:37s\n",
      "epoch 33 | loss: 0.10311 | val_0_auc: 0.99282 |  0:00:38s\n",
      "epoch 34 | loss: 0.11223 | val_0_auc: 0.99072 |  0:00:39s\n",
      "epoch 35 | loss: 0.10514 | val_0_auc: 0.99252 |  0:00:40s\n",
      "epoch 36 | loss: 0.1094  | val_0_auc: 0.99188 |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.99332\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99331992  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 42.4795\n",
      "Function value obtained: -0.9933\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0026905547929545, 'lambda_sparse': 0.04047151693049093, 'n_steps': 10, 'n_a': 64, 'n_d': 64, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.91742 | val_0_auc: 0.8554  |  0:00:03s\n",
      "epoch 1  | loss: 0.71945 | val_0_auc: 0.8537  |  0:00:06s\n",
      "epoch 2  | loss: 0.57313 | val_0_auc: 0.90768 |  0:00:09s\n",
      "epoch 3  | loss: 0.41611 | val_0_auc: 0.93817 |  0:00:13s\n",
      "epoch 4  | loss: 0.35192 | val_0_auc: 0.94867 |  0:00:16s\n",
      "epoch 5  | loss: 0.27892 | val_0_auc: 0.95092 |  0:00:19s\n",
      "epoch 6  | loss: 0.2566  | val_0_auc: 0.95944 |  0:00:22s\n",
      "epoch 7  | loss: 0.2315  | val_0_auc: 0.96943 |  0:00:26s\n",
      "epoch 8  | loss: 0.23465 | val_0_auc: 0.96958 |  0:00:29s\n",
      "epoch 9  | loss: 0.23888 | val_0_auc: 0.97452 |  0:00:32s\n",
      "epoch 10 | loss: 0.27104 | val_0_auc: 0.96909 |  0:00:36s\n",
      "epoch 11 | loss: 0.22136 | val_0_auc: 0.98286 |  0:00:39s\n",
      "epoch 12 | loss: 0.18607 | val_0_auc: 0.98639 |  0:00:42s\n",
      "epoch 13 | loss: 0.16522 | val_0_auc: 0.98517 |  0:00:46s\n",
      "epoch 14 | loss: 0.17141 | val_0_auc: 0.98322 |  0:00:49s\n",
      "epoch 15 | loss: 0.16735 | val_0_auc: 0.98348 |  0:00:52s\n",
      "epoch 16 | loss: 0.15632 | val_0_auc: 0.98667 |  0:00:55s\n",
      "epoch 17 | loss: 0.14922 | val_0_auc: 0.98639 |  0:00:59s\n",
      "epoch 18 | loss: 0.1467  | val_0_auc: 0.98818 |  0:01:02s\n",
      "epoch 19 | loss: 0.14476 | val_0_auc: 0.98784 |  0:01:05s\n",
      "epoch 20 | loss: 0.13893 | val_0_auc: 0.98922 |  0:01:08s\n",
      "epoch 21 | loss: 0.13838 | val_0_auc: 0.98817 |  0:01:12s\n",
      "epoch 22 | loss: 0.13193 | val_0_auc: 0.98973 |  0:01:15s\n",
      "epoch 23 | loss: 0.1321  | val_0_auc: 0.99042 |  0:01:18s\n",
      "epoch 24 | loss: 0.14009 | val_0_auc: 0.99081 |  0:01:22s\n",
      "epoch 25 | loss: 0.16075 | val_0_auc: 0.98794 |  0:01:25s\n",
      "epoch 26 | loss: 0.15546 | val_0_auc: 0.98819 |  0:01:29s\n",
      "epoch 27 | loss: 0.13933 | val_0_auc: 0.98915 |  0:01:32s\n",
      "epoch 28 | loss: 0.13012 | val_0_auc: 0.99192 |  0:01:36s\n",
      "epoch 29 | loss: 0.12283 | val_0_auc: 0.98874 |  0:01:39s\n",
      "epoch 30 | loss: 0.12738 | val_0_auc: 0.99177 |  0:01:42s\n",
      "epoch 31 | loss: 0.12317 | val_0_auc: 0.98859 |  0:01:46s\n",
      "epoch 32 | loss: 0.12607 | val_0_auc: 0.98874 |  0:01:49s\n",
      "epoch 33 | loss: 0.1222  | val_0_auc: 0.98957 |  0:01:52s\n",
      "epoch 34 | loss: 0.13167 | val_0_auc: 0.99052 |  0:01:55s\n",
      "epoch 35 | loss: 0.12378 | val_0_auc: 0.99178 |  0:01:58s\n",
      "epoch 36 | loss: 0.12385 | val_0_auc: 0.99233 |  0:02:02s\n",
      "epoch 37 | loss: 0.12871 | val_0_auc: 0.98993 |  0:02:05s\n",
      "epoch 38 | loss: 0.12039 | val_0_auc: 0.99049 |  0:02:08s\n",
      "epoch 39 | loss: 0.121   | val_0_auc: 0.99017 |  0:02:11s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99233\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99232772  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 132.6657\n",
      "Function value obtained: -0.9923\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1884827038868113, 'lambda_sparse': 0.044890591705362096, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.73652 | val_0_auc: 0.78482 |  0:00:00s\n",
      "epoch 1  | loss: 0.44116 | val_0_auc: 0.85483 |  0:00:01s\n",
      "epoch 2  | loss: 0.33864 | val_0_auc: 0.88484 |  0:00:02s\n",
      "epoch 3  | loss: 0.29719 | val_0_auc: 0.91031 |  0:00:03s\n",
      "epoch 4  | loss: 0.26525 | val_0_auc: 0.92477 |  0:00:04s\n",
      "epoch 5  | loss: 0.24404 | val_0_auc: 0.93396 |  0:00:05s\n",
      "epoch 6  | loss: 0.23893 | val_0_auc: 0.93946 |  0:00:06s\n",
      "epoch 7  | loss: 0.23611 | val_0_auc: 0.95052 |  0:00:07s\n",
      "epoch 8  | loss: 0.22499 | val_0_auc: 0.95614 |  0:00:08s\n",
      "epoch 9  | loss: 0.22191 | val_0_auc: 0.95489 |  0:00:09s\n",
      "epoch 10 | loss: 0.20472 | val_0_auc: 0.96289 |  0:00:09s\n",
      "epoch 11 | loss: 0.20026 | val_0_auc: 0.96705 |  0:00:10s\n",
      "epoch 12 | loss: 0.18193 | val_0_auc: 0.96784 |  0:00:11s\n",
      "epoch 13 | loss: 0.19037 | val_0_auc: 0.96894 |  0:00:12s\n",
      "epoch 14 | loss: 0.18305 | val_0_auc: 0.97357 |  0:00:13s\n",
      "epoch 15 | loss: 0.17973 | val_0_auc: 0.97791 |  0:00:14s\n",
      "epoch 16 | loss: 0.16576 | val_0_auc: 0.97174 |  0:00:15s\n",
      "epoch 17 | loss: 0.16663 | val_0_auc: 0.97276 |  0:00:16s\n",
      "epoch 18 | loss: 0.15612 | val_0_auc: 0.98014 |  0:00:16s\n",
      "epoch 19 | loss: 0.15804 | val_0_auc: 0.98046 |  0:00:17s\n",
      "epoch 20 | loss: 0.1579  | val_0_auc: 0.97933 |  0:00:18s\n",
      "epoch 21 | loss: 0.15907 | val_0_auc: 0.97287 |  0:00:19s\n",
      "epoch 22 | loss: 0.14823 | val_0_auc: 0.97854 |  0:00:20s\n",
      "epoch 23 | loss: 0.14637 | val_0_auc: 0.97913 |  0:00:21s\n",
      "epoch 24 | loss: 0.13937 | val_0_auc: 0.98212 |  0:00:22s\n",
      "epoch 25 | loss: 0.13902 | val_0_auc: 0.97854 |  0:00:23s\n",
      "epoch 26 | loss: 0.13762 | val_0_auc: 0.98082 |  0:00:24s\n",
      "epoch 27 | loss: 0.1431  | val_0_auc: 0.98204 |  0:00:24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28 | loss: 0.13688 | val_0_auc: 0.98374 |  0:00:25s\n",
      "epoch 29 | loss: 0.14359 | val_0_auc: 0.98379 |  0:00:26s\n",
      "epoch 30 | loss: 0.13187 | val_0_auc: 0.98707 |  0:00:27s\n",
      "epoch 31 | loss: 0.13054 | val_0_auc: 0.98629 |  0:00:28s\n",
      "epoch 32 | loss: 0.12252 | val_0_auc: 0.98569 |  0:00:29s\n",
      "epoch 33 | loss: 0.12433 | val_0_auc: 0.98625 |  0:00:30s\n",
      "epoch 34 | loss: 0.11962 | val_0_auc: 0.98514 |  0:00:31s\n",
      "epoch 35 | loss: 0.12589 | val_0_auc: 0.98716 |  0:00:31s\n",
      "epoch 36 | loss: 0.12541 | val_0_auc: 0.98794 |  0:00:32s\n",
      "epoch 37 | loss: 0.12748 | val_0_auc: 0.98835 |  0:00:33s\n",
      "epoch 38 | loss: 0.12641 | val_0_auc: 0.98449 |  0:00:34s\n",
      "epoch 39 | loss: 0.12077 | val_0_auc: 0.98603 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.98835\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98835491  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 35.9018\n",
      "Function value obtained: -0.9884\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.042226716655698, 'lambda_sparse': 0.09439209844515684, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.7856  | val_0_auc: 0.80947 |  0:00:00s\n",
      "epoch 1  | loss: 0.47028 | val_0_auc: 0.86867 |  0:00:01s\n",
      "epoch 2  | loss: 0.36492 | val_0_auc: 0.90118 |  0:00:02s\n",
      "epoch 3  | loss: 0.32548 | val_0_auc: 0.9194  |  0:00:03s\n",
      "epoch 4  | loss: 0.29607 | val_0_auc: 0.93516 |  0:00:04s\n",
      "epoch 5  | loss: 0.25805 | val_0_auc: 0.94375 |  0:00:05s\n",
      "epoch 6  | loss: 0.24547 | val_0_auc: 0.94722 |  0:00:06s\n",
      "epoch 7  | loss: 0.2285  | val_0_auc: 0.95132 |  0:00:07s\n",
      "epoch 8  | loss: 0.21692 | val_0_auc: 0.96597 |  0:00:08s\n",
      "epoch 9  | loss: 0.2091  | val_0_auc: 0.9685  |  0:00:08s\n",
      "epoch 10 | loss: 0.19793 | val_0_auc: 0.96586 |  0:00:09s\n",
      "epoch 11 | loss: 0.19083 | val_0_auc: 0.96547 |  0:00:10s\n",
      "epoch 12 | loss: 0.17826 | val_0_auc: 0.97078 |  0:00:11s\n",
      "epoch 13 | loss: 0.17331 | val_0_auc: 0.96805 |  0:00:12s\n",
      "epoch 14 | loss: 0.16937 | val_0_auc: 0.97023 |  0:00:13s\n",
      "epoch 15 | loss: 0.17104 | val_0_auc: 0.96736 |  0:00:14s\n",
      "epoch 16 | loss: 0.16191 | val_0_auc: 0.96709 |  0:00:15s\n",
      "epoch 17 | loss: 0.15346 | val_0_auc: 0.97183 |  0:00:16s\n",
      "epoch 18 | loss: 0.14925 | val_0_auc: 0.97179 |  0:00:16s\n",
      "epoch 19 | loss: 0.14774 | val_0_auc: 0.97471 |  0:00:17s\n",
      "epoch 20 | loss: 0.14469 | val_0_auc: 0.9735  |  0:00:18s\n",
      "epoch 21 | loss: 0.1505  | val_0_auc: 0.97559 |  0:00:19s\n",
      "epoch 22 | loss: 0.14442 | val_0_auc: 0.97421 |  0:00:20s\n",
      "epoch 23 | loss: 0.15287 | val_0_auc: 0.9751  |  0:00:21s\n",
      "epoch 24 | loss: 0.15535 | val_0_auc: 0.97587 |  0:00:22s\n",
      "epoch 25 | loss: 0.14937 | val_0_auc: 0.97601 |  0:00:23s\n",
      "epoch 26 | loss: 0.13871 | val_0_auc: 0.98152 |  0:00:24s\n",
      "epoch 27 | loss: 0.14133 | val_0_auc: 0.98243 |  0:00:25s\n",
      "epoch 28 | loss: 0.12786 | val_0_auc: 0.98456 |  0:00:25s\n",
      "epoch 29 | loss: 0.12637 | val_0_auc: 0.98587 |  0:00:26s\n",
      "epoch 30 | loss: 0.12074 | val_0_auc: 0.98663 |  0:00:27s\n",
      "epoch 31 | loss: 0.12189 | val_0_auc: 0.9868  |  0:00:28s\n",
      "epoch 32 | loss: 0.10819 | val_0_auc: 0.988   |  0:00:29s\n",
      "epoch 33 | loss: 0.10939 | val_0_auc: 0.98779 |  0:00:30s\n",
      "epoch 34 | loss: 0.10836 | val_0_auc: 0.9884  |  0:00:31s\n",
      "epoch 35 | loss: 0.10159 | val_0_auc: 0.98689 |  0:00:32s\n",
      "epoch 36 | loss: 0.1126  | val_0_auc: 0.98689 |  0:00:33s\n",
      "epoch 37 | loss: 0.11823 | val_0_auc: 0.98472 |  0:00:33s\n",
      "epoch 38 | loss: 0.12095 | val_0_auc: 0.98624 |  0:00:34s\n",
      "epoch 39 | loss: 0.11575 | val_0_auc: 0.98939 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.98939\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98938962  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 36.0249\n",
      "Function value obtained: -0.9894\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4853771117171342, 'lambda_sparse': 0.0010689582012454117, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.76193 | val_0_auc: 0.83931 |  0:00:01s\n",
      "epoch 1  | loss: 0.3951  | val_0_auc: 0.88964 |  0:00:02s\n",
      "epoch 2  | loss: 0.32172 | val_0_auc: 0.89669 |  0:00:04s\n",
      "epoch 3  | loss: 0.28474 | val_0_auc: 0.92305 |  0:00:05s\n",
      "epoch 4  | loss: 0.27063 | val_0_auc: 0.94762 |  0:00:07s\n",
      "epoch 5  | loss: 0.25823 | val_0_auc: 0.94922 |  0:00:09s\n",
      "epoch 6  | loss: 0.24029 | val_0_auc: 0.94758 |  0:00:10s\n",
      "epoch 7  | loss: 0.23193 | val_0_auc: 0.96015 |  0:00:12s\n",
      "epoch 8  | loss: 0.23911 | val_0_auc: 0.96082 |  0:00:13s\n",
      "epoch 9  | loss: 0.22342 | val_0_auc: 0.96575 |  0:00:15s\n",
      "epoch 10 | loss: 0.2496  | val_0_auc: 0.94046 |  0:00:16s\n",
      "epoch 11 | loss: 0.23048 | val_0_auc: 0.96419 |  0:00:18s\n",
      "epoch 12 | loss: 0.21369 | val_0_auc: 0.96238 |  0:00:19s\n",
      "epoch 13 | loss: 0.2102  | val_0_auc: 0.9612  |  0:00:21s\n",
      "epoch 14 | loss: 0.19696 | val_0_auc: 0.96863 |  0:00:22s\n",
      "epoch 15 | loss: 0.18477 | val_0_auc: 0.96417 |  0:00:24s\n",
      "epoch 16 | loss: 0.18148 | val_0_auc: 0.9702  |  0:00:25s\n",
      "epoch 17 | loss: 0.16625 | val_0_auc: 0.96562 |  0:00:27s\n",
      "epoch 18 | loss: 0.15917 | val_0_auc: 0.97076 |  0:00:28s\n",
      "epoch 19 | loss: 0.17207 | val_0_auc: 0.97547 |  0:00:30s\n",
      "epoch 20 | loss: 0.18976 | val_0_auc: 0.97132 |  0:00:31s\n",
      "epoch 21 | loss: 0.17102 | val_0_auc: 0.96848 |  0:00:33s\n",
      "epoch 22 | loss: 0.15446 | val_0_auc: 0.97011 |  0:00:34s\n",
      "epoch 23 | loss: 0.14859 | val_0_auc: 0.9709  |  0:00:35s\n",
      "epoch 24 | loss: 0.14812 | val_0_auc: 0.97429 |  0:00:37s\n",
      "epoch 25 | loss: 0.14926 | val_0_auc: 0.97625 |  0:00:38s\n",
      "epoch 26 | loss: 0.15286 | val_0_auc: 0.97899 |  0:00:40s\n",
      "epoch 27 | loss: 0.14871 | val_0_auc: 0.97765 |  0:00:41s\n",
      "epoch 28 | loss: 0.15346 | val_0_auc: 0.97318 |  0:00:43s\n",
      "epoch 29 | loss: 0.14297 | val_0_auc: 0.97585 |  0:00:44s\n",
      "epoch 30 | loss: 0.15147 | val_0_auc: 0.97545 |  0:00:46s\n",
      "epoch 31 | loss: 0.1438  | val_0_auc: 0.97651 |  0:00:47s\n",
      "epoch 32 | loss: 0.14496 | val_0_auc: 0.97501 |  0:00:49s\n",
      "epoch 33 | loss: 0.13795 | val_0_auc: 0.97739 |  0:00:50s\n",
      "epoch 34 | loss: 0.13383 | val_0_auc: 0.98029 |  0:00:52s\n",
      "epoch 35 | loss: 0.12158 | val_0_auc: 0.98206 |  0:00:53s\n",
      "epoch 36 | loss: 0.12416 | val_0_auc: 0.98339 |  0:00:55s\n",
      "epoch 37 | loss: 0.12532 | val_0_auc: 0.98374 |  0:00:56s\n",
      "epoch 38 | loss: 0.12482 | val_0_auc: 0.98218 |  0:00:58s\n",
      "epoch 39 | loss: 0.1298  | val_0_auc: 0.98295 |  0:00:59s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.98374\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98373615  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 60.4742\n",
      "Function value obtained: -0.9837\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0436324061750264, 'lambda_sparse': 0.08758151661442899, 'n_steps': 6, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.15682 | val_0_auc: 0.83914 |  0:00:01s\n",
      "epoch 1  | loss: 0.46728 | val_0_auc: 0.8963  |  0:00:03s\n",
      "epoch 2  | loss: 0.36334 | val_0_auc: 0.91509 |  0:00:05s\n",
      "epoch 3  | loss: 0.32652 | val_0_auc: 0.93423 |  0:00:07s\n",
      "epoch 4  | loss: 0.30523 | val_0_auc: 0.93613 |  0:00:09s\n",
      "epoch 5  | loss: 0.28067 | val_0_auc: 0.94843 |  0:00:11s\n",
      "epoch 6  | loss: 0.25785 | val_0_auc: 0.95722 |  0:00:13s\n",
      "epoch 7  | loss: 0.23847 | val_0_auc: 0.95897 |  0:00:15s\n",
      "epoch 8  | loss: 0.22157 | val_0_auc: 0.9639  |  0:00:17s\n",
      "epoch 9  | loss: 0.21078 | val_0_auc: 0.97622 |  0:00:19s\n",
      "epoch 10 | loss: 0.19356 | val_0_auc: 0.97787 |  0:00:21s\n",
      "epoch 11 | loss: 0.18705 | val_0_auc: 0.97099 |  0:00:23s\n",
      "epoch 12 | loss: 0.18492 | val_0_auc: 0.97553 |  0:00:25s\n",
      "epoch 13 | loss: 0.18204 | val_0_auc: 0.98124 |  0:00:27s\n",
      "epoch 14 | loss: 0.17765 | val_0_auc: 0.98178 |  0:00:28s\n",
      "epoch 15 | loss: 0.1715  | val_0_auc: 0.98332 |  0:00:30s\n",
      "epoch 16 | loss: 0.17082 | val_0_auc: 0.98205 |  0:00:32s\n",
      "epoch 17 | loss: 0.16009 | val_0_auc: 0.9853  |  0:00:34s\n",
      "epoch 18 | loss: 0.15743 | val_0_auc: 0.98458 |  0:00:36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19 | loss: 0.16027 | val_0_auc: 0.98624 |  0:00:38s\n",
      "epoch 20 | loss: 0.153   | val_0_auc: 0.98394 |  0:00:40s\n",
      "epoch 21 | loss: 0.15372 | val_0_auc: 0.98602 |  0:00:42s\n",
      "epoch 22 | loss: 0.17728 | val_0_auc: 0.98685 |  0:00:44s\n",
      "epoch 23 | loss: 0.16163 | val_0_auc: 0.98798 |  0:00:46s\n",
      "epoch 24 | loss: 0.15645 | val_0_auc: 0.98636 |  0:00:48s\n",
      "epoch 25 | loss: 0.15042 | val_0_auc: 0.98747 |  0:00:50s\n",
      "epoch 26 | loss: 0.1441  | val_0_auc: 0.989   |  0:00:52s\n",
      "epoch 27 | loss: 0.13922 | val_0_auc: 0.98795 |  0:00:54s\n",
      "epoch 28 | loss: 0.13053 | val_0_auc: 0.9886  |  0:00:55s\n",
      "epoch 29 | loss: 0.13547 | val_0_auc: 0.98187 |  0:00:57s\n",
      "epoch 30 | loss: 0.13764 | val_0_auc: 0.98724 |  0:00:59s\n",
      "epoch 31 | loss: 0.14285 | val_0_auc: 0.98537 |  0:01:01s\n",
      "epoch 32 | loss: 0.13517 | val_0_auc: 0.98275 |  0:01:03s\n",
      "epoch 33 | loss: 0.148   | val_0_auc: 0.98811 |  0:01:05s\n",
      "epoch 34 | loss: 0.14497 | val_0_auc: 0.9857  |  0:01:07s\n",
      "epoch 35 | loss: 0.14617 | val_0_auc: 0.98705 |  0:01:09s\n",
      "epoch 36 | loss: 0.14121 | val_0_auc: 0.97855 |  0:01:11s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.989\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98899679  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 71.9814\n",
      "Function value obtained: -0.9890\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1846468488153306, 'lambda_sparse': 0.053232601100916994, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.14332 | val_0_auc: 0.8424  |  0:00:01s\n",
      "epoch 1  | loss: 0.45307 | val_0_auc: 0.88312 |  0:00:03s\n",
      "epoch 2  | loss: 0.36454 | val_0_auc: 0.9037  |  0:00:05s\n",
      "epoch 3  | loss: 0.31422 | val_0_auc: 0.92337 |  0:00:06s\n",
      "epoch 4  | loss: 0.27353 | val_0_auc: 0.9386  |  0:00:08s\n",
      "epoch 5  | loss: 0.24602 | val_0_auc: 0.94529 |  0:00:10s\n",
      "epoch 6  | loss: 0.22039 | val_0_auc: 0.94265 |  0:00:11s\n",
      "epoch 7  | loss: 0.2217  | val_0_auc: 0.95091 |  0:00:13s\n",
      "epoch 8  | loss: 0.22077 | val_0_auc: 0.95838 |  0:00:15s\n",
      "epoch 9  | loss: 0.21335 | val_0_auc: 0.96405 |  0:00:16s\n",
      "epoch 10 | loss: 0.19679 | val_0_auc: 0.96201 |  0:00:18s\n",
      "epoch 11 | loss: 0.18968 | val_0_auc: 0.96874 |  0:00:20s\n",
      "epoch 12 | loss: 0.20224 | val_0_auc: 0.97036 |  0:00:21s\n",
      "epoch 13 | loss: 0.19019 | val_0_auc: 0.97471 |  0:00:23s\n",
      "epoch 14 | loss: 0.17839 | val_0_auc: 0.97105 |  0:00:25s\n",
      "epoch 15 | loss: 0.18454 | val_0_auc: 0.97376 |  0:00:26s\n",
      "epoch 16 | loss: 0.17011 | val_0_auc: 0.98239 |  0:00:28s\n",
      "epoch 17 | loss: 0.16307 | val_0_auc: 0.98046 |  0:00:30s\n",
      "epoch 18 | loss: 0.16228 | val_0_auc: 0.98656 |  0:00:31s\n",
      "epoch 19 | loss: 0.16013 | val_0_auc: 0.98358 |  0:00:33s\n",
      "epoch 20 | loss: 0.15799 | val_0_auc: 0.97749 |  0:00:35s\n",
      "epoch 21 | loss: 0.16002 | val_0_auc: 0.98044 |  0:00:36s\n",
      "epoch 22 | loss: 0.15158 | val_0_auc: 0.98356 |  0:00:38s\n",
      "epoch 23 | loss: 0.14127 | val_0_auc: 0.98518 |  0:00:40s\n",
      "epoch 24 | loss: 0.13398 | val_0_auc: 0.98955 |  0:00:41s\n",
      "epoch 25 | loss: 0.1286  | val_0_auc: 0.98983 |  0:00:43s\n",
      "epoch 26 | loss: 0.12752 | val_0_auc: 0.98843 |  0:00:44s\n",
      "epoch 27 | loss: 0.12922 | val_0_auc: 0.98745 |  0:00:46s\n",
      "epoch 28 | loss: 0.13873 | val_0_auc: 0.9894  |  0:00:48s\n",
      "epoch 29 | loss: 0.12589 | val_0_auc: 0.99047 |  0:00:49s\n",
      "epoch 30 | loss: 0.12789 | val_0_auc: 0.99149 |  0:00:51s\n",
      "epoch 31 | loss: 0.12405 | val_0_auc: 0.98917 |  0:00:53s\n",
      "epoch 32 | loss: 0.13143 | val_0_auc: 0.98777 |  0:00:54s\n",
      "epoch 33 | loss: 0.1336  | val_0_auc: 0.9909  |  0:00:56s\n",
      "epoch 34 | loss: 0.12784 | val_0_auc: 0.98994 |  0:00:58s\n",
      "epoch 35 | loss: 0.12901 | val_0_auc: 0.99047 |  0:00:59s\n",
      "epoch 36 | loss: 0.13208 | val_0_auc: 0.99048 |  0:01:01s\n",
      "epoch 37 | loss: 0.12663 | val_0_auc: 0.99149 |  0:01:03s\n",
      "epoch 38 | loss: 0.1109  | val_0_auc: 0.99331 |  0:01:04s\n",
      "epoch 39 | loss: 0.11872 | val_0_auc: 0.99288 |  0:01:06s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99331\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99331384  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 67.2773\n",
      "Function value obtained: -0.9933\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4384176364857861, 'lambda_sparse': 0.08296908388022578, 'n_steps': 8, 'n_a': 8, 'n_d': 8, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.85761 | val_0_auc: 0.72024 |  0:00:01s\n",
      "epoch 1  | loss: 0.58381 | val_0_auc: 0.81017 |  0:00:02s\n",
      "epoch 2  | loss: 0.48668 | val_0_auc: 0.84475 |  0:00:04s\n",
      "epoch 3  | loss: 0.43443 | val_0_auc: 0.85225 |  0:00:05s\n",
      "epoch 4  | loss: 0.41763 | val_0_auc: 0.86916 |  0:00:06s\n",
      "epoch 5  | loss: 0.38848 | val_0_auc: 0.87477 |  0:00:07s\n",
      "epoch 6  | loss: 0.35451 | val_0_auc: 0.89739 |  0:00:09s\n",
      "epoch 7  | loss: 0.35397 | val_0_auc: 0.89579 |  0:00:10s\n",
      "epoch 8  | loss: 0.31691 | val_0_auc: 0.91476 |  0:00:11s\n",
      "epoch 9  | loss: 0.30931 | val_0_auc: 0.93041 |  0:00:13s\n",
      "epoch 10 | loss: 0.28515 | val_0_auc: 0.94314 |  0:00:14s\n",
      "epoch 11 | loss: 0.27135 | val_0_auc: 0.95143 |  0:00:15s\n",
      "epoch 12 | loss: 0.29784 | val_0_auc: 0.95019 |  0:00:16s\n",
      "epoch 13 | loss: 0.29323 | val_0_auc: 0.961   |  0:00:18s\n",
      "epoch 14 | loss: 0.26332 | val_0_auc: 0.95925 |  0:00:19s\n",
      "epoch 15 | loss: 0.24329 | val_0_auc: 0.95607 |  0:00:20s\n",
      "epoch 16 | loss: 0.23631 | val_0_auc: 0.96367 |  0:00:22s\n",
      "epoch 17 | loss: 0.20509 | val_0_auc: 0.97173 |  0:00:23s\n",
      "epoch 18 | loss: 0.19715 | val_0_auc: 0.97087 |  0:00:24s\n",
      "epoch 19 | loss: 0.19579 | val_0_auc: 0.97383 |  0:00:25s\n",
      "epoch 20 | loss: 0.20238 | val_0_auc: 0.97194 |  0:00:27s\n",
      "epoch 21 | loss: 0.22193 | val_0_auc: 0.95743 |  0:00:28s\n",
      "epoch 22 | loss: 0.20631 | val_0_auc: 0.96873 |  0:00:29s\n",
      "epoch 23 | loss: 0.21595 | val_0_auc: 0.96318 |  0:00:31s\n",
      "epoch 24 | loss: 0.21541 | val_0_auc: 0.96665 |  0:00:32s\n",
      "epoch 25 | loss: 0.21591 | val_0_auc: 0.96642 |  0:00:33s\n",
      "epoch 26 | loss: 0.2121  | val_0_auc: 0.9667  |  0:00:34s\n",
      "epoch 27 | loss: 0.24713 | val_0_auc: 0.96269 |  0:00:36s\n",
      "epoch 28 | loss: 0.21766 | val_0_auc: 0.96758 |  0:00:37s\n",
      "epoch 29 | loss: 0.21997 | val_0_auc: 0.95511 |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.97383\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97383043  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 39.1789\n",
      "Function value obtained: -0.9738\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8688215130566916, 'lambda_sparse': 0.023574964699866446, 'n_steps': 7, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.79223 | val_0_auc: 0.74914 |  0:00:01s\n",
      "epoch 1  | loss: 0.51329 | val_0_auc: 0.8019  |  0:00:02s\n",
      "epoch 2  | loss: 0.45657 | val_0_auc: 0.81771 |  0:00:04s\n",
      "epoch 3  | loss: 0.42634 | val_0_auc: 0.86205 |  0:00:05s\n",
      "epoch 4  | loss: 0.37571 | val_0_auc: 0.88569 |  0:00:06s\n",
      "epoch 5  | loss: 0.35204 | val_0_auc: 0.89291 |  0:00:08s\n",
      "epoch 6  | loss: 0.33445 | val_0_auc: 0.90567 |  0:00:09s\n",
      "epoch 7  | loss: 0.32805 | val_0_auc: 0.90115 |  0:00:11s\n",
      "epoch 8  | loss: 0.3205  | val_0_auc: 0.91882 |  0:00:12s\n",
      "epoch 9  | loss: 0.32154 | val_0_auc: 0.91569 |  0:00:13s\n",
      "epoch 10 | loss: 0.32458 | val_0_auc: 0.91121 |  0:00:15s\n",
      "epoch 11 | loss: 0.31138 | val_0_auc: 0.90637 |  0:00:16s\n",
      "epoch 12 | loss: 0.30941 | val_0_auc: 0.91953 |  0:00:18s\n",
      "epoch 13 | loss: 0.30415 | val_0_auc: 0.91247 |  0:00:19s\n",
      "epoch 14 | loss: 0.30505 | val_0_auc: 0.91501 |  0:00:20s\n",
      "epoch 15 | loss: 0.29001 | val_0_auc: 0.92974 |  0:00:22s\n",
      "epoch 16 | loss: 0.28709 | val_0_auc: 0.93582 |  0:00:23s\n",
      "epoch 17 | loss: 0.29667 | val_0_auc: 0.93761 |  0:00:25s\n",
      "epoch 18 | loss: 0.27281 | val_0_auc: 0.93973 |  0:00:26s\n",
      "epoch 19 | loss: 0.29251 | val_0_auc: 0.92322 |  0:00:28s\n",
      "epoch 20 | loss: 0.28058 | val_0_auc: 0.93199 |  0:00:29s\n",
      "epoch 21 | loss: 0.26847 | val_0_auc: 0.94176 |  0:00:30s\n",
      "epoch 22 | loss: 0.24643 | val_0_auc: 0.93976 |  0:00:32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 0.24192 | val_0_auc: 0.93794 |  0:00:33s\n",
      "epoch 24 | loss: 0.24059 | val_0_auc: 0.93964 |  0:00:34s\n",
      "epoch 25 | loss: 0.24419 | val_0_auc: 0.9363  |  0:00:36s\n",
      "epoch 26 | loss: 0.25089 | val_0_auc: 0.94116 |  0:00:37s\n",
      "epoch 27 | loss: 0.23989 | val_0_auc: 0.94403 |  0:00:39s\n",
      "epoch 28 | loss: 0.22961 | val_0_auc: 0.94854 |  0:00:40s\n",
      "epoch 29 | loss: 0.2298  | val_0_auc: 0.94903 |  0:00:41s\n",
      "epoch 30 | loss: 0.21489 | val_0_auc: 0.94408 |  0:00:43s\n",
      "epoch 31 | loss: 0.21556 | val_0_auc: 0.94795 |  0:00:44s\n",
      "epoch 32 | loss: 0.21676 | val_0_auc: 0.95365 |  0:00:45s\n",
      "epoch 33 | loss: 0.21377 | val_0_auc: 0.95967 |  0:00:47s\n",
      "epoch 34 | loss: 0.20359 | val_0_auc: 0.9577  |  0:00:48s\n",
      "epoch 35 | loss: 0.2062  | val_0_auc: 0.95902 |  0:00:50s\n",
      "epoch 36 | loss: 0.2034  | val_0_auc: 0.96924 |  0:00:51s\n",
      "epoch 37 | loss: 0.17953 | val_0_auc: 0.9637  |  0:00:52s\n",
      "epoch 38 | loss: 0.182   | val_0_auc: 0.9713  |  0:00:54s\n",
      "epoch 39 | loss: 0.16865 | val_0_auc: 0.97326 |  0:00:55s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.97326\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97325941  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 56.1067\n",
      "Function value obtained: -0.9733\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.621202397371734, 'lambda_sparse': 0.02502662978364688, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.67713 | val_0_auc: 0.82424 |  0:00:00s\n",
      "epoch 1  | loss: 0.41976 | val_0_auc: 0.87906 |  0:00:01s\n",
      "epoch 2  | loss: 0.34746 | val_0_auc: 0.89564 |  0:00:02s\n",
      "epoch 3  | loss: 0.29079 | val_0_auc: 0.91835 |  0:00:03s\n",
      "epoch 4  | loss: 0.26337 | val_0_auc: 0.92759 |  0:00:04s\n",
      "epoch 5  | loss: 0.24318 | val_0_auc: 0.93351 |  0:00:05s\n",
      "epoch 6  | loss: 0.23662 | val_0_auc: 0.93625 |  0:00:06s\n",
      "epoch 7  | loss: 0.23018 | val_0_auc: 0.93727 |  0:00:07s\n",
      "epoch 8  | loss: 0.22193 | val_0_auc: 0.94217 |  0:00:08s\n",
      "epoch 9  | loss: 0.21626 | val_0_auc: 0.94666 |  0:00:08s\n",
      "epoch 10 | loss: 0.20956 | val_0_auc: 0.94624 |  0:00:09s\n",
      "epoch 11 | loss: 0.20462 | val_0_auc: 0.94051 |  0:00:10s\n",
      "epoch 12 | loss: 0.19996 | val_0_auc: 0.95122 |  0:00:11s\n",
      "epoch 13 | loss: 0.20478 | val_0_auc: 0.9557  |  0:00:12s\n",
      "epoch 14 | loss: 0.19065 | val_0_auc: 0.94665 |  0:00:13s\n",
      "epoch 15 | loss: 0.18679 | val_0_auc: 0.94797 |  0:00:14s\n",
      "epoch 16 | loss: 0.19129 | val_0_auc: 0.95257 |  0:00:15s\n",
      "epoch 17 | loss: 0.18418 | val_0_auc: 0.95451 |  0:00:16s\n",
      "epoch 18 | loss: 0.18543 | val_0_auc: 0.96249 |  0:00:16s\n",
      "epoch 19 | loss: 0.17218 | val_0_auc: 0.96621 |  0:00:17s\n",
      "epoch 20 | loss: 0.16165 | val_0_auc: 0.9658  |  0:00:18s\n",
      "epoch 21 | loss: 0.16541 | val_0_auc: 0.96902 |  0:00:19s\n",
      "epoch 22 | loss: 0.1544  | val_0_auc: 0.97474 |  0:00:20s\n",
      "epoch 23 | loss: 0.15838 | val_0_auc: 0.97805 |  0:00:21s\n",
      "epoch 24 | loss: 0.15144 | val_0_auc: 0.98151 |  0:00:22s\n",
      "epoch 25 | loss: 0.15646 | val_0_auc: 0.97775 |  0:00:23s\n",
      "epoch 26 | loss: 0.1499  | val_0_auc: 0.98122 |  0:00:23s\n",
      "epoch 27 | loss: 0.14831 | val_0_auc: 0.9832  |  0:00:24s\n",
      "epoch 28 | loss: 0.1374  | val_0_auc: 0.98147 |  0:00:25s\n",
      "epoch 29 | loss: 0.13463 | val_0_auc: 0.98341 |  0:00:26s\n",
      "epoch 30 | loss: 0.13184 | val_0_auc: 0.97696 |  0:00:27s\n",
      "epoch 31 | loss: 0.15016 | val_0_auc: 0.97631 |  0:00:28s\n",
      "epoch 32 | loss: 0.13715 | val_0_auc: 0.98198 |  0:00:29s\n",
      "epoch 33 | loss: 0.12051 | val_0_auc: 0.98362 |  0:00:30s\n",
      "epoch 34 | loss: 0.12589 | val_0_auc: 0.98793 |  0:00:30s\n",
      "epoch 35 | loss: 0.1221  | val_0_auc: 0.98702 |  0:00:31s\n",
      "epoch 36 | loss: 0.11728 | val_0_auc: 0.98906 |  0:00:32s\n",
      "epoch 37 | loss: 0.10929 | val_0_auc: 0.9853  |  0:00:33s\n",
      "epoch 38 | loss: 0.11268 | val_0_auc: 0.99009 |  0:00:34s\n",
      "epoch 39 | loss: 0.10693 | val_0_auc: 0.98948 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99009\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99009428  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 35.8155\n",
      "Function value obtained: -0.9901\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6688993863495538, 'lambda_sparse': 0.09878783771009586, 'n_steps': 10, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.40841 | val_0_auc: 0.76145 |  0:00:02s\n",
      "epoch 1  | loss: 0.68834 | val_0_auc: 0.8241  |  0:00:04s\n",
      "epoch 2  | loss: 0.53916 | val_0_auc: 0.83394 |  0:00:06s\n",
      "epoch 3  | loss: 0.4763  | val_0_auc: 0.85259 |  0:00:08s\n",
      "epoch 4  | loss: 0.42824 | val_0_auc: 0.87343 |  0:00:10s\n",
      "epoch 5  | loss: 0.42015 | val_0_auc: 0.87678 |  0:00:12s\n",
      "epoch 6  | loss: 0.52142 | val_0_auc: 0.89084 |  0:00:14s\n",
      "epoch 7  | loss: 0.41081 | val_0_auc: 0.91639 |  0:00:17s\n",
      "epoch 8  | loss: 0.3445  | val_0_auc: 0.90236 |  0:00:19s\n",
      "epoch 9  | loss: 0.33783 | val_0_auc: 0.89014 |  0:00:21s\n",
      "epoch 10 | loss: 0.34536 | val_0_auc: 0.90153 |  0:00:23s\n",
      "epoch 11 | loss: 0.37014 | val_0_auc: 0.8896  |  0:00:25s\n",
      "epoch 12 | loss: 0.37278 | val_0_auc: 0.88688 |  0:00:27s\n",
      "epoch 13 | loss: 0.36553 | val_0_auc: 0.89394 |  0:00:29s\n",
      "epoch 14 | loss: 0.41347 | val_0_auc: 0.89801 |  0:00:31s\n",
      "epoch 15 | loss: 0.36429 | val_0_auc: 0.9122  |  0:00:33s\n",
      "epoch 16 | loss: 0.33511 | val_0_auc: 0.91036 |  0:00:35s\n",
      "epoch 17 | loss: 0.35266 | val_0_auc: 0.92315 |  0:00:37s\n",
      "epoch 18 | loss: 0.31819 | val_0_auc: 0.92707 |  0:00:39s\n",
      "epoch 19 | loss: 0.2904  | val_0_auc: 0.91728 |  0:00:42s\n",
      "epoch 20 | loss: 0.42847 | val_0_auc: 0.90663 |  0:00:44s\n",
      "epoch 21 | loss: 0.45336 | val_0_auc: 0.92814 |  0:00:46s\n",
      "epoch 22 | loss: 0.32785 | val_0_auc: 0.92504 |  0:00:48s\n",
      "epoch 23 | loss: 0.30983 | val_0_auc: 0.91798 |  0:00:50s\n",
      "epoch 24 | loss: 0.31836 | val_0_auc: 0.92777 |  0:00:52s\n",
      "epoch 25 | loss: 0.29197 | val_0_auc: 0.93878 |  0:00:54s\n",
      "epoch 26 | loss: 0.28875 | val_0_auc: 0.93941 |  0:00:56s\n",
      "epoch 27 | loss: 0.27993 | val_0_auc: 0.94224 |  0:00:58s\n",
      "epoch 28 | loss: 0.28708 | val_0_auc: 0.94097 |  0:01:00s\n",
      "epoch 29 | loss: 0.29528 | val_0_auc: 0.93612 |  0:01:03s\n",
      "epoch 30 | loss: 0.27567 | val_0_auc: 0.95749 |  0:01:05s\n",
      "epoch 31 | loss: 0.29476 | val_0_auc: 0.93995 |  0:01:07s\n",
      "epoch 32 | loss: 0.28764 | val_0_auc: 0.94228 |  0:01:09s\n",
      "epoch 33 | loss: 0.26463 | val_0_auc: 0.93821 |  0:01:11s\n",
      "epoch 34 | loss: 0.25095 | val_0_auc: 0.93967 |  0:01:13s\n",
      "epoch 35 | loss: 0.23568 | val_0_auc: 0.95582 |  0:01:15s\n",
      "epoch 36 | loss: 0.22324 | val_0_auc: 0.95857 |  0:01:17s\n",
      "epoch 37 | loss: 0.23565 | val_0_auc: 0.95411 |  0:01:19s\n",
      "epoch 38 | loss: 0.26144 | val_0_auc: 0.9489  |  0:01:21s\n",
      "epoch 39 | loss: 0.25468 | val_0_auc: 0.93737 |  0:01:23s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.95857\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95857294  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 84.5963\n",
      "Function value obtained: -0.9586\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8276534162583702, 'lambda_sparse': 0.007053275158795996, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.97328 | val_0_auc: 0.82505 |  0:00:01s\n",
      "epoch 1  | loss: 0.4222  | val_0_auc: 0.87533 |  0:00:02s\n",
      "epoch 2  | loss: 0.34884 | val_0_auc: 0.88973 |  0:00:03s\n",
      "epoch 3  | loss: 0.33451 | val_0_auc: 0.90194 |  0:00:05s\n",
      "epoch 4  | loss: 0.29762 | val_0_auc: 0.92587 |  0:00:06s\n",
      "epoch 5  | loss: 0.26753 | val_0_auc: 0.93892 |  0:00:07s\n",
      "epoch 6  | loss: 0.24634 | val_0_auc: 0.92092 |  0:00:09s\n",
      "epoch 7  | loss: 0.23781 | val_0_auc: 0.94335 |  0:00:10s\n",
      "epoch 8  | loss: 0.23323 | val_0_auc: 0.94574 |  0:00:11s\n",
      "epoch 9  | loss: 0.22434 | val_0_auc: 0.94009 |  0:00:12s\n",
      "epoch 10 | loss: 0.22415 | val_0_auc: 0.96003 |  0:00:14s\n",
      "epoch 11 | loss: 0.20697 | val_0_auc: 0.95885 |  0:00:15s\n",
      "epoch 12 | loss: 0.21838 | val_0_auc: 0.94971 |  0:00:16s\n",
      "epoch 13 | loss: 0.20042 | val_0_auc: 0.95541 |  0:00:18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14 | loss: 0.19526 | val_0_auc: 0.95491 |  0:00:19s\n",
      "epoch 15 | loss: 0.18373 | val_0_auc: 0.96846 |  0:00:20s\n",
      "epoch 16 | loss: 0.18127 | val_0_auc: 0.97617 |  0:00:21s\n",
      "epoch 17 | loss: 0.16883 | val_0_auc: 0.97696 |  0:00:23s\n",
      "epoch 18 | loss: 0.15785 | val_0_auc: 0.96868 |  0:00:24s\n",
      "epoch 19 | loss: 0.16755 | val_0_auc: 0.97583 |  0:00:25s\n",
      "epoch 20 | loss: 0.16488 | val_0_auc: 0.97888 |  0:00:27s\n",
      "epoch 21 | loss: 0.15885 | val_0_auc: 0.97252 |  0:00:28s\n",
      "epoch 22 | loss: 0.14817 | val_0_auc: 0.97479 |  0:00:29s\n",
      "epoch 23 | loss: 0.14221 | val_0_auc: 0.98239 |  0:00:30s\n",
      "epoch 24 | loss: 0.14168 | val_0_auc: 0.97811 |  0:00:32s\n",
      "epoch 25 | loss: 0.14822 | val_0_auc: 0.98531 |  0:00:33s\n",
      "epoch 26 | loss: 0.1417  | val_0_auc: 0.97986 |  0:00:34s\n",
      "epoch 27 | loss: 0.14897 | val_0_auc: 0.97584 |  0:00:35s\n",
      "epoch 28 | loss: 0.15382 | val_0_auc: 0.97902 |  0:00:37s\n",
      "epoch 29 | loss: 0.14894 | val_0_auc: 0.98157 |  0:00:38s\n",
      "epoch 30 | loss: 0.14517 | val_0_auc: 0.97973 |  0:00:39s\n",
      "epoch 31 | loss: 0.1448  | val_0_auc: 0.97825 |  0:00:41s\n",
      "epoch 32 | loss: 0.14006 | val_0_auc: 0.98255 |  0:00:42s\n",
      "epoch 33 | loss: 0.1336  | val_0_auc: 0.98363 |  0:00:43s\n",
      "epoch 34 | loss: 0.13124 | val_0_auc: 0.9809  |  0:00:44s\n",
      "epoch 35 | loss: 0.13055 | val_0_auc: 0.98012 |  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.98531\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98531151  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 46.6636\n",
      "Function value obtained: -0.9853\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1573024919125026, 'lambda_sparse': 0.0994283963057919, 'n_steps': 10, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.94551 | val_0_auc: 0.82095 |  0:00:01s\n",
      "epoch 1  | loss: 0.53512 | val_0_auc: 0.844   |  0:00:03s\n",
      "epoch 2  | loss: 0.45645 | val_0_auc: 0.87736 |  0:00:05s\n",
      "epoch 3  | loss: 0.43176 | val_0_auc: 0.89088 |  0:00:07s\n",
      "epoch 4  | loss: 0.38071 | val_0_auc: 0.89837 |  0:00:09s\n",
      "epoch 5  | loss: 0.37055 | val_0_auc: 0.9192  |  0:00:11s\n",
      "epoch 6  | loss: 0.33151 | val_0_auc: 0.91844 |  0:00:13s\n",
      "epoch 7  | loss: 0.31475 | val_0_auc: 0.93926 |  0:00:15s\n",
      "epoch 8  | loss: 0.27875 | val_0_auc: 0.94927 |  0:00:17s\n",
      "epoch 9  | loss: 0.27906 | val_0_auc: 0.96044 |  0:00:18s\n",
      "epoch 10 | loss: 0.28178 | val_0_auc: 0.95458 |  0:00:20s\n",
      "epoch 11 | loss: 0.25695 | val_0_auc: 0.9571  |  0:00:22s\n",
      "epoch 12 | loss: 0.27821 | val_0_auc: 0.94837 |  0:00:24s\n",
      "epoch 13 | loss: 0.27723 | val_0_auc: 0.95674 |  0:00:26s\n",
      "epoch 14 | loss: 0.24369 | val_0_auc: 0.95537 |  0:00:28s\n",
      "epoch 15 | loss: 0.22226 | val_0_auc: 0.95871 |  0:00:30s\n",
      "epoch 16 | loss: 0.22711 | val_0_auc: 0.9618  |  0:00:31s\n",
      "epoch 17 | loss: 0.23518 | val_0_auc: 0.95616 |  0:00:33s\n",
      "epoch 18 | loss: 0.23788 | val_0_auc: 0.96367 |  0:00:35s\n",
      "epoch 19 | loss: 0.24494 | val_0_auc: 0.96395 |  0:00:37s\n",
      "epoch 20 | loss: 0.23459 | val_0_auc: 0.97259 |  0:00:39s\n",
      "epoch 21 | loss: 0.22656 | val_0_auc: 0.97173 |  0:00:41s\n",
      "epoch 22 | loss: 0.21233 | val_0_auc: 0.97369 |  0:00:43s\n",
      "epoch 23 | loss: 0.21812 | val_0_auc: 0.97014 |  0:00:45s\n",
      "epoch 24 | loss: 0.23256 | val_0_auc: 0.96904 |  0:00:47s\n",
      "epoch 25 | loss: 0.21154 | val_0_auc: 0.9749  |  0:00:49s\n",
      "epoch 26 | loss: 0.22171 | val_0_auc: 0.9674  |  0:00:50s\n",
      "epoch 27 | loss: 0.20848 | val_0_auc: 0.97429 |  0:00:52s\n",
      "epoch 28 | loss: 0.19768 | val_0_auc: 0.97356 |  0:00:54s\n",
      "epoch 29 | loss: 0.21923 | val_0_auc: 0.97399 |  0:00:56s\n",
      "epoch 30 | loss: 0.20376 | val_0_auc: 0.96877 |  0:00:58s\n",
      "epoch 31 | loss: 0.19434 | val_0_auc: 0.96791 |  0:01:00s\n",
      "epoch 32 | loss: 0.19375 | val_0_auc: 0.96259 |  0:01:02s\n",
      "epoch 33 | loss: 0.19955 | val_0_auc: 0.97207 |  0:01:03s\n",
      "epoch 34 | loss: 0.19916 | val_0_auc: 0.97408 |  0:01:05s\n",
      "epoch 35 | loss: 0.20742 | val_0_auc: 0.96741 |  0:01:07s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.9749\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97490159  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 68.2712\n",
      "Function value obtained: -0.9749\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5132660681461374, 'lambda_sparse': 0.04206906926892985, 'n_steps': 8, 'n_a': 64, 'n_d': 64, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.00862 | val_0_auc: 0.86039 |  0:00:02s\n",
      "epoch 1  | loss: 0.84138 | val_0_auc: 0.83619 |  0:00:05s\n",
      "epoch 2  | loss: 0.56707 | val_0_auc: 0.88286 |  0:00:07s\n",
      "epoch 3  | loss: 0.66624 | val_0_auc: 0.86075 |  0:00:10s\n",
      "epoch 4  | loss: 0.54451 | val_0_auc: 0.8977  |  0:00:12s\n",
      "epoch 5  | loss: 0.38898 | val_0_auc: 0.90706 |  0:00:15s\n",
      "epoch 6  | loss: 0.35072 | val_0_auc: 0.88886 |  0:00:17s\n",
      "epoch 7  | loss: 0.43545 | val_0_auc: 0.91637 |  0:00:20s\n",
      "epoch 8  | loss: 0.32298 | val_0_auc: 0.91012 |  0:00:23s\n",
      "epoch 9  | loss: 0.32153 | val_0_auc: 0.88776 |  0:00:25s\n",
      "epoch 10 | loss: 0.30512 | val_0_auc: 0.92439 |  0:00:28s\n",
      "epoch 11 | loss: 0.28987 | val_0_auc: 0.91616 |  0:00:30s\n",
      "epoch 12 | loss: 0.27266 | val_0_auc: 0.9348  |  0:00:33s\n",
      "epoch 13 | loss: 0.28261 | val_0_auc: 0.92613 |  0:00:36s\n",
      "epoch 14 | loss: 0.29085 | val_0_auc: 0.92126 |  0:00:38s\n",
      "epoch 15 | loss: 0.27003 | val_0_auc: 0.939   |  0:00:41s\n",
      "epoch 16 | loss: 0.23748 | val_0_auc: 0.94648 |  0:00:43s\n",
      "epoch 17 | loss: 0.23588 | val_0_auc: 0.94187 |  0:00:46s\n",
      "epoch 18 | loss: 0.22718 | val_0_auc: 0.93729 |  0:00:48s\n",
      "epoch 19 | loss: 0.23916 | val_0_auc: 0.94881 |  0:00:51s\n",
      "epoch 20 | loss: 0.2381  | val_0_auc: 0.9533  |  0:00:54s\n",
      "epoch 21 | loss: 0.22211 | val_0_auc: 0.9558  |  0:00:56s\n",
      "epoch 22 | loss: 0.22673 | val_0_auc: 0.95859 |  0:00:59s\n",
      "epoch 23 | loss: 0.21188 | val_0_auc: 0.95017 |  0:01:01s\n",
      "epoch 24 | loss: 0.21132 | val_0_auc: 0.95304 |  0:01:04s\n",
      "epoch 25 | loss: 0.24237 | val_0_auc: 0.95527 |  0:01:06s\n",
      "epoch 26 | loss: 0.24242 | val_0_auc: 0.95198 |  0:01:09s\n",
      "epoch 27 | loss: 0.23408 | val_0_auc: 0.95007 |  0:01:11s\n",
      "epoch 28 | loss: 0.235   | val_0_auc: 0.94582 |  0:01:14s\n",
      "epoch 29 | loss: 0.20022 | val_0_auc: 0.95214 |  0:01:16s\n",
      "epoch 30 | loss: 0.20623 | val_0_auc: 0.95721 |  0:01:19s\n",
      "epoch 31 | loss: 0.22804 | val_0_auc: 0.94726 |  0:01:21s\n",
      "epoch 32 | loss: 0.22296 | val_0_auc: 0.94478 |  0:01:24s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.95859\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95859117  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 85.2985\n",
      "Function value obtained: -0.9586\n",
      "Current minimum: -0.9937\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.68760697678713, 'lambda_sparse': 0.053676304384930394, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.69181 | val_0_auc: 0.80996 |  0:00:00s\n",
      "epoch 1  | loss: 0.40706 | val_0_auc: 0.89814 |  0:00:01s\n",
      "epoch 2  | loss: 0.31591 | val_0_auc: 0.9331  |  0:00:02s\n",
      "epoch 3  | loss: 0.26324 | val_0_auc: 0.94044 |  0:00:03s\n",
      "epoch 4  | loss: 0.22692 | val_0_auc: 0.9503  |  0:00:04s\n",
      "epoch 5  | loss: 0.21305 | val_0_auc: 0.9517  |  0:00:05s\n",
      "epoch 6  | loss: 0.206   | val_0_auc: 0.95851 |  0:00:06s\n",
      "epoch 7  | loss: 0.19204 | val_0_auc: 0.96025 |  0:00:07s\n",
      "epoch 8  | loss: 0.18869 | val_0_auc: 0.96188 |  0:00:07s\n",
      "epoch 9  | loss: 0.18359 | val_0_auc: 0.96551 |  0:00:08s\n",
      "epoch 10 | loss: 0.17542 | val_0_auc: 0.96944 |  0:00:09s\n",
      "epoch 11 | loss: 0.16786 | val_0_auc: 0.97203 |  0:00:10s\n",
      "epoch 12 | loss: 0.15398 | val_0_auc: 0.97749 |  0:00:11s\n",
      "epoch 13 | loss: 0.15266 | val_0_auc: 0.97681 |  0:00:12s\n",
      "epoch 14 | loss: 0.13958 | val_0_auc: 0.97888 |  0:00:13s\n",
      "epoch 15 | loss: 0.13472 | val_0_auc: 0.98336 |  0:00:14s\n",
      "epoch 16 | loss: 0.13758 | val_0_auc: 0.97442 |  0:00:15s\n",
      "epoch 17 | loss: 0.12861 | val_0_auc: 0.98131 |  0:00:16s\n",
      "epoch 18 | loss: 0.12688 | val_0_auc: 0.98403 |  0:00:16s\n",
      "epoch 19 | loss: 0.12273 | val_0_auc: 0.98327 |  0:00:17s\n",
      "epoch 20 | loss: 0.12228 | val_0_auc: 0.98272 |  0:00:18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 0.12793 | val_0_auc: 0.98439 |  0:00:19s\n",
      "epoch 22 | loss: 0.12215 | val_0_auc: 0.98173 |  0:00:20s\n",
      "epoch 23 | loss: 0.12957 | val_0_auc: 0.98395 |  0:00:21s\n",
      "epoch 24 | loss: 0.12337 | val_0_auc: 0.98373 |  0:00:22s\n",
      "epoch 25 | loss: 0.11434 | val_0_auc: 0.98463 |  0:00:23s\n",
      "epoch 26 | loss: 0.11412 | val_0_auc: 0.98536 |  0:00:23s\n",
      "epoch 27 | loss: 0.11247 | val_0_auc: 0.98757 |  0:00:24s\n",
      "epoch 28 | loss: 0.11408 | val_0_auc: 0.98125 |  0:00:25s\n",
      "epoch 29 | loss: 0.10735 | val_0_auc: 0.9844  |  0:00:26s\n",
      "epoch 30 | loss: 0.1062  | val_0_auc: 0.98818 |  0:00:27s\n",
      "epoch 31 | loss: 0.09864 | val_0_auc: 0.99072 |  0:00:28s\n",
      "epoch 32 | loss: 0.09238 | val_0_auc: 0.99109 |  0:00:29s\n",
      "epoch 33 | loss: 0.09005 | val_0_auc: 0.99024 |  0:00:30s\n",
      "epoch 34 | loss: 0.09719 | val_0_auc: 0.99049 |  0:00:30s\n",
      "epoch 35 | loss: 0.09667 | val_0_auc: 0.99084 |  0:00:31s\n",
      "epoch 36 | loss: 0.09259 | val_0_auc: 0.98998 |  0:00:32s\n",
      "epoch 37 | loss: 0.09296 | val_0_auc: 0.99077 |  0:00:33s\n",
      "epoch 38 | loss: 0.10042 | val_0_auc: 0.99162 |  0:00:34s\n",
      "epoch 39 | loss: 0.09195 | val_0_auc: 0.99138 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99162\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99161699  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 38.3068\n",
      "Function value obtained: -0.9916\n",
      "Current minimum: -0.9937\n",
      "STARTED: syn5\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.36091255908231745, 'max_depth': 12, 'n_estimators': 655}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99617682  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.0812\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16110897049226142, 'max_depth': 7, 'n_estimators': 579}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99677108  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.1121\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9968\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.177952268286884, 'max_depth': 13, 'n_estimators': 167}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99667342  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.1246\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9968\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4411130259567979, 'max_depth': 13, 'n_estimators': 772}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99574464  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.0482\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9968\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10568187137310191, 'max_depth': 4, 'n_estimators': 270}\n",
      "AUC:  0.99687705  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.1056\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.11815560175290783, 'max_depth': 7, 'n_estimators': 305}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99677731  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.1510\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4549856419010597, 'max_depth': 4, 'n_estimators': 823}\n",
      "AUC:  0.99612488  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.0612\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2856621971716971, 'max_depth': 11, 'n_estimators': 923}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99671082  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.0851\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4841012739606382, 'max_depth': 12, 'n_estimators': 657}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99588385  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.0485\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48344934643274295, 'max_depth': 11, 'n_estimators': 398}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99616851  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.0616\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43135166285230964, 'max_depth': 5, 'n_estimators': 884}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99633681  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0547\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2585670530752063, 'max_depth': 11, 'n_estimators': 851}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99638045  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.0742\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31028638362009175, 'max_depth': 10, 'n_estimators': 956}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99672536  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.1122\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.34778044320046275, 'max_depth': 7, 'n_estimators': 309}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99656849  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0594\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2604012299988373, 'max_depth': 13, 'n_estimators': 236}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99584437  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0506\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4690126768120838, 'max_depth': 9, 'n_estimators': 953}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99631188  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.0651\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18528397867070745, 'max_depth': 9, 'n_estimators': 636}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99655498  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.1059\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3364045364349866, 'max_depth': 5, 'n_estimators': 438}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99673991  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.0708\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3303306138640519, 'max_depth': 13, 'n_estimators': 932}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99668796  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.0758\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4862695680139867, 'max_depth': 10, 'n_estimators': 564}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99657368  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.1019\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2252279324498739, 'max_depth': 11, 'n_estimators': 258}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9968064  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.0923\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48177332593272815, 'max_depth': 14, 'n_estimators': 869}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99609994  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.0594\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19202819357345072, 'max_depth': 3, 'n_estimators': 481}\n",
      "AUC:  0.99689886  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.0758\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3755774638394842, 'max_depth': 9, 'n_estimators': 246}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99638045  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0746\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2499202520316368, 'max_depth': 9, 'n_estimators': 269}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99675653  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.1114\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14244374976727375, 'max_depth': 5, 'n_estimators': 198}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99691029  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.1018\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4161499940820278, 'max_depth': 4, 'n_estimators': 873}\n",
      "AUC:  0.99657576  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.0750\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4850827162973014, 'max_depth': 12, 'n_estimators': 358}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99590255  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.0455\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48395304854170385, 'max_depth': 14, 'n_estimators': 421}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99623292  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.0675\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.40703935876086883, 'max_depth': 13, 'n_estimators': 731}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99650512  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.9641\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4611689810104832, 'max_depth': 10, 'n_estimators': 400}\n",
      "[03:00:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99616539  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.1427\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9962\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18722090652185458, 'max_depth': 7, 'n_estimators': 452}\n",
      "[03:00:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99715963  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.2950\n",
      "Function value obtained: -0.9972\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.63228191658084, 'max_depth': 9, 'n_estimators': 584}\n",
      "[03:00:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99630149  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.1567\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19523616792188164, 'max_depth': 7, 'n_estimators': 972}\n",
      "[03:00:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99697055  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.3161\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49983394912099954, 'max_depth': 13, 'n_estimators': 315}\n",
      "[03:00:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99605423  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.3291\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.601073431621893, 'max_depth': 7, 'n_estimators': 343}\n",
      "[03:00:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99597527  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.1425\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5131100014113071, 'max_depth': 10, 'n_estimators': 428}\n",
      "[03:00:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9962537  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.2424\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18001831781344202, 'max_depth': 8, 'n_estimators': 613}\n",
      "[03:00:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99698094  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.2517\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.26315550970859636, 'max_depth': 11, 'n_estimators': 911}\n",
      "[03:00:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9960293  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.3351\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9078131768221395, 'max_depth': 15, 'n_estimators': 330}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99554101  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.2240\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31578324223104964, 'max_depth': 7, 'n_estimators': 548}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99698509  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.1892\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5671859899619481, 'max_depth': 5, 'n_estimators': 359}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.996422  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1292\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.437344281810245, 'max_depth': 5, 'n_estimators': 920}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99725832  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.1299\n",
      "Function value obtained: -0.9973\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7455588566487656, 'max_depth': 8, 'n_estimators': 223}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99555763  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.1194\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.808129965337389, 'max_depth': 11, 'n_estimators': 997}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99509636  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.1400\n",
      "Function value obtained: -0.9951\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9605183354733343, 'max_depth': 14, 'n_estimators': 524}\n",
      "[03:00:24] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99497169  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.1555\n",
      "Function value obtained: -0.9950\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.18527312183871544, 'max_depth': 7, 'n_estimators': 613}\n",
      "[03:00:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99714924  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.3904\n",
      "Function value obtained: -0.9971\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.307141819278016, 'max_depth': 11, 'n_estimators': 348}\n",
      "[03:00:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99566984  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.2120\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8365769931032287, 'max_depth': 5, 'n_estimators': 441}\n",
      "[03:00:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99632746  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.0909\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.12140332800904427, 'max_depth': 13, 'n_estimators': 890}\n",
      "[03:00:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99507142  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.2343\n",
      "Function value obtained: -0.9951\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3230520165833749, 'max_depth': 11, 'n_estimators': 746}\n",
      "[03:00:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99668589  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.3531\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1453386543204326, 'max_depth': 7, 'n_estimators': 264}\n",
      "[03:00:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99698717  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.2853\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.45843417286748234, 'max_depth': 9, 'n_estimators': 461}\n",
      "[03:00:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99614358  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.1610\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2524066323031939, 'max_depth': 9, 'n_estimators': 879}\n",
      "[03:00:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99630669  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.2072\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31701216836173224, 'max_depth': 10, 'n_estimators': 583}\n",
      "[03:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99673368  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.2777\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6000391640425576, 'max_depth': 12, 'n_estimators': 586}\n",
      "[03:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99592125  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.1838\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.21724408724240923, 'max_depth': 8, 'n_estimators': 250}\n",
      "[03:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99678978  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.2292\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8844438344881459, 'max_depth': 12, 'n_estimators': 870}\n",
      "[03:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99585892  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.1496\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7960948331528556, 'max_depth': 4, 'n_estimators': 188}\n",
      "[03:00:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99668796  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.1060\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16967361278386955, 'max_depth': 8, 'n_estimators': 252}\n",
      "[03:00:28] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99676277  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.6900\n",
      "Function value obtained: -0.9968\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4077724819198958, 'lambda_sparse': 0.018624824521328607, 'n_steps': 3, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.7759  | val_0_auc: 0.84645 |  0:00:01s\n",
      "epoch 1  | loss: 0.35878 | val_0_auc: 0.91623 |  0:00:02s\n",
      "epoch 2  | loss: 0.29874 | val_0_auc: 0.9232  |  0:00:03s\n",
      "epoch 3  | loss: 0.26362 | val_0_auc: 0.93592 |  0:00:04s\n",
      "epoch 4  | loss: 0.25012 | val_0_auc: 0.94487 |  0:00:05s\n",
      "epoch 5  | loss: 0.22698 | val_0_auc: 0.95508 |  0:00:06s\n",
      "epoch 6  | loss: 0.21621 | val_0_auc: 0.95835 |  0:00:08s\n",
      "epoch 7  | loss: 0.19996 | val_0_auc: 0.96539 |  0:00:09s\n",
      "epoch 8  | loss: 0.19673 | val_0_auc: 0.96959 |  0:00:10s\n",
      "epoch 9  | loss: 0.18855 | val_0_auc: 0.97113 |  0:00:11s\n",
      "epoch 10 | loss: 0.17885 | val_0_auc: 0.9736  |  0:00:12s\n",
      "epoch 11 | loss: 0.17139 | val_0_auc: 0.97843 |  0:00:13s\n",
      "epoch 12 | loss: 0.16741 | val_0_auc: 0.9822  |  0:00:15s\n",
      "epoch 13 | loss: 0.15039 | val_0_auc: 0.98256 |  0:00:16s\n",
      "epoch 14 | loss: 0.14488 | val_0_auc: 0.98547 |  0:00:17s\n",
      "epoch 15 | loss: 0.14181 | val_0_auc: 0.9843  |  0:00:18s\n",
      "epoch 16 | loss: 0.15317 | val_0_auc: 0.9891  |  0:00:19s\n",
      "epoch 17 | loss: 0.14286 | val_0_auc: 0.99031 |  0:00:20s\n",
      "epoch 18 | loss: 0.1346  | val_0_auc: 0.98252 |  0:00:21s\n",
      "epoch 19 | loss: 0.13187 | val_0_auc: 0.98999 |  0:00:23s\n",
      "epoch 20 | loss: 0.12355 | val_0_auc: 0.98692 |  0:00:24s\n",
      "epoch 21 | loss: 0.12547 | val_0_auc: 0.99227 |  0:00:25s\n",
      "epoch 22 | loss: 0.12291 | val_0_auc: 0.98853 |  0:00:26s\n",
      "epoch 23 | loss: 0.11487 | val_0_auc: 0.99472 |  0:00:27s\n",
      "epoch 24 | loss: 0.11393 | val_0_auc: 0.99028 |  0:00:28s\n",
      "epoch 25 | loss: 0.10745 | val_0_auc: 0.98931 |  0:00:30s\n",
      "epoch 26 | loss: 0.10181 | val_0_auc: 0.99021 |  0:00:31s\n",
      "epoch 27 | loss: 0.10023 | val_0_auc: 0.99257 |  0:00:32s\n",
      "epoch 28 | loss: 0.09604 | val_0_auc: 0.99412 |  0:00:33s\n",
      "epoch 29 | loss: 0.10497 | val_0_auc: 0.99385 |  0:00:34s\n",
      "epoch 30 | loss: 0.1104  | val_0_auc: 0.99496 |  0:00:35s\n",
      "epoch 31 | loss: 0.11271 | val_0_auc: 0.99187 |  0:00:36s\n",
      "epoch 32 | loss: 0.09701 | val_0_auc: 0.99305 |  0:00:38s\n",
      "epoch 33 | loss: 0.10254 | val_0_auc: 0.99635 |  0:00:39s\n",
      "epoch 34 | loss: 0.09071 | val_0_auc: 0.99517 |  0:00:40s\n",
      "epoch 35 | loss: 0.09596 | val_0_auc: 0.99367 |  0:00:41s\n",
      "epoch 36 | loss: 0.09878 | val_0_auc: 0.99499 |  0:00:42s\n",
      "epoch 37 | loss: 0.09836 | val_0_auc: 0.99541 |  0:00:43s\n",
      "epoch 38 | loss: 0.10043 | val_0_auc: 0.99216 |  0:00:44s\n",
      "epoch 39 | loss: 0.0947  | val_0_auc: 0.99233 |  0:00:46s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.99635\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9963472  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 46.5291\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1231764447063715, 'lambda_sparse': 0.01678480754810585, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.86753 | val_0_auc: 0.8621  |  0:00:01s\n",
      "epoch 1  | loss: 0.43531 | val_0_auc: 0.91165 |  0:00:02s\n",
      "epoch 2  | loss: 0.32993 | val_0_auc: 0.92221 |  0:00:04s\n",
      "epoch 3  | loss: 0.28313 | val_0_auc: 0.94589 |  0:00:05s\n",
      "epoch 4  | loss: 0.26082 | val_0_auc: 0.95908 |  0:00:07s\n",
      "epoch 5  | loss: 0.22335 | val_0_auc: 0.97046 |  0:00:08s\n",
      "epoch 6  | loss: 0.20971 | val_0_auc: 0.96584 |  0:00:10s\n",
      "epoch 7  | loss: 0.20322 | val_0_auc: 0.96068 |  0:00:11s\n",
      "epoch 8  | loss: 0.19024 | val_0_auc: 0.97543 |  0:00:13s\n",
      "epoch 9  | loss: 0.18566 | val_0_auc: 0.97875 |  0:00:15s\n",
      "epoch 10 | loss: 0.18191 | val_0_auc: 0.98121 |  0:00:16s\n",
      "epoch 11 | loss: 0.1642  | val_0_auc: 0.986   |  0:00:18s\n",
      "epoch 12 | loss: 0.15783 | val_0_auc: 0.98458 |  0:00:19s\n",
      "epoch 13 | loss: 0.16226 | val_0_auc: 0.98541 |  0:00:20s\n",
      "epoch 14 | loss: 0.14642 | val_0_auc: 0.9865  |  0:00:22s\n",
      "epoch 15 | loss: 0.13972 | val_0_auc: 0.98697 |  0:00:23s\n",
      "epoch 16 | loss: 0.14053 | val_0_auc: 0.98875 |  0:00:25s\n",
      "epoch 17 | loss: 0.15063 | val_0_auc: 0.98817 |  0:00:26s\n",
      "epoch 18 | loss: 0.14014 | val_0_auc: 0.99172 |  0:00:28s\n",
      "epoch 19 | loss: 0.13022 | val_0_auc: 0.98747 |  0:00:29s\n",
      "epoch 20 | loss: 0.12388 | val_0_auc: 0.99041 |  0:00:31s\n",
      "epoch 21 | loss: 0.12149 | val_0_auc: 0.9889  |  0:00:32s\n",
      "epoch 22 | loss: 0.11596 | val_0_auc: 0.99029 |  0:00:34s\n",
      "epoch 23 | loss: 0.11312 | val_0_auc: 0.98722 |  0:00:35s\n",
      "epoch 24 | loss: 0.10987 | val_0_auc: 0.99019 |  0:00:37s\n",
      "epoch 25 | loss: 0.11815 | val_0_auc: 0.99334 |  0:00:38s\n",
      "epoch 26 | loss: 0.12687 | val_0_auc: 0.99179 |  0:00:40s\n",
      "epoch 27 | loss: 0.12086 | val_0_auc: 0.99069 |  0:00:41s\n",
      "epoch 28 | loss: 0.11278 | val_0_auc: 0.9905  |  0:00:43s\n",
      "epoch 29 | loss: 0.10833 | val_0_auc: 0.99326 |  0:00:44s\n",
      "epoch 30 | loss: 0.11063 | val_0_auc: 0.99011 |  0:00:46s\n",
      "epoch 31 | loss: 0.10998 | val_0_auc: 0.99321 |  0:00:47s\n",
      "epoch 32 | loss: 0.10447 | val_0_auc: 0.98896 |  0:00:49s\n",
      "epoch 33 | loss: 0.11269 | val_0_auc: 0.98792 |  0:00:50s\n",
      "epoch 34 | loss: 0.11324 | val_0_auc: 0.99123 |  0:00:52s\n",
      "epoch 35 | loss: 0.11238 | val_0_auc: 0.98755 |  0:00:53s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.99334\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99334061  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 54.0607\n",
      "Function value obtained: -0.9933\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9390831161992033, 'lambda_sparse': 0.004565549761185418, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.9513  | val_0_auc: 0.75812 |  0:00:01s\n",
      "epoch 1  | loss: 0.57093 | val_0_auc: 0.7811  |  0:00:03s\n",
      "epoch 2  | loss: 0.56718 | val_0_auc: 0.81315 |  0:00:04s\n",
      "epoch 3  | loss: 0.48903 | val_0_auc: 0.82349 |  0:00:06s\n",
      "epoch 4  | loss: 0.45612 | val_0_auc: 0.86122 |  0:00:07s\n",
      "epoch 5  | loss: 0.43362 | val_0_auc: 0.88027 |  0:00:09s\n",
      "epoch 6  | loss: 0.39845 | val_0_auc: 0.88946 |  0:00:10s\n",
      "epoch 7  | loss: 0.3687  | val_0_auc: 0.89527 |  0:00:12s\n",
      "epoch 8  | loss: 0.35583 | val_0_auc: 0.8951  |  0:00:14s\n",
      "epoch 9  | loss: 0.32763 | val_0_auc: 0.91568 |  0:00:15s\n",
      "epoch 10 | loss: 0.29909 | val_0_auc: 0.92057 |  0:00:17s\n",
      "epoch 11 | loss: 0.30253 | val_0_auc: 0.90393 |  0:00:18s\n",
      "epoch 12 | loss: 0.29795 | val_0_auc: 0.92695 |  0:00:20s\n",
      "epoch 13 | loss: 0.30614 | val_0_auc: 0.9275  |  0:00:21s\n",
      "epoch 14 | loss: 0.31452 | val_0_auc: 0.93773 |  0:00:23s\n",
      "epoch 15 | loss: 0.30878 | val_0_auc: 0.93774 |  0:00:24s\n",
      "epoch 16 | loss: 0.29746 | val_0_auc: 0.93669 |  0:00:26s\n",
      "epoch 17 | loss: 0.31188 | val_0_auc: 0.91871 |  0:00:28s\n",
      "epoch 18 | loss: 0.30877 | val_0_auc: 0.9222  |  0:00:29s\n",
      "epoch 19 | loss: 0.27685 | val_0_auc: 0.9415  |  0:00:31s\n",
      "epoch 20 | loss: 0.27205 | val_0_auc: 0.93851 |  0:00:32s\n",
      "epoch 21 | loss: 0.26395 | val_0_auc: 0.93713 |  0:00:34s\n",
      "epoch 22 | loss: 0.25414 | val_0_auc: 0.9483  |  0:00:35s\n",
      "epoch 23 | loss: 0.26858 | val_0_auc: 0.94704 |  0:00:37s\n",
      "epoch 24 | loss: 0.24653 | val_0_auc: 0.94886 |  0:00:38s\n",
      "epoch 25 | loss: 0.23988 | val_0_auc: 0.96126 |  0:00:40s\n",
      "epoch 26 | loss: 0.22961 | val_0_auc: 0.96241 |  0:00:42s\n",
      "epoch 27 | loss: 0.22155 | val_0_auc: 0.95314 |  0:00:43s\n",
      "epoch 28 | loss: 0.23973 | val_0_auc: 0.95957 |  0:00:45s\n",
      "epoch 29 | loss: 0.23001 | val_0_auc: 0.94718 |  0:00:46s\n",
      "epoch 30 | loss: 0.21662 | val_0_auc: 0.96094 |  0:00:48s\n",
      "epoch 31 | loss: 0.20449 | val_0_auc: 0.97052 |  0:00:49s\n",
      "epoch 32 | loss: 0.21164 | val_0_auc: 0.95009 |  0:00:51s\n",
      "epoch 33 | loss: 0.21343 | val_0_auc: 0.94733 |  0:00:52s\n",
      "epoch 34 | loss: 0.22428 | val_0_auc: 0.94905 |  0:00:54s\n",
      "epoch 35 | loss: 0.2283  | val_0_auc: 0.95473 |  0:00:56s\n",
      "epoch 36 | loss: 0.21814 | val_0_auc: 0.96709 |  0:00:57s\n",
      "epoch 37 | loss: 0.21551 | val_0_auc: 0.97541 |  0:00:59s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 38 | loss: 0.21437 | val_0_auc: 0.97092 |  0:01:00s\n",
      "epoch 39 | loss: 0.20947 | val_0_auc: 0.96596 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.97541\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97540907  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 62.6329\n",
      "Function value obtained: -0.9754\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9642063974743174, 'lambda_sparse': 0.03612164153707591, 'n_steps': 8, 'n_a': 24, 'n_d': 24, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.62382 | val_0_auc: 0.74337 |  0:00:01s\n",
      "epoch 1  | loss: 0.62247 | val_0_auc: 0.82813 |  0:00:03s\n",
      "epoch 2  | loss: 0.47703 | val_0_auc: 0.87085 |  0:00:05s\n",
      "epoch 3  | loss: 0.457   | val_0_auc: 0.88226 |  0:00:06s\n",
      "epoch 4  | loss: 0.52381 | val_0_auc: 0.86474 |  0:00:08s\n",
      "epoch 5  | loss: 0.51157 | val_0_auc: 0.90375 |  0:00:10s\n",
      "epoch 6  | loss: 0.43501 | val_0_auc: 0.8795  |  0:00:12s\n",
      "epoch 7  | loss: 0.52487 | val_0_auc: 0.89117 |  0:00:14s\n",
      "epoch 8  | loss: 0.48205 | val_0_auc: 0.90231 |  0:00:15s\n",
      "epoch 9  | loss: 0.39387 | val_0_auc: 0.92074 |  0:00:17s\n",
      "epoch 10 | loss: 0.34622 | val_0_auc: 0.92095 |  0:00:19s\n",
      "epoch 11 | loss: 0.31789 | val_0_auc: 0.93318 |  0:00:21s\n",
      "epoch 12 | loss: 0.30161 | val_0_auc: 0.94164 |  0:00:22s\n",
      "epoch 13 | loss: 0.28009 | val_0_auc: 0.95106 |  0:00:24s\n",
      "epoch 14 | loss: 0.28778 | val_0_auc: 0.94135 |  0:00:26s\n",
      "epoch 15 | loss: 0.26853 | val_0_auc: 0.95859 |  0:00:27s\n",
      "epoch 16 | loss: 0.25475 | val_0_auc: 0.96189 |  0:00:29s\n",
      "epoch 17 | loss: 0.25348 | val_0_auc: 0.96103 |  0:00:31s\n",
      "epoch 18 | loss: 0.25389 | val_0_auc: 0.95863 |  0:00:33s\n",
      "epoch 19 | loss: 0.25402 | val_0_auc: 0.95448 |  0:00:34s\n",
      "epoch 20 | loss: 0.24691 | val_0_auc: 0.95832 |  0:00:36s\n",
      "epoch 21 | loss: 0.23574 | val_0_auc: 0.96591 |  0:00:38s\n",
      "epoch 22 | loss: 0.21082 | val_0_auc: 0.96588 |  0:00:39s\n",
      "epoch 23 | loss: 0.20664 | val_0_auc: 0.97206 |  0:00:41s\n",
      "epoch 24 | loss: 0.20397 | val_0_auc: 0.9747  |  0:00:43s\n",
      "epoch 25 | loss: 0.19106 | val_0_auc: 0.97959 |  0:00:45s\n",
      "epoch 26 | loss: 0.18843 | val_0_auc: 0.97827 |  0:00:46s\n",
      "epoch 27 | loss: 0.1835  | val_0_auc: 0.97701 |  0:00:48s\n",
      "epoch 28 | loss: 0.19075 | val_0_auc: 0.98315 |  0:00:50s\n",
      "epoch 29 | loss: 0.20505 | val_0_auc: 0.97946 |  0:00:52s\n",
      "epoch 30 | loss: 0.19978 | val_0_auc: 0.97681 |  0:00:53s\n",
      "epoch 31 | loss: 0.21663 | val_0_auc: 0.97331 |  0:00:55s\n",
      "epoch 32 | loss: 0.21886 | val_0_auc: 0.95849 |  0:00:57s\n",
      "epoch 33 | loss: 0.20865 | val_0_auc: 0.97124 |  0:00:58s\n",
      "epoch 34 | loss: 0.18928 | val_0_auc: 0.97847 |  0:01:00s\n",
      "epoch 35 | loss: 0.18218 | val_0_auc: 0.97729 |  0:01:02s\n",
      "epoch 36 | loss: 0.17481 | val_0_auc: 0.97813 |  0:01:03s\n",
      "epoch 37 | loss: 0.17308 | val_0_auc: 0.97016 |  0:01:05s\n",
      "epoch 38 | loss: 0.17702 | val_0_auc: 0.97439 |  0:01:07s\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.98315\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98314685  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 68.0276\n",
      "Function value obtained: -0.9831\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8801880248568608, 'lambda_sparse': 0.09537094160479029, 'n_steps': 10, 'n_a': 32, 'n_d': 32, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.99868 | val_0_auc: 0.82721 |  0:00:02s\n",
      "epoch 1  | loss: 0.73275 | val_0_auc: 0.86144 |  0:00:04s\n",
      "epoch 2  | loss: 0.62276 | val_0_auc: 0.85876 |  0:00:07s\n",
      "epoch 3  | loss: 0.59427 | val_0_auc: 0.904   |  0:00:09s\n",
      "epoch 4  | loss: 0.48059 | val_0_auc: 0.88679 |  0:00:11s\n",
      "epoch 5  | loss: 0.5367  | val_0_auc: 0.85417 |  0:00:13s\n",
      "epoch 6  | loss: 0.46542 | val_0_auc: 0.89875 |  0:00:16s\n",
      "epoch 7  | loss: 0.39681 | val_0_auc: 0.90226 |  0:00:18s\n",
      "epoch 8  | loss: 0.36315 | val_0_auc: 0.89663 |  0:00:20s\n",
      "epoch 9  | loss: 0.40024 | val_0_auc: 0.89839 |  0:00:23s\n",
      "epoch 10 | loss: 0.38873 | val_0_auc: 0.91198 |  0:00:25s\n",
      "epoch 11 | loss: 0.33456 | val_0_auc: 0.93746 |  0:00:27s\n",
      "epoch 12 | loss: 0.34523 | val_0_auc: 0.8995  |  0:00:29s\n",
      "epoch 13 | loss: 0.35897 | val_0_auc: 0.92953 |  0:00:32s\n",
      "epoch 14 | loss: 0.43798 | val_0_auc: 0.91483 |  0:00:34s\n",
      "epoch 15 | loss: 0.42244 | val_0_auc: 0.92277 |  0:00:36s\n",
      "epoch 16 | loss: 0.50131 | val_0_auc: 0.91511 |  0:00:39s\n",
      "epoch 17 | loss: 0.38178 | val_0_auc: 0.91103 |  0:00:41s\n",
      "epoch 18 | loss: 0.38159 | val_0_auc: 0.93472 |  0:00:43s\n",
      "epoch 19 | loss: 0.3131  | val_0_auc: 0.92872 |  0:00:45s\n",
      "epoch 20 | loss: 0.30256 | val_0_auc: 0.92719 |  0:00:48s\n",
      "epoch 21 | loss: 0.31913 | val_0_auc: 0.92708 |  0:00:50s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.93746\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.93745987  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 51.4112\n",
      "Function value obtained: -0.9375\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0355786346761235, 'lambda_sparse': 0.05407735765700069, 'n_steps': 10, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.35211 | val_0_auc: 0.85834 |  0:00:02s\n",
      "epoch 1  | loss: 0.5856  | val_0_auc: 0.87299 |  0:00:04s\n",
      "epoch 2  | loss: 0.41958 | val_0_auc: 0.92235 |  0:00:06s\n",
      "epoch 3  | loss: 0.37409 | val_0_auc: 0.95128 |  0:00:08s\n",
      "epoch 4  | loss: 0.30972 | val_0_auc: 0.96714 |  0:00:10s\n",
      "epoch 5  | loss: 0.27592 | val_0_auc: 0.96621 |  0:00:13s\n",
      "epoch 6  | loss: 0.24356 | val_0_auc: 0.97922 |  0:00:15s\n",
      "epoch 7  | loss: 0.23506 | val_0_auc: 0.98094 |  0:00:17s\n",
      "epoch 8  | loss: 0.20919 | val_0_auc: 0.97786 |  0:00:20s\n",
      "epoch 9  | loss: 0.21707 | val_0_auc: 0.97829 |  0:00:22s\n",
      "epoch 10 | loss: 0.20176 | val_0_auc: 0.97466 |  0:00:24s\n",
      "epoch 11 | loss: 0.19909 | val_0_auc: 0.98339 |  0:00:26s\n",
      "epoch 12 | loss: 0.20871 | val_0_auc: 0.98709 |  0:00:28s\n",
      "epoch 13 | loss: 0.20194 | val_0_auc: 0.9883  |  0:00:30s\n",
      "epoch 14 | loss: 0.17881 | val_0_auc: 0.9878  |  0:00:32s\n",
      "epoch 15 | loss: 0.18086 | val_0_auc: 0.98667 |  0:00:35s\n",
      "epoch 16 | loss: 0.15648 | val_0_auc: 0.98905 |  0:00:37s\n",
      "epoch 17 | loss: 0.15505 | val_0_auc: 0.98915 |  0:00:39s\n",
      "epoch 18 | loss: 0.15627 | val_0_auc: 0.99088 |  0:00:41s\n",
      "epoch 19 | loss: 0.15705 | val_0_auc: 0.9899  |  0:00:43s\n",
      "epoch 20 | loss: 0.15562 | val_0_auc: 0.98991 |  0:00:45s\n",
      "epoch 21 | loss: 0.1557  | val_0_auc: 0.99125 |  0:00:47s\n",
      "epoch 22 | loss: 0.15196 | val_0_auc: 0.99165 |  0:00:49s\n",
      "epoch 23 | loss: 0.15048 | val_0_auc: 0.99286 |  0:00:51s\n",
      "epoch 24 | loss: 0.14629 | val_0_auc: 0.99353 |  0:00:53s\n",
      "epoch 25 | loss: 0.14851 | val_0_auc: 0.993   |  0:00:56s\n",
      "epoch 26 | loss: 0.13947 | val_0_auc: 0.99076 |  0:00:58s\n",
      "epoch 27 | loss: 0.15588 | val_0_auc: 0.99182 |  0:01:00s\n",
      "epoch 28 | loss: 0.13986 | val_0_auc: 0.99187 |  0:01:02s\n",
      "epoch 29 | loss: 0.15653 | val_0_auc: 0.99157 |  0:01:04s\n",
      "epoch 30 | loss: 0.13787 | val_0_auc: 0.99163 |  0:01:06s\n",
      "epoch 31 | loss: 0.13363 | val_0_auc: 0.99452 |  0:01:08s\n",
      "epoch 32 | loss: 0.14195 | val_0_auc: 0.99299 |  0:01:10s\n",
      "epoch 33 | loss: 0.15602 | val_0_auc: 0.99522 |  0:01:12s\n",
      "epoch 34 | loss: 0.14133 | val_0_auc: 0.99397 |  0:01:14s\n",
      "epoch 35 | loss: 0.14314 | val_0_auc: 0.99357 |  0:01:17s\n",
      "epoch 36 | loss: 0.12858 | val_0_auc: 0.99395 |  0:01:19s\n",
      "epoch 37 | loss: 0.13607 | val_0_auc: 0.99295 |  0:01:21s\n",
      "epoch 38 | loss: 0.12391 | val_0_auc: 0.99508 |  0:01:23s\n",
      "epoch 39 | loss: 0.12382 | val_0_auc: 0.9931  |  0:01:25s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.99522\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99521895  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 86.1029\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7195386204919783, 'lambda_sparse': 0.09795429972105785, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.6}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.87066 | val_0_auc: 0.81274 |  0:00:01s\n",
      "epoch 1  | loss: 0.52625 | val_0_auc: 0.85698 |  0:00:03s\n",
      "epoch 2  | loss: 0.43456 | val_0_auc: 0.89    |  0:00:05s\n",
      "epoch 3  | loss: 0.40896 | val_0_auc: 0.88719 |  0:00:07s\n",
      "epoch 4  | loss: 0.4353  | val_0_auc: 0.88507 |  0:00:09s\n",
      "epoch 5  | loss: 0.41528 | val_0_auc: 0.90546 |  0:00:11s\n",
      "epoch 6  | loss: 0.34945 | val_0_auc: 0.93026 |  0:00:13s\n",
      "epoch 7  | loss: 0.35007 | val_0_auc: 0.92065 |  0:00:15s\n",
      "epoch 8  | loss: 0.42249 | val_0_auc: 0.92767 |  0:00:17s\n",
      "epoch 9  | loss: 0.33449 | val_0_auc: 0.92135 |  0:00:19s\n",
      "epoch 10 | loss: 0.31944 | val_0_auc: 0.93342 |  0:00:21s\n",
      "epoch 11 | loss: 0.30671 | val_0_auc: 0.92923 |  0:00:23s\n",
      "epoch 12 | loss: 0.30045 | val_0_auc: 0.93501 |  0:00:24s\n",
      "epoch 13 | loss: 0.29542 | val_0_auc: 0.94106 |  0:00:26s\n",
      "epoch 14 | loss: 0.29144 | val_0_auc: 0.93288 |  0:00:28s\n",
      "epoch 15 | loss: 0.30967 | val_0_auc: 0.94232 |  0:00:30s\n",
      "epoch 16 | loss: 0.3358  | val_0_auc: 0.93959 |  0:00:32s\n",
      "epoch 17 | loss: 0.31898 | val_0_auc: 0.94582 |  0:00:34s\n",
      "epoch 18 | loss: 0.31298 | val_0_auc: 0.95075 |  0:00:36s\n",
      "epoch 19 | loss: 0.2817  | val_0_auc: 0.95982 |  0:00:38s\n",
      "epoch 20 | loss: 0.2417  | val_0_auc: 0.9742  |  0:00:40s\n",
      "epoch 21 | loss: 0.23415 | val_0_auc: 0.96244 |  0:00:42s\n",
      "epoch 22 | loss: 0.22616 | val_0_auc: 0.96802 |  0:00:44s\n",
      "epoch 23 | loss: 0.22386 | val_0_auc: 0.96736 |  0:00:46s\n",
      "epoch 24 | loss: 0.20041 | val_0_auc: 0.97656 |  0:00:47s\n",
      "epoch 25 | loss: 0.20931 | val_0_auc: 0.97345 |  0:00:49s\n",
      "epoch 26 | loss: 0.22119 | val_0_auc: 0.9771  |  0:00:51s\n",
      "epoch 27 | loss: 0.21291 | val_0_auc: 0.96508 |  0:00:53s\n",
      "epoch 28 | loss: 0.22738 | val_0_auc: 0.96489 |  0:00:55s\n",
      "epoch 29 | loss: 0.26839 | val_0_auc: 0.96503 |  0:00:57s\n",
      "epoch 30 | loss: 0.2611  | val_0_auc: 0.97112 |  0:00:59s\n",
      "epoch 31 | loss: 0.23399 | val_0_auc: 0.96869 |  0:01:01s\n",
      "epoch 32 | loss: 0.21566 | val_0_auc: 0.97577 |  0:01:03s\n",
      "epoch 33 | loss: 0.20936 | val_0_auc: 0.97635 |  0:01:05s\n",
      "epoch 34 | loss: 0.19984 | val_0_auc: 0.97753 |  0:01:06s\n",
      "epoch 35 | loss: 0.20846 | val_0_auc: 0.97016 |  0:01:08s\n",
      "epoch 36 | loss: 0.22603 | val_0_auc: 0.97249 |  0:01:10s\n",
      "epoch 37 | loss: 0.21093 | val_0_auc: 0.98021 |  0:01:12s\n",
      "epoch 38 | loss: 0.20639 | val_0_auc: 0.97712 |  0:01:14s\n",
      "epoch 39 | loss: 0.19471 | val_0_auc: 0.98515 |  0:01:16s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.98515\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98514986  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 77.1747\n",
      "Function value obtained: -0.9851\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9003227334215127, 'lambda_sparse': 0.005578615928537821, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.96055 | val_0_auc: 0.69323 |  0:00:01s\n",
      "epoch 1  | loss: 0.64664 | val_0_auc: 0.78817 |  0:00:03s\n",
      "epoch 2  | loss: 0.57268 | val_0_auc: 0.83716 |  0:00:04s\n",
      "epoch 3  | loss: 0.4685  | val_0_auc: 0.83335 |  0:00:06s\n",
      "epoch 4  | loss: 0.43728 | val_0_auc: 0.84213 |  0:00:07s\n",
      "epoch 5  | loss: 0.44843 | val_0_auc: 0.84233 |  0:00:09s\n",
      "epoch 6  | loss: 0.41469 | val_0_auc: 0.87029 |  0:00:10s\n",
      "epoch 7  | loss: 0.39102 | val_0_auc: 0.88907 |  0:00:12s\n",
      "epoch 8  | loss: 0.38439 | val_0_auc: 0.90252 |  0:00:14s\n",
      "epoch 9  | loss: 0.36588 | val_0_auc: 0.90705 |  0:00:15s\n",
      "epoch 10 | loss: 0.35255 | val_0_auc: 0.9106  |  0:00:17s\n",
      "epoch 11 | loss: 0.34475 | val_0_auc: 0.90952 |  0:00:18s\n",
      "epoch 12 | loss: 0.32338 | val_0_auc: 0.92117 |  0:00:20s\n",
      "epoch 13 | loss: 0.30778 | val_0_auc: 0.92495 |  0:00:21s\n",
      "epoch 14 | loss: 0.30247 | val_0_auc: 0.92136 |  0:00:23s\n",
      "epoch 15 | loss: 0.28483 | val_0_auc: 0.9314  |  0:00:24s\n",
      "epoch 16 | loss: 0.30731 | val_0_auc: 0.9287  |  0:00:26s\n",
      "epoch 17 | loss: 0.32526 | val_0_auc: 0.90934 |  0:00:28s\n",
      "epoch 18 | loss: 0.32601 | val_0_auc: 0.92941 |  0:00:29s\n",
      "epoch 19 | loss: 0.30756 | val_0_auc: 0.92465 |  0:00:31s\n",
      "epoch 20 | loss: 0.29611 | val_0_auc: 0.92777 |  0:00:32s\n",
      "epoch 21 | loss: 0.29993 | val_0_auc: 0.93519 |  0:00:34s\n",
      "epoch 22 | loss: 0.32166 | val_0_auc: 0.92704 |  0:00:35s\n",
      "epoch 23 | loss: 0.35081 | val_0_auc: 0.93519 |  0:00:37s\n",
      "epoch 24 | loss: 0.28146 | val_0_auc: 0.94762 |  0:00:38s\n",
      "epoch 25 | loss: 0.29123 | val_0_auc: 0.9465  |  0:00:40s\n",
      "epoch 26 | loss: 0.26026 | val_0_auc: 0.96128 |  0:00:41s\n",
      "epoch 27 | loss: 0.24261 | val_0_auc: 0.95726 |  0:00:43s\n",
      "epoch 28 | loss: 0.24225 | val_0_auc: 0.95487 |  0:00:45s\n",
      "epoch 29 | loss: 0.23712 | val_0_auc: 0.96075 |  0:00:46s\n",
      "epoch 30 | loss: 0.24375 | val_0_auc: 0.95834 |  0:00:48s\n",
      "epoch 31 | loss: 0.23337 | val_0_auc: 0.96298 |  0:00:49s\n",
      "epoch 32 | loss: 0.22057 | val_0_auc: 0.96811 |  0:00:51s\n",
      "epoch 33 | loss: 0.22324 | val_0_auc: 0.96194 |  0:00:52s\n",
      "epoch 34 | loss: 0.2293  | val_0_auc: 0.9598  |  0:00:54s\n",
      "epoch 35 | loss: 0.2102  | val_0_auc: 0.96586 |  0:00:55s\n",
      "epoch 36 | loss: 0.21365 | val_0_auc: 0.96488 |  0:00:57s\n",
      "epoch 37 | loss: 0.24028 | val_0_auc: 0.95814 |  0:00:59s\n",
      "epoch 38 | loss: 0.24026 | val_0_auc: 0.95471 |  0:01:00s\n",
      "epoch 39 | loss: 0.23534 | val_0_auc: 0.96499 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 32 and best_val_0_auc = 0.96811\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96810971  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 62.6516\n",
      "Function value obtained: -0.9681\n",
      "Current minimum: -0.9963\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0128816501984081, 'lambda_sparse': 0.05182941323342218, 'n_steps': 4, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.66189 | val_0_auc: 0.88057 |  0:00:01s\n",
      "epoch 1  | loss: 0.34565 | val_0_auc: 0.91293 |  0:00:02s\n",
      "epoch 2  | loss: 0.30469 | val_0_auc: 0.93742 |  0:00:03s\n",
      "epoch 3  | loss: 0.27224 | val_0_auc: 0.93159 |  0:00:04s\n",
      "epoch 4  | loss: 0.23985 | val_0_auc: 0.96476 |  0:00:05s\n",
      "epoch 5  | loss: 0.21954 | val_0_auc: 0.97013 |  0:00:06s\n",
      "epoch 6  | loss: 0.20798 | val_0_auc: 0.97323 |  0:00:07s\n",
      "epoch 7  | loss: 0.19453 | val_0_auc: 0.97507 |  0:00:08s\n",
      "epoch 8  | loss: 0.17505 | val_0_auc: 0.98559 |  0:00:09s\n",
      "epoch 9  | loss: 0.16452 | val_0_auc: 0.98356 |  0:00:10s\n",
      "epoch 10 | loss: 0.16014 | val_0_auc: 0.98844 |  0:00:12s\n",
      "epoch 11 | loss: 0.15082 | val_0_auc: 0.98759 |  0:00:13s\n",
      "epoch 12 | loss: 0.16327 | val_0_auc: 0.98427 |  0:00:14s\n",
      "epoch 13 | loss: 0.15818 | val_0_auc: 0.98187 |  0:00:15s\n",
      "epoch 14 | loss: 0.15627 | val_0_auc: 0.98527 |  0:00:16s\n",
      "epoch 15 | loss: 0.14951 | val_0_auc: 0.98901 |  0:00:17s\n",
      "epoch 16 | loss: 0.16087 | val_0_auc: 0.98747 |  0:00:18s\n",
      "epoch 17 | loss: 0.1485  | val_0_auc: 0.98942 |  0:00:19s\n",
      "epoch 18 | loss: 0.13681 | val_0_auc: 0.99081 |  0:00:20s\n",
      "epoch 19 | loss: 0.13712 | val_0_auc: 0.99346 |  0:00:21s\n",
      "epoch 20 | loss: 0.13136 | val_0_auc: 0.9941  |  0:00:23s\n",
      "epoch 21 | loss: 0.12104 | val_0_auc: 0.995   |  0:00:24s\n",
      "epoch 22 | loss: 0.11779 | val_0_auc: 0.99364 |  0:00:25s\n",
      "epoch 23 | loss: 0.11745 | val_0_auc: 0.99393 |  0:00:26s\n",
      "epoch 24 | loss: 0.11386 | val_0_auc: 0.99559 |  0:00:27s\n",
      "epoch 25 | loss: 0.10638 | val_0_auc: 0.99554 |  0:00:28s\n",
      "epoch 26 | loss: 0.1007  | val_0_auc: 0.99577 |  0:00:29s\n",
      "epoch 27 | loss: 0.10649 | val_0_auc: 0.99688 |  0:00:30s\n",
      "epoch 28 | loss: 0.09985 | val_0_auc: 0.99641 |  0:00:31s\n",
      "epoch 29 | loss: 0.09986 | val_0_auc: 0.99629 |  0:00:32s\n",
      "epoch 30 | loss: 0.10568 | val_0_auc: 0.99479 |  0:00:33s\n",
      "epoch 31 | loss: 0.09823 | val_0_auc: 0.99581 |  0:00:34s\n",
      "epoch 32 | loss: 0.10143 | val_0_auc: 0.99579 |  0:00:36s\n",
      "epoch 33 | loss: 0.10046 | val_0_auc: 0.99569 |  0:00:37s\n",
      "epoch 34 | loss: 0.09661 | val_0_auc: 0.9939  |  0:00:38s\n",
      "epoch 35 | loss: 0.09815 | val_0_auc: 0.99684 |  0:00:39s\n",
      "epoch 36 | loss: 0.10661 | val_0_auc: 0.99699 |  0:00:40s\n",
      "epoch 37 | loss: 0.10114 | val_0_auc: 0.99306 |  0:00:41s\n",
      "epoch 38 | loss: 0.09357 | val_0_auc: 0.99625 |  0:00:42s\n",
      "epoch 39 | loss: 0.0965  | val_0_auc: 0.99668 |  0:00:43s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99699\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9969934  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 44.0476\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9970\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1449286909854504, 'lambda_sparse': 0.07744848705407832, 'n_steps': 3, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.72935 | val_0_auc: 0.84259 |  0:00:00s\n",
      "epoch 1  | loss: 0.41192 | val_0_auc: 0.89633 |  0:00:01s\n",
      "epoch 2  | loss: 0.33202 | val_0_auc: 0.92369 |  0:00:02s\n",
      "epoch 3  | loss: 0.28022 | val_0_auc: 0.93836 |  0:00:02s\n",
      "epoch 4  | loss: 0.24468 | val_0_auc: 0.95325 |  0:00:03s\n",
      "epoch 5  | loss: 0.22459 | val_0_auc: 0.96806 |  0:00:04s\n",
      "epoch 6  | loss: 0.21899 | val_0_auc: 0.97963 |  0:00:05s\n",
      "epoch 7  | loss: 0.19422 | val_0_auc: 0.98332 |  0:00:05s\n",
      "epoch 8  | loss: 0.16367 | val_0_auc: 0.98508 |  0:00:06s\n",
      "epoch 9  | loss: 0.16305 | val_0_auc: 0.97938 |  0:00:07s\n",
      "epoch 10 | loss: 0.14986 | val_0_auc: 0.99212 |  0:00:08s\n",
      "epoch 11 | loss: 0.14585 | val_0_auc: 0.98859 |  0:00:08s\n",
      "epoch 12 | loss: 0.14536 | val_0_auc: 0.9918  |  0:00:09s\n",
      "epoch 13 | loss: 0.14793 | val_0_auc: 0.99268 |  0:00:10s\n",
      "epoch 14 | loss: 0.13531 | val_0_auc: 0.99448 |  0:00:10s\n",
      "epoch 15 | loss: 0.1273  | val_0_auc: 0.99468 |  0:00:11s\n",
      "epoch 16 | loss: 0.11745 | val_0_auc: 0.99606 |  0:00:12s\n",
      "epoch 17 | loss: 0.12178 | val_0_auc: 0.99139 |  0:00:13s\n",
      "epoch 18 | loss: 0.12028 | val_0_auc: 0.98929 |  0:00:13s\n",
      "epoch 19 | loss: 0.11723 | val_0_auc: 0.98941 |  0:00:14s\n",
      "epoch 20 | loss: 0.12107 | val_0_auc: 0.98965 |  0:00:15s\n",
      "epoch 21 | loss: 0.1171  | val_0_auc: 0.99337 |  0:00:16s\n",
      "epoch 22 | loss: 0.11903 | val_0_auc: 0.99084 |  0:00:16s\n",
      "epoch 23 | loss: 0.11761 | val_0_auc: 0.99411 |  0:00:17s\n",
      "epoch 24 | loss: 0.10923 | val_0_auc: 0.99468 |  0:00:18s\n",
      "epoch 25 | loss: 0.10846 | val_0_auc: 0.99218 |  0:00:18s\n",
      "epoch 26 | loss: 0.10932 | val_0_auc: 0.99369 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.99606\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99605631  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 19.8543\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9970\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1227966185155351, 'lambda_sparse': 0.05232096763520789, 'n_steps': 4, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.68297 | val_0_auc: 0.85087 |  0:00:01s\n",
      "epoch 1  | loss: 0.40949 | val_0_auc: 0.88544 |  0:00:01s\n",
      "epoch 2  | loss: 0.33448 | val_0_auc: 0.91407 |  0:00:02s\n",
      "epoch 3  | loss: 0.28311 | val_0_auc: 0.94238 |  0:00:03s\n",
      "epoch 4  | loss: 0.2482  | val_0_auc: 0.94676 |  0:00:04s\n",
      "epoch 5  | loss: 0.22938 | val_0_auc: 0.96162 |  0:00:05s\n",
      "epoch 6  | loss: 0.20721 | val_0_auc: 0.96847 |  0:00:06s\n",
      "epoch 7  | loss: 0.19322 | val_0_auc: 0.97243 |  0:00:07s\n",
      "epoch 8  | loss: 0.18322 | val_0_auc: 0.97863 |  0:00:09s\n",
      "epoch 9  | loss: 0.16964 | val_0_auc: 0.98585 |  0:00:10s\n",
      "epoch 10 | loss: 0.15815 | val_0_auc: 0.98776 |  0:00:11s\n",
      "epoch 11 | loss: 0.15818 | val_0_auc: 0.98935 |  0:00:12s\n",
      "epoch 12 | loss: 0.15421 | val_0_auc: 0.99023 |  0:00:13s\n",
      "epoch 13 | loss: 0.1408  | val_0_auc: 0.99074 |  0:00:14s\n",
      "epoch 14 | loss: 0.13876 | val_0_auc: 0.99065 |  0:00:15s\n",
      "epoch 15 | loss: 0.14366 | val_0_auc: 0.99141 |  0:00:15s\n",
      "epoch 16 | loss: 0.14163 | val_0_auc: 0.99149 |  0:00:16s\n",
      "epoch 17 | loss: 0.13136 | val_0_auc: 0.99601 |  0:00:17s\n",
      "epoch 18 | loss: 0.1225  | val_0_auc: 0.99217 |  0:00:18s\n",
      "epoch 19 | loss: 0.10997 | val_0_auc: 0.99556 |  0:00:19s\n",
      "epoch 20 | loss: 0.10914 | val_0_auc: 0.99645 |  0:00:20s\n",
      "epoch 21 | loss: 0.10131 | val_0_auc: 0.9965  |  0:00:21s\n",
      "epoch 22 | loss: 0.10054 | val_0_auc: 0.99645 |  0:00:22s\n",
      "epoch 23 | loss: 0.10141 | val_0_auc: 0.99538 |  0:00:23s\n",
      "epoch 24 | loss: 0.10237 | val_0_auc: 0.99614 |  0:00:24s\n",
      "epoch 25 | loss: 0.09955 | val_0_auc: 0.99555 |  0:00:25s\n",
      "epoch 26 | loss: 0.10145 | val_0_auc: 0.99572 |  0:00:26s\n",
      "epoch 27 | loss: 0.09674 | val_0_auc: 0.99483 |  0:00:27s\n",
      "epoch 28 | loss: 0.09779 | val_0_auc: 0.99719 |  0:00:28s\n",
      "epoch 29 | loss: 0.09708 | val_0_auc: 0.99664 |  0:00:29s\n",
      "epoch 30 | loss: 0.10517 | val_0_auc: 0.99678 |  0:00:30s\n",
      "epoch 31 | loss: 0.09239 | val_0_auc: 0.99573 |  0:00:31s\n",
      "epoch 32 | loss: 0.09324 | val_0_auc: 0.99662 |  0:00:32s\n",
      "epoch 33 | loss: 0.0888  | val_0_auc: 0.99725 |  0:00:33s\n",
      "epoch 34 | loss: 0.08974 | val_0_auc: 0.99583 |  0:00:34s\n",
      "epoch 35 | loss: 0.09069 | val_0_auc: 0.99569 |  0:00:35s\n",
      "epoch 36 | loss: 0.09736 | val_0_auc: 0.99476 |  0:00:36s\n",
      "epoch 37 | loss: 0.09038 | val_0_auc: 0.99612 |  0:00:37s\n",
      "epoch 38 | loss: 0.09948 | val_0_auc: 0.99728 |  0:00:38s\n",
      "epoch 39 | loss: 0.08837 | val_0_auc: 0.99587 |  0:00:39s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99728\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99727599  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 40.1944\n",
      "Function value obtained: -0.9973\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2261056948717066, 'lambda_sparse': 0.015491844469481186, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.77019 | val_0_auc: 0.76348 |  0:00:01s\n",
      "epoch 1  | loss: 0.52281 | val_0_auc: 0.83876 |  0:00:02s\n",
      "epoch 2  | loss: 0.43752 | val_0_auc: 0.88312 |  0:00:03s\n",
      "epoch 3  | loss: 0.35008 | val_0_auc: 0.9039  |  0:00:04s\n",
      "epoch 4  | loss: 0.31025 | val_0_auc: 0.91296 |  0:00:06s\n",
      "epoch 5  | loss: 0.28112 | val_0_auc: 0.92406 |  0:00:07s\n",
      "epoch 6  | loss: 0.25632 | val_0_auc: 0.94502 |  0:00:08s\n",
      "epoch 7  | loss: 0.23717 | val_0_auc: 0.95307 |  0:00:09s\n",
      "epoch 8  | loss: 0.24569 | val_0_auc: 0.95125 |  0:00:11s\n",
      "epoch 9  | loss: 0.2233  | val_0_auc: 0.95732 |  0:00:12s\n",
      "epoch 10 | loss: 0.22143 | val_0_auc: 0.95747 |  0:00:13s\n",
      "epoch 11 | loss: 0.21526 | val_0_auc: 0.9736  |  0:00:14s\n",
      "epoch 12 | loss: 0.20891 | val_0_auc: 0.96833 |  0:00:15s\n",
      "epoch 13 | loss: 0.20559 | val_0_auc: 0.96582 |  0:00:17s\n",
      "epoch 14 | loss: 0.20373 | val_0_auc: 0.97524 |  0:00:18s\n",
      "epoch 15 | loss: 0.1962  | val_0_auc: 0.97623 |  0:00:19s\n",
      "epoch 16 | loss: 0.19035 | val_0_auc: 0.97849 |  0:00:20s\n",
      "epoch 17 | loss: 0.17812 | val_0_auc: 0.97369 |  0:00:22s\n",
      "epoch 18 | loss: 0.1846  | val_0_auc: 0.98012 |  0:00:23s\n",
      "epoch 19 | loss: 0.16731 | val_0_auc: 0.97955 |  0:00:24s\n",
      "epoch 20 | loss: 0.16396 | val_0_auc: 0.98116 |  0:00:26s\n",
      "epoch 21 | loss: 0.17191 | val_0_auc: 0.97542 |  0:00:27s\n",
      "epoch 22 | loss: 0.17169 | val_0_auc: 0.98067 |  0:00:28s\n",
      "epoch 23 | loss: 0.18025 | val_0_auc: 0.98105 |  0:00:29s\n",
      "epoch 24 | loss: 0.16443 | val_0_auc: 0.98495 |  0:00:30s\n",
      "epoch 25 | loss: 0.15426 | val_0_auc: 0.98605 |  0:00:32s\n",
      "epoch 26 | loss: 0.14998 | val_0_auc: 0.98401 |  0:00:33s\n",
      "epoch 27 | loss: 0.14739 | val_0_auc: 0.9798  |  0:00:34s\n",
      "epoch 28 | loss: 0.15912 | val_0_auc: 0.97977 |  0:00:35s\n",
      "epoch 29 | loss: 0.15056 | val_0_auc: 0.97849 |  0:00:36s\n",
      "epoch 30 | loss: 0.14631 | val_0_auc: 0.98332 |  0:00:38s\n",
      "epoch 31 | loss: 0.14792 | val_0_auc: 0.98789 |  0:00:39s\n",
      "epoch 32 | loss: 0.14338 | val_0_auc: 0.98736 |  0:00:40s\n",
      "epoch 33 | loss: 0.13783 | val_0_auc: 0.98868 |  0:00:41s\n",
      "epoch 34 | loss: 0.14142 | val_0_auc: 0.98253 |  0:00:42s\n",
      "epoch 35 | loss: 0.14264 | val_0_auc: 0.98781 |  0:00:44s\n",
      "epoch 36 | loss: 0.1441  | val_0_auc: 0.9836  |  0:00:45s\n",
      "epoch 37 | loss: 0.15175 | val_0_auc: 0.98804 |  0:00:46s\n",
      "epoch 38 | loss: 0.13721 | val_0_auc: 0.98943 |  0:00:47s\n",
      "epoch 39 | loss: 0.13219 | val_0_auc: 0.98701 |  0:00:48s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.98943\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98942808  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 49.3860\n",
      "Function value obtained: -0.9894\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1594807748112903, 'lambda_sparse': 0.04213668506588438, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.7}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.40059 | val_0_auc: 0.7452  |  0:00:01s\n",
      "epoch 1  | loss: 0.58673 | val_0_auc: 0.8413  |  0:00:03s\n",
      "epoch 2  | loss: 0.42228 | val_0_auc: 0.88036 |  0:00:05s\n",
      "epoch 3  | loss: 0.35837 | val_0_auc: 0.90274 |  0:00:06s\n",
      "epoch 4  | loss: 0.31593 | val_0_auc: 0.93348 |  0:00:08s\n",
      "epoch 5  | loss: 0.28145 | val_0_auc: 0.95308 |  0:00:10s\n",
      "epoch 6  | loss: 0.27582 | val_0_auc: 0.95384 |  0:00:12s\n",
      "epoch 7  | loss: 0.25154 | val_0_auc: 0.93788 |  0:00:13s\n",
      "epoch 8  | loss: 0.25696 | val_0_auc: 0.95671 |  0:00:15s\n",
      "epoch 9  | loss: 0.29254 | val_0_auc: 0.97504 |  0:00:17s\n",
      "epoch 10 | loss: 0.23349 | val_0_auc: 0.96819 |  0:00:19s\n",
      "epoch 11 | loss: 0.23884 | val_0_auc: 0.97208 |  0:00:20s\n",
      "epoch 12 | loss: 0.28525 | val_0_auc: 0.97248 |  0:00:22s\n",
      "epoch 13 | loss: 0.26952 | val_0_auc: 0.97758 |  0:00:24s\n",
      "epoch 14 | loss: 0.27751 | val_0_auc: 0.97479 |  0:00:25s\n",
      "epoch 15 | loss: 0.22044 | val_0_auc: 0.98167 |  0:00:27s\n",
      "epoch 16 | loss: 0.19851 | val_0_auc: 0.98271 |  0:00:29s\n",
      "epoch 17 | loss: 0.1986  | val_0_auc: 0.97789 |  0:00:31s\n",
      "epoch 18 | loss: 0.19992 | val_0_auc: 0.98315 |  0:00:32s\n",
      "epoch 19 | loss: 0.18411 | val_0_auc: 0.98505 |  0:00:34s\n",
      "epoch 20 | loss: 0.17651 | val_0_auc: 0.98122 |  0:00:36s\n",
      "epoch 21 | loss: 0.1759  | val_0_auc: 0.98679 |  0:00:37s\n",
      "epoch 22 | loss: 0.16712 | val_0_auc: 0.98525 |  0:00:39s\n",
      "epoch 23 | loss: 0.16785 | val_0_auc: 0.98708 |  0:00:41s\n",
      "epoch 24 | loss: 0.15227 | val_0_auc: 0.9897  |  0:00:43s\n",
      "epoch 25 | loss: 0.1442  | val_0_auc: 0.98675 |  0:00:44s\n",
      "epoch 26 | loss: 0.16166 | val_0_auc: 0.9886  |  0:00:46s\n",
      "epoch 27 | loss: 0.14585 | val_0_auc: 0.98677 |  0:00:48s\n",
      "epoch 28 | loss: 0.1424  | val_0_auc: 0.99237 |  0:00:49s\n",
      "epoch 29 | loss: 0.13522 | val_0_auc: 0.99037 |  0:00:51s\n",
      "epoch 30 | loss: 0.14214 | val_0_auc: 0.98527 |  0:00:53s\n",
      "epoch 31 | loss: 0.14705 | val_0_auc: 0.99053 |  0:00:54s\n",
      "epoch 32 | loss: 0.14659 | val_0_auc: 0.98575 |  0:00:56s\n",
      "epoch 33 | loss: 0.14539 | val_0_auc: 0.98986 |  0:00:58s\n",
      "epoch 34 | loss: 0.14828 | val_0_auc: 0.99017 |  0:01:00s\n",
      "epoch 35 | loss: 0.1338  | val_0_auc: 0.98978 |  0:01:01s\n",
      "epoch 36 | loss: 0.13143 | val_0_auc: 0.99104 |  0:01:03s\n",
      "epoch 37 | loss: 0.12498 | val_0_auc: 0.99174 |  0:01:05s\n",
      "epoch 38 | loss: 0.13762 | val_0_auc: 0.98935 |  0:01:07s\n",
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_val_0_auc = 0.99237\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99237027  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 67.5500\n",
      "Function value obtained: -0.9924\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4792925159387489, 'lambda_sparse': 0.016348086489081717, 'n_steps': 10, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.89459 | val_0_auc: 0.80777 |  0:00:02s\n",
      "epoch 1  | loss: 0.53285 | val_0_auc: 0.86378 |  0:00:04s\n",
      "epoch 2  | loss: 0.45221 | val_0_auc: 0.85404 |  0:00:06s\n",
      "epoch 3  | loss: 0.40897 | val_0_auc: 0.88294 |  0:00:09s\n",
      "epoch 4  | loss: 0.47443 | val_0_auc: 0.88411 |  0:00:11s\n",
      "epoch 5  | loss: 0.4274  | val_0_auc: 0.89822 |  0:00:13s\n",
      "epoch 6  | loss: 0.39083 | val_0_auc: 0.91226 |  0:00:16s\n",
      "epoch 7  | loss: 0.39079 | val_0_auc: 0.90888 |  0:00:18s\n",
      "epoch 8  | loss: 0.38341 | val_0_auc: 0.90551 |  0:00:20s\n",
      "epoch 9  | loss: 0.4067  | val_0_auc: 0.91148 |  0:00:23s\n",
      "epoch 10 | loss: 0.38303 | val_0_auc: 0.92433 |  0:00:25s\n",
      "epoch 11 | loss: 0.34745 | val_0_auc: 0.93023 |  0:00:27s\n",
      "epoch 12 | loss: 0.34105 | val_0_auc: 0.92295 |  0:00:30s\n",
      "epoch 13 | loss: 0.41804 | val_0_auc: 0.89355 |  0:00:32s\n",
      "epoch 14 | loss: 0.99435 | val_0_auc: 0.90344 |  0:00:34s\n",
      "epoch 15 | loss: 1.0485  | val_0_auc: 0.88436 |  0:00:37s\n",
      "epoch 16 | loss: 0.67683 | val_0_auc: 0.90147 |  0:00:39s\n",
      "epoch 17 | loss: 0.40547 | val_0_auc: 0.91249 |  0:00:41s\n",
      "epoch 18 | loss: 0.34743 | val_0_auc: 0.91371 |  0:00:43s\n",
      "epoch 19 | loss: 0.33423 | val_0_auc: 0.92081 |  0:00:46s\n",
      "epoch 20 | loss: 0.3346  | val_0_auc: 0.9228  |  0:00:48s\n",
      "epoch 21 | loss: 0.32827 | val_0_auc: 0.91624 |  0:00:50s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.93023\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.93023116  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 51.7601\n",
      "Function value obtained: -0.9302\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3914890440790924, 'lambda_sparse': 0.06565875600615868, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.72861 | val_0_auc: 0.79342 |  0:00:01s\n",
      "epoch 1  | loss: 0.45969 | val_0_auc: 0.86359 |  0:00:01s\n",
      "epoch 2  | loss: 0.38732 | val_0_auc: 0.91973 |  0:00:02s\n",
      "epoch 3  | loss: 0.32541 | val_0_auc: 0.93209 |  0:00:03s\n",
      "epoch 4  | loss: 0.29764 | val_0_auc: 0.9275  |  0:00:04s\n",
      "epoch 5  | loss: 0.27635 | val_0_auc: 0.94647 |  0:00:05s\n",
      "epoch 6  | loss: 0.25113 | val_0_auc: 0.95474 |  0:00:06s\n",
      "epoch 7  | loss: 0.23635 | val_0_auc: 0.95755 |  0:00:07s\n",
      "epoch 8  | loss: 0.22581 | val_0_auc: 0.96017 |  0:00:08s\n",
      "epoch 9  | loss: 0.21445 | val_0_auc: 0.95697 |  0:00:09s\n",
      "epoch 10 | loss: 0.20694 | val_0_auc: 0.96765 |  0:00:09s\n",
      "epoch 11 | loss: 0.19664 | val_0_auc: 0.96976 |  0:00:10s\n",
      "epoch 12 | loss: 0.19136 | val_0_auc: 0.97261 |  0:00:11s\n",
      "epoch 13 | loss: 0.18276 | val_0_auc: 0.97975 |  0:00:12s\n",
      "epoch 14 | loss: 0.16231 | val_0_auc: 0.98217 |  0:00:13s\n",
      "epoch 15 | loss: 0.15266 | val_0_auc: 0.98611 |  0:00:14s\n",
      "epoch 16 | loss: 0.15188 | val_0_auc: 0.98649 |  0:00:15s\n",
      "epoch 17 | loss: 0.16009 | val_0_auc: 0.98616 |  0:00:16s\n",
      "epoch 18 | loss: 0.15077 | val_0_auc: 0.98709 |  0:00:17s\n",
      "epoch 19 | loss: 0.14448 | val_0_auc: 0.98929 |  0:00:17s\n",
      "epoch 20 | loss: 0.13377 | val_0_auc: 0.99287 |  0:00:18s\n",
      "epoch 21 | loss: 0.12386 | val_0_auc: 0.99151 |  0:00:19s\n",
      "epoch 22 | loss: 0.11869 | val_0_auc: 0.99423 |  0:00:20s\n",
      "epoch 23 | loss: 0.1149  | val_0_auc: 0.99284 |  0:00:21s\n",
      "epoch 24 | loss: 0.10911 | val_0_auc: 0.9939  |  0:00:22s\n",
      "epoch 25 | loss: 0.11136 | val_0_auc: 0.99547 |  0:00:23s\n",
      "epoch 26 | loss: 0.10426 | val_0_auc: 0.99478 |  0:00:24s\n",
      "epoch 27 | loss: 0.1011  | val_0_auc: 0.99387 |  0:00:24s\n",
      "epoch 28 | loss: 0.11134 | val_0_auc: 0.99434 |  0:00:25s\n",
      "epoch 29 | loss: 0.10496 | val_0_auc: 0.98994 |  0:00:26s\n",
      "epoch 30 | loss: 0.10776 | val_0_auc: 0.9943  |  0:00:27s\n",
      "epoch 31 | loss: 0.09931 | val_0_auc: 0.99444 |  0:00:28s\n",
      "epoch 32 | loss: 0.09595 | val_0_auc: 0.99432 |  0:00:29s\n",
      "epoch 33 | loss: 0.10286 | val_0_auc: 0.99419 |  0:00:30s\n",
      "epoch 34 | loss: 0.10628 | val_0_auc: 0.99393 |  0:00:31s\n",
      "epoch 35 | loss: 0.09601 | val_0_auc: 0.99463 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.99547\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99547244  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 32.3896\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.28886498008948, 'lambda_sparse': 0.07617229858813518, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.84856 | val_0_auc: 0.82649 |  0:00:01s\n",
      "epoch 1  | loss: 0.47634 | val_0_auc: 0.86524 |  0:00:02s\n",
      "epoch 2  | loss: 0.40803 | val_0_auc: 0.90579 |  0:00:04s\n",
      "epoch 3  | loss: 0.36951 | val_0_auc: 0.9059  |  0:00:05s\n",
      "epoch 4  | loss: 0.34591 | val_0_auc: 0.93902 |  0:00:06s\n",
      "epoch 5  | loss: 0.31489 | val_0_auc: 0.9452  |  0:00:08s\n",
      "epoch 6  | loss: 0.27188 | val_0_auc: 0.95356 |  0:00:09s\n",
      "epoch 7  | loss: 0.25066 | val_0_auc: 0.96584 |  0:00:10s\n",
      "epoch 8  | loss: 0.23025 | val_0_auc: 0.95929 |  0:00:12s\n",
      "epoch 9  | loss: 0.21686 | val_0_auc: 0.96603 |  0:00:13s\n",
      "epoch 10 | loss: 0.20475 | val_0_auc: 0.96771 |  0:00:15s\n",
      "epoch 11 | loss: 0.19923 | val_0_auc: 0.97035 |  0:00:16s\n",
      "epoch 12 | loss: 0.20393 | val_0_auc: 0.96354 |  0:00:17s\n",
      "epoch 13 | loss: 0.18849 | val_0_auc: 0.97776 |  0:00:19s\n",
      "epoch 14 | loss: 0.18758 | val_0_auc: 0.98321 |  0:00:20s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15 | loss: 0.17555 | val_0_auc: 0.98135 |  0:00:21s\n",
      "epoch 16 | loss: 0.16649 | val_0_auc: 0.98672 |  0:00:23s\n",
      "epoch 17 | loss: 0.16667 | val_0_auc: 0.98392 |  0:00:24s\n",
      "epoch 18 | loss: 0.15178 | val_0_auc: 0.98734 |  0:00:25s\n",
      "epoch 19 | loss: 0.15313 | val_0_auc: 0.99175 |  0:00:27s\n",
      "epoch 20 | loss: 0.14691 | val_0_auc: 0.98721 |  0:00:28s\n",
      "epoch 21 | loss: 0.14872 | val_0_auc: 0.99128 |  0:00:29s\n",
      "epoch 22 | loss: 0.13805 | val_0_auc: 0.98579 |  0:00:31s\n",
      "epoch 23 | loss: 0.14928 | val_0_auc: 0.98594 |  0:00:32s\n",
      "epoch 24 | loss: 0.15162 | val_0_auc: 0.99243 |  0:00:34s\n",
      "epoch 25 | loss: 0.14776 | val_0_auc: 0.98713 |  0:00:35s\n",
      "epoch 26 | loss: 0.14937 | val_0_auc: 0.98519 |  0:00:36s\n",
      "epoch 27 | loss: 0.14507 | val_0_auc: 0.98589 |  0:00:38s\n",
      "epoch 28 | loss: 0.14015 | val_0_auc: 0.98983 |  0:00:39s\n",
      "epoch 29 | loss: 0.13722 | val_0_auc: 0.99016 |  0:00:40s\n",
      "epoch 30 | loss: 0.13008 | val_0_auc: 0.99017 |  0:00:42s\n",
      "epoch 31 | loss: 0.13825 | val_0_auc: 0.99025 |  0:00:43s\n",
      "epoch 32 | loss: 0.13731 | val_0_auc: 0.9918  |  0:00:44s\n",
      "epoch 33 | loss: 0.13497 | val_0_auc: 0.99089 |  0:00:46s\n",
      "epoch 34 | loss: 0.12234 | val_0_auc: 0.99086 |  0:00:47s\n",
      "\n",
      "Early stopping occurred at epoch 34 with best_epoch = 24 and best_val_0_auc = 0.99243\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99242845  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 48.2152\n",
      "Function value obtained: -0.9924\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.537791747294594, 'lambda_sparse': 0.01082785556644739, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.82595 | val_0_auc: 0.69911 |  0:00:01s\n",
      "epoch 1  | loss: 0.5158  | val_0_auc: 0.81003 |  0:00:02s\n",
      "epoch 2  | loss: 0.44232 | val_0_auc: 0.86762 |  0:00:03s\n",
      "epoch 3  | loss: 0.37082 | val_0_auc: 0.89671 |  0:00:04s\n",
      "epoch 4  | loss: 0.349   | val_0_auc: 0.89528 |  0:00:06s\n",
      "epoch 5  | loss: 0.33544 | val_0_auc: 0.91952 |  0:00:07s\n",
      "epoch 6  | loss: 0.31932 | val_0_auc: 0.93143 |  0:00:08s\n",
      "epoch 7  | loss: 0.30114 | val_0_auc: 0.92069 |  0:00:09s\n",
      "epoch 8  | loss: 0.29531 | val_0_auc: 0.92871 |  0:00:11s\n",
      "epoch 9  | loss: 0.28326 | val_0_auc: 0.9247  |  0:00:12s\n",
      "epoch 10 | loss: 0.29136 | val_0_auc: 0.91887 |  0:00:13s\n",
      "epoch 11 | loss: 0.28942 | val_0_auc: 0.9257  |  0:00:14s\n",
      "epoch 12 | loss: 0.26639 | val_0_auc: 0.93349 |  0:00:15s\n",
      "epoch 13 | loss: 0.28548 | val_0_auc: 0.92257 |  0:00:17s\n",
      "epoch 14 | loss: 0.27197 | val_0_auc: 0.92984 |  0:00:18s\n",
      "epoch 15 | loss: 0.25545 | val_0_auc: 0.93586 |  0:00:19s\n",
      "epoch 16 | loss: 0.25283 | val_0_auc: 0.94586 |  0:00:20s\n",
      "epoch 17 | loss: 0.2357  | val_0_auc: 0.94929 |  0:00:22s\n",
      "epoch 18 | loss: 0.24168 | val_0_auc: 0.95311 |  0:00:23s\n",
      "epoch 19 | loss: 0.22943 | val_0_auc: 0.94034 |  0:00:24s\n",
      "epoch 20 | loss: 0.25157 | val_0_auc: 0.93879 |  0:00:25s\n",
      "epoch 21 | loss: 0.23852 | val_0_auc: 0.94254 |  0:00:26s\n",
      "epoch 22 | loss: 0.23511 | val_0_auc: 0.94552 |  0:00:28s\n",
      "epoch 23 | loss: 0.22327 | val_0_auc: 0.95127 |  0:00:29s\n",
      "epoch 24 | loss: 0.21673 | val_0_auc: 0.95506 |  0:00:30s\n",
      "epoch 25 | loss: 0.20226 | val_0_auc: 0.95989 |  0:00:31s\n",
      "epoch 26 | loss: 0.20398 | val_0_auc: 0.96139 |  0:00:33s\n",
      "epoch 27 | loss: 0.20325 | val_0_auc: 0.95909 |  0:00:34s\n",
      "epoch 28 | loss: 0.21439 | val_0_auc: 0.96785 |  0:00:35s\n",
      "epoch 29 | loss: 0.19653 | val_0_auc: 0.96796 |  0:00:36s\n",
      "epoch 30 | loss: 0.18775 | val_0_auc: 0.96249 |  0:00:37s\n",
      "epoch 31 | loss: 0.19666 | val_0_auc: 0.96603 |  0:00:39s\n",
      "epoch 32 | loss: 0.20547 | val_0_auc: 0.96367 |  0:00:40s\n",
      "epoch 33 | loss: 0.18185 | val_0_auc: 0.97536 |  0:00:41s\n",
      "epoch 34 | loss: 0.18818 | val_0_auc: 0.96793 |  0:00:42s\n",
      "epoch 35 | loss: 0.18569 | val_0_auc: 0.97153 |  0:00:44s\n",
      "epoch 36 | loss: 0.17599 | val_0_auc: 0.96918 |  0:00:45s\n",
      "epoch 37 | loss: 0.17892 | val_0_auc: 0.97453 |  0:00:46s\n",
      "epoch 38 | loss: 0.18981 | val_0_auc: 0.97366 |  0:00:47s\n",
      "epoch 39 | loss: 0.18061 | val_0_auc: 0.9772  |  0:00:48s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.9772\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97719599  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 49.3172\n",
      "Function value obtained: -0.9772\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1640027390571481, 'lambda_sparse': 0.09213199770940783, 'n_steps': 3, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.56224 | val_0_auc: 0.87829 |  0:00:00s\n",
      "epoch 1  | loss: 0.37624 | val_0_auc: 0.91148 |  0:00:01s\n",
      "epoch 2  | loss: 0.31918 | val_0_auc: 0.93588 |  0:00:02s\n",
      "epoch 3  | loss: 0.29448 | val_0_auc: 0.94118 |  0:00:03s\n",
      "epoch 4  | loss: 0.265   | val_0_auc: 0.94916 |  0:00:04s\n",
      "epoch 5  | loss: 0.24666 | val_0_auc: 0.95164 |  0:00:04s\n",
      "epoch 6  | loss: 0.23686 | val_0_auc: 0.95423 |  0:00:05s\n",
      "epoch 7  | loss: 0.2233  | val_0_auc: 0.96696 |  0:00:06s\n",
      "epoch 8  | loss: 0.21171 | val_0_auc: 0.9694  |  0:00:07s\n",
      "epoch 9  | loss: 0.2039  | val_0_auc: 0.97373 |  0:00:08s\n",
      "epoch 10 | loss: 0.20504 | val_0_auc: 0.976   |  0:00:09s\n",
      "epoch 11 | loss: 0.19582 | val_0_auc: 0.97478 |  0:00:09s\n",
      "epoch 12 | loss: 0.19329 | val_0_auc: 0.97737 |  0:00:10s\n",
      "epoch 13 | loss: 0.18383 | val_0_auc: 0.97538 |  0:00:11s\n",
      "epoch 14 | loss: 0.17944 | val_0_auc: 0.97853 |  0:00:12s\n",
      "epoch 15 | loss: 0.18668 | val_0_auc: 0.9708  |  0:00:13s\n",
      "epoch 16 | loss: 0.18605 | val_0_auc: 0.97606 |  0:00:13s\n",
      "epoch 17 | loss: 0.18217 | val_0_auc: 0.979   |  0:00:14s\n",
      "epoch 18 | loss: 0.18146 | val_0_auc: 0.9755  |  0:00:15s\n",
      "epoch 19 | loss: 0.19113 | val_0_auc: 0.97928 |  0:00:16s\n",
      "epoch 20 | loss: 0.18087 | val_0_auc: 0.97753 |  0:00:17s\n",
      "epoch 21 | loss: 0.17402 | val_0_auc: 0.97904 |  0:00:17s\n",
      "epoch 22 | loss: 0.17181 | val_0_auc: 0.97521 |  0:00:18s\n",
      "epoch 23 | loss: 0.17536 | val_0_auc: 0.97556 |  0:00:19s\n",
      "epoch 24 | loss: 0.18761 | val_0_auc: 0.97341 |  0:00:20s\n",
      "epoch 25 | loss: 0.18657 | val_0_auc: 0.96625 |  0:00:21s\n",
      "epoch 26 | loss: 0.17787 | val_0_auc: 0.97249 |  0:00:21s\n",
      "epoch 27 | loss: 0.19285 | val_0_auc: 0.97249 |  0:00:22s\n",
      "epoch 28 | loss: 0.18629 | val_0_auc: 0.97456 |  0:00:23s\n",
      "epoch 29 | loss: 0.17992 | val_0_auc: 0.97634 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.97928\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97927588  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 24.6200\n",
      "Function value obtained: -0.9793\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1517109210404446, 'lambda_sparse': 0.05119753112156941, 'n_steps': 4, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.80899 | val_0_auc: 0.75098 |  0:00:00s\n",
      "epoch 1  | loss: 0.56256 | val_0_auc: 0.83251 |  0:00:01s\n",
      "epoch 2  | loss: 0.45086 | val_0_auc: 0.88952 |  0:00:02s\n",
      "epoch 3  | loss: 0.36961 | val_0_auc: 0.91301 |  0:00:03s\n",
      "epoch 4  | loss: 0.31418 | val_0_auc: 0.92764 |  0:00:03s\n",
      "epoch 5  | loss: 0.28405 | val_0_auc: 0.93778 |  0:00:04s\n",
      "epoch 6  | loss: 0.27405 | val_0_auc: 0.94183 |  0:00:05s\n",
      "epoch 7  | loss: 0.26439 | val_0_auc: 0.94569 |  0:00:05s\n",
      "epoch 8  | loss: 0.25625 | val_0_auc: 0.95437 |  0:00:06s\n",
      "epoch 9  | loss: 0.25148 | val_0_auc: 0.96052 |  0:00:07s\n",
      "epoch 10 | loss: 0.24604 | val_0_auc: 0.97029 |  0:00:08s\n",
      "epoch 11 | loss: 0.22939 | val_0_auc: 0.9647  |  0:00:08s\n",
      "epoch 12 | loss: 0.21569 | val_0_auc: 0.96445 |  0:00:09s\n",
      "epoch 13 | loss: 0.21252 | val_0_auc: 0.96705 |  0:00:10s\n",
      "epoch 14 | loss: 0.20843 | val_0_auc: 0.96787 |  0:00:11s\n",
      "epoch 15 | loss: 0.21414 | val_0_auc: 0.9717  |  0:00:11s\n",
      "epoch 16 | loss: 0.20539 | val_0_auc: 0.97022 |  0:00:12s\n",
      "epoch 17 | loss: 0.19914 | val_0_auc: 0.97032 |  0:00:13s\n",
      "epoch 18 | loss: 0.19921 | val_0_auc: 0.97742 |  0:00:14s\n",
      "epoch 19 | loss: 0.19813 | val_0_auc: 0.97486 |  0:00:14s\n",
      "epoch 20 | loss: 0.19078 | val_0_auc: 0.9806  |  0:00:15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21 | loss: 0.19084 | val_0_auc: 0.97938 |  0:00:16s\n",
      "epoch 22 | loss: 0.17605 | val_0_auc: 0.981   |  0:00:16s\n",
      "epoch 23 | loss: 0.17236 | val_0_auc: 0.98306 |  0:00:17s\n",
      "epoch 24 | loss: 0.16009 | val_0_auc: 0.98741 |  0:00:18s\n",
      "epoch 25 | loss: 0.15241 | val_0_auc: 0.98668 |  0:00:19s\n",
      "epoch 26 | loss: 0.14612 | val_0_auc: 0.99162 |  0:00:19s\n",
      "epoch 27 | loss: 0.14543 | val_0_auc: 0.99184 |  0:00:20s\n",
      "epoch 28 | loss: 0.14946 | val_0_auc: 0.98966 |  0:00:21s\n",
      "epoch 29 | loss: 0.14396 | val_0_auc: 0.98624 |  0:00:22s\n",
      "epoch 30 | loss: 0.1394  | val_0_auc: 0.99052 |  0:00:22s\n",
      "epoch 31 | loss: 0.13129 | val_0_auc: 0.99219 |  0:00:23s\n",
      "epoch 32 | loss: 0.13615 | val_0_auc: 0.99077 |  0:00:24s\n",
      "epoch 33 | loss: 0.12905 | val_0_auc: 0.99265 |  0:00:25s\n",
      "epoch 34 | loss: 0.14428 | val_0_auc: 0.99108 |  0:00:25s\n",
      "epoch 35 | loss: 0.13412 | val_0_auc: 0.99268 |  0:00:26s\n",
      "epoch 36 | loss: 0.12244 | val_0_auc: 0.99243 |  0:00:27s\n",
      "epoch 37 | loss: 0.11953 | val_0_auc: 0.99414 |  0:00:28s\n",
      "epoch 38 | loss: 0.12623 | val_0_auc: 0.99256 |  0:00:28s\n",
      "epoch 39 | loss: 0.11687 | val_0_auc: 0.99397 |  0:00:29s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99414\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99414264  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 29.7651\n",
      "Function value obtained: -0.9941\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.874325039345203, 'lambda_sparse': 0.06983463988649458, 'n_steps': 4, 'n_a': 8, 'n_d': 8, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.82209 | val_0_auc: 0.69883 |  0:00:00s\n",
      "epoch 1  | loss: 0.60263 | val_0_auc: 0.75052 |  0:00:01s\n",
      "epoch 2  | loss: 0.54284 | val_0_auc: 0.80477 |  0:00:02s\n",
      "epoch 3  | loss: 0.48624 | val_0_auc: 0.84869 |  0:00:02s\n",
      "epoch 4  | loss: 0.46208 | val_0_auc: 0.85913 |  0:00:03s\n",
      "epoch 5  | loss: 0.42137 | val_0_auc: 0.89234 |  0:00:04s\n",
      "epoch 6  | loss: 0.38946 | val_0_auc: 0.89298 |  0:00:05s\n",
      "epoch 7  | loss: 0.36149 | val_0_auc: 0.92143 |  0:00:05s\n",
      "epoch 8  | loss: 0.33195 | val_0_auc: 0.92533 |  0:00:06s\n",
      "epoch 9  | loss: 0.31871 | val_0_auc: 0.93334 |  0:00:07s\n",
      "epoch 10 | loss: 0.29098 | val_0_auc: 0.93988 |  0:00:08s\n",
      "epoch 11 | loss: 0.26964 | val_0_auc: 0.94179 |  0:00:08s\n",
      "epoch 12 | loss: 0.27283 | val_0_auc: 0.93716 |  0:00:09s\n",
      "epoch 13 | loss: 0.26987 | val_0_auc: 0.94879 |  0:00:10s\n",
      "epoch 14 | loss: 0.25809 | val_0_auc: 0.94559 |  0:00:11s\n",
      "epoch 15 | loss: 0.25202 | val_0_auc: 0.9546  |  0:00:11s\n",
      "epoch 16 | loss: 0.24982 | val_0_auc: 0.94661 |  0:00:12s\n",
      "epoch 17 | loss: 0.2611  | val_0_auc: 0.94216 |  0:00:13s\n",
      "epoch 18 | loss: 0.2782  | val_0_auc: 0.94244 |  0:00:13s\n",
      "epoch 19 | loss: 0.27038 | val_0_auc: 0.94514 |  0:00:14s\n",
      "epoch 20 | loss: 0.26207 | val_0_auc: 0.95646 |  0:00:15s\n",
      "epoch 21 | loss: 0.24674 | val_0_auc: 0.95524 |  0:00:16s\n",
      "epoch 22 | loss: 0.22935 | val_0_auc: 0.95624 |  0:00:16s\n",
      "epoch 23 | loss: 0.21606 | val_0_auc: 0.95727 |  0:00:17s\n",
      "epoch 24 | loss: 0.23314 | val_0_auc: 0.95144 |  0:00:18s\n",
      "epoch 25 | loss: 0.23138 | val_0_auc: 0.95796 |  0:00:19s\n",
      "epoch 26 | loss: 0.2352  | val_0_auc: 0.95988 |  0:00:19s\n",
      "epoch 27 | loss: 0.22139 | val_0_auc: 0.9564  |  0:00:20s\n",
      "epoch 28 | loss: 0.22458 | val_0_auc: 0.95685 |  0:00:21s\n",
      "epoch 29 | loss: 0.22457 | val_0_auc: 0.9602  |  0:00:22s\n",
      "epoch 30 | loss: 0.22531 | val_0_auc: 0.95522 |  0:00:22s\n",
      "epoch 31 | loss: 0.21387 | val_0_auc: 0.96215 |  0:00:23s\n",
      "epoch 32 | loss: 0.21517 | val_0_auc: 0.96314 |  0:00:24s\n",
      "epoch 33 | loss: 0.19812 | val_0_auc: 0.9563  |  0:00:25s\n",
      "epoch 34 | loss: 0.21072 | val_0_auc: 0.96068 |  0:00:25s\n",
      "epoch 35 | loss: 0.19755 | val_0_auc: 0.96205 |  0:00:26s\n",
      "epoch 36 | loss: 0.17954 | val_0_auc: 0.96414 |  0:00:27s\n",
      "epoch 37 | loss: 0.18482 | val_0_auc: 0.96709 |  0:00:27s\n",
      "epoch 38 | loss: 0.1854  | val_0_auc: 0.96454 |  0:00:28s\n",
      "epoch 39 | loss: 0.17931 | val_0_auc: 0.97081 |  0:00:29s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.97081\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97080879  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 29.7059\n",
      "Function value obtained: -0.9708\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2824696433833136, 'lambda_sparse': 0.05485136965155538, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.98899 | val_0_auc: 0.86443 |  0:00:01s\n",
      "epoch 1  | loss: 0.41671 | val_0_auc: 0.88432 |  0:00:03s\n",
      "epoch 2  | loss: 0.37794 | val_0_auc: 0.89446 |  0:00:05s\n",
      "epoch 3  | loss: 0.36847 | val_0_auc: 0.90555 |  0:00:06s\n",
      "epoch 4  | loss: 0.32101 | val_0_auc: 0.93105 |  0:00:08s\n",
      "epoch 5  | loss: 0.289   | val_0_auc: 0.94439 |  0:00:10s\n",
      "epoch 6  | loss: 0.28186 | val_0_auc: 0.9258  |  0:00:11s\n",
      "epoch 7  | loss: 0.25852 | val_0_auc: 0.94291 |  0:00:13s\n",
      "epoch 8  | loss: 0.25425 | val_0_auc: 0.96588 |  0:00:15s\n",
      "epoch 9  | loss: 0.24738 | val_0_auc: 0.9708  |  0:00:17s\n",
      "epoch 10 | loss: 0.21381 | val_0_auc: 0.97862 |  0:00:18s\n",
      "epoch 11 | loss: 0.20463 | val_0_auc: 0.9767  |  0:00:20s\n",
      "epoch 12 | loss: 0.19525 | val_0_auc: 0.97697 |  0:00:22s\n",
      "epoch 13 | loss: 0.1841  | val_0_auc: 0.97525 |  0:00:23s\n",
      "epoch 14 | loss: 0.17628 | val_0_auc: 0.98474 |  0:00:25s\n",
      "epoch 15 | loss: 0.17268 | val_0_auc: 0.98109 |  0:00:27s\n",
      "epoch 16 | loss: 0.15808 | val_0_auc: 0.98086 |  0:00:28s\n",
      "epoch 17 | loss: 0.16203 | val_0_auc: 0.98245 |  0:00:30s\n",
      "epoch 18 | loss: 0.1679  | val_0_auc: 0.98556 |  0:00:32s\n",
      "epoch 19 | loss: 0.14176 | val_0_auc: 0.98474 |  0:00:34s\n",
      "epoch 20 | loss: 0.1396  | val_0_auc: 0.98606 |  0:00:35s\n",
      "epoch 21 | loss: 0.14116 | val_0_auc: 0.98366 |  0:00:37s\n",
      "epoch 22 | loss: 0.13525 | val_0_auc: 0.98698 |  0:00:39s\n",
      "epoch 23 | loss: 0.134   | val_0_auc: 0.98741 |  0:00:40s\n",
      "epoch 24 | loss: 0.13105 | val_0_auc: 0.99284 |  0:00:42s\n",
      "epoch 25 | loss: 0.12456 | val_0_auc: 0.98755 |  0:00:44s\n",
      "epoch 26 | loss: 0.12225 | val_0_auc: 0.99104 |  0:00:45s\n",
      "epoch 27 | loss: 0.13076 | val_0_auc: 0.98882 |  0:00:47s\n",
      "epoch 28 | loss: 0.13138 | val_0_auc: 0.99289 |  0:00:49s\n",
      "epoch 29 | loss: 0.13957 | val_0_auc: 0.99095 |  0:00:51s\n",
      "epoch 30 | loss: 0.12254 | val_0_auc: 0.99387 |  0:00:52s\n",
      "epoch 31 | loss: 0.12437 | val_0_auc: 0.99005 |  0:00:54s\n",
      "epoch 32 | loss: 0.12657 | val_0_auc: 0.99169 |  0:00:56s\n",
      "epoch 33 | loss: 0.12235 | val_0_auc: 0.99365 |  0:00:57s\n",
      "epoch 34 | loss: 0.1172  | val_0_auc: 0.99335 |  0:00:59s\n",
      "epoch 35 | loss: 0.13129 | val_0_auc: 0.99098 |  0:01:01s\n",
      "epoch 36 | loss: 0.13386 | val_0_auc: 0.99098 |  0:01:02s\n",
      "epoch 37 | loss: 0.12835 | val_0_auc: 0.98974 |  0:01:04s\n",
      "epoch 38 | loss: 0.11656 | val_0_auc: 0.99244 |  0:01:06s\n",
      "epoch 39 | loss: 0.11704 | val_0_auc: 0.99125 |  0:01:07s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 30 and best_val_0_auc = 0.99387\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99386629  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 68.5006\n",
      "Function value obtained: -0.9939\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0136672168226748, 'lambda_sparse': 0.025627563530688718, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.67151 | val_0_auc: 0.82816 |  0:00:00s\n",
      "epoch 1  | loss: 0.43586 | val_0_auc: 0.88581 |  0:00:01s\n",
      "epoch 2  | loss: 0.33126 | val_0_auc: 0.90576 |  0:00:02s\n",
      "epoch 3  | loss: 0.28099 | val_0_auc: 0.91994 |  0:00:03s\n",
      "epoch 4  | loss: 0.25561 | val_0_auc: 0.93441 |  0:00:04s\n",
      "epoch 5  | loss: 0.24264 | val_0_auc: 0.93568 |  0:00:05s\n",
      "epoch 6  | loss: 0.23756 | val_0_auc: 0.93841 |  0:00:06s\n",
      "epoch 7  | loss: 0.23386 | val_0_auc: 0.94255 |  0:00:07s\n",
      "epoch 8  | loss: 0.22314 | val_0_auc: 0.94315 |  0:00:08s\n",
      "epoch 9  | loss: 0.21432 | val_0_auc: 0.94693 |  0:00:09s\n",
      "epoch 10 | loss: 0.20961 | val_0_auc: 0.9521  |  0:00:09s\n",
      "epoch 11 | loss: 0.19785 | val_0_auc: 0.95373 |  0:00:10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.19795 | val_0_auc: 0.95645 |  0:00:11s\n",
      "epoch 13 | loss: 0.18788 | val_0_auc: 0.96558 |  0:00:12s\n",
      "epoch 14 | loss: 0.17683 | val_0_auc: 0.97282 |  0:00:13s\n",
      "epoch 15 | loss: 0.16617 | val_0_auc: 0.97707 |  0:00:14s\n",
      "epoch 16 | loss: 0.15234 | val_0_auc: 0.98556 |  0:00:15s\n",
      "epoch 17 | loss: 0.1362  | val_0_auc: 0.98291 |  0:00:16s\n",
      "epoch 18 | loss: 0.13089 | val_0_auc: 0.98555 |  0:00:17s\n",
      "epoch 19 | loss: 0.12377 | val_0_auc: 0.98914 |  0:00:17s\n",
      "epoch 20 | loss: 0.11456 | val_0_auc: 0.99199 |  0:00:18s\n",
      "epoch 21 | loss: 0.1175  | val_0_auc: 0.99447 |  0:00:19s\n",
      "epoch 22 | loss: 0.11345 | val_0_auc: 0.99143 |  0:00:20s\n",
      "epoch 23 | loss: 0.10592 | val_0_auc: 0.99035 |  0:00:21s\n",
      "epoch 24 | loss: 0.09732 | val_0_auc: 0.9936  |  0:00:22s\n",
      "epoch 25 | loss: 0.10281 | val_0_auc: 0.99202 |  0:00:23s\n",
      "epoch 26 | loss: 0.10394 | val_0_auc: 0.99567 |  0:00:24s\n",
      "epoch 27 | loss: 0.0986  | val_0_auc: 0.99285 |  0:00:25s\n",
      "epoch 28 | loss: 0.10528 | val_0_auc: 0.99242 |  0:00:25s\n",
      "epoch 29 | loss: 0.08854 | val_0_auc: 0.99227 |  0:00:26s\n",
      "epoch 30 | loss: 0.092   | val_0_auc: 0.99411 |  0:00:27s\n",
      "epoch 31 | loss: 0.0874  | val_0_auc: 0.9898  |  0:00:28s\n",
      "epoch 32 | loss: 0.10013 | val_0_auc: 0.99325 |  0:00:29s\n",
      "epoch 33 | loss: 0.08733 | val_0_auc: 0.99578 |  0:00:30s\n",
      "epoch 34 | loss: 0.0875  | val_0_auc: 0.99204 |  0:00:31s\n",
      "epoch 35 | loss: 0.09264 | val_0_auc: 0.99449 |  0:00:32s\n",
      "epoch 36 | loss: 0.09368 | val_0_auc: 0.99485 |  0:00:33s\n",
      "epoch 37 | loss: 0.10017 | val_0_auc: 0.99528 |  0:00:33s\n",
      "epoch 38 | loss: 0.08881 | val_0_auc: 0.99259 |  0:00:34s\n",
      "epoch 39 | loss: 0.08522 | val_0_auc: 0.9953  |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.99578\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99578204  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 36.0889\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6951428995284437, 'lambda_sparse': 0.018920609930241427, 'n_steps': 6, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.36709 | val_0_auc: 0.80289 |  0:00:01s\n",
      "epoch 1  | loss: 0.71119 | val_0_auc: 0.76968 |  0:00:03s\n",
      "epoch 2  | loss: 0.57358 | val_0_auc: 0.84085 |  0:00:05s\n",
      "epoch 3  | loss: 0.46737 | val_0_auc: 0.85053 |  0:00:07s\n",
      "epoch 4  | loss: 0.44473 | val_0_auc: 0.87429 |  0:00:09s\n",
      "epoch 5  | loss: 0.39196 | val_0_auc: 0.88622 |  0:00:11s\n",
      "epoch 6  | loss: 0.34341 | val_0_auc: 0.9206  |  0:00:13s\n",
      "epoch 7  | loss: 0.29506 | val_0_auc: 0.91778 |  0:00:15s\n",
      "epoch 8  | loss: 0.26924 | val_0_auc: 0.93314 |  0:00:17s\n",
      "epoch 9  | loss: 0.26895 | val_0_auc: 0.92813 |  0:00:19s\n",
      "epoch 10 | loss: 0.24513 | val_0_auc: 0.94888 |  0:00:21s\n",
      "epoch 11 | loss: 0.23467 | val_0_auc: 0.95987 |  0:00:23s\n",
      "epoch 12 | loss: 0.24629 | val_0_auc: 0.9466  |  0:00:25s\n",
      "epoch 13 | loss: 0.2395  | val_0_auc: 0.94711 |  0:00:27s\n",
      "epoch 14 | loss: 0.23105 | val_0_auc: 0.95591 |  0:00:29s\n",
      "epoch 15 | loss: 0.2429  | val_0_auc: 0.94727 |  0:00:31s\n",
      "epoch 16 | loss: 0.23611 | val_0_auc: 0.95685 |  0:00:32s\n",
      "epoch 17 | loss: 0.21434 | val_0_auc: 0.95659 |  0:00:34s\n",
      "epoch 18 | loss: 0.19459 | val_0_auc: 0.96662 |  0:00:36s\n",
      "epoch 19 | loss: 0.19208 | val_0_auc: 0.96818 |  0:00:38s\n",
      "epoch 20 | loss: 0.18443 | val_0_auc: 0.98062 |  0:00:40s\n",
      "epoch 21 | loss: 0.18172 | val_0_auc: 0.98239 |  0:00:42s\n",
      "epoch 22 | loss: 0.17988 | val_0_auc: 0.98029 |  0:00:44s\n",
      "epoch 23 | loss: 0.17191 | val_0_auc: 0.97825 |  0:00:46s\n",
      "epoch 24 | loss: 0.16734 | val_0_auc: 0.9818  |  0:00:48s\n",
      "epoch 25 | loss: 0.15979 | val_0_auc: 0.985   |  0:00:50s\n",
      "epoch 26 | loss: 0.17226 | val_0_auc: 0.9818  |  0:00:52s\n",
      "epoch 27 | loss: 0.15646 | val_0_auc: 0.98549 |  0:00:54s\n",
      "epoch 28 | loss: 0.15091 | val_0_auc: 0.98587 |  0:00:56s\n",
      "epoch 29 | loss: 0.14877 | val_0_auc: 0.97842 |  0:00:58s\n",
      "epoch 30 | loss: 0.14788 | val_0_auc: 0.98146 |  0:01:00s\n",
      "epoch 31 | loss: 0.14208 | val_0_auc: 0.98068 |  0:01:02s\n",
      "epoch 32 | loss: 0.14661 | val_0_auc: 0.98583 |  0:01:03s\n",
      "epoch 33 | loss: 0.13157 | val_0_auc: 0.98726 |  0:01:05s\n",
      "epoch 34 | loss: 0.13376 | val_0_auc: 0.98846 |  0:01:07s\n",
      "epoch 35 | loss: 0.13153 | val_0_auc: 0.98964 |  0:01:09s\n",
      "epoch 36 | loss: 0.13164 | val_0_auc: 0.99103 |  0:01:11s\n",
      "epoch 37 | loss: 0.13252 | val_0_auc: 0.98703 |  0:01:13s\n",
      "epoch 38 | loss: 0.13145 | val_0_auc: 0.98649 |  0:01:15s\n",
      "epoch 39 | loss: 0.13805 | val_0_auc: 0.97816 |  0:01:17s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99103\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99103215  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 78.2308\n",
      "Function value obtained: -0.9910\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.909279063470153, 'lambda_sparse': 0.08453422030733973, 'n_steps': 3, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.75108 | val_0_auc: 0.82381 |  0:00:00s\n",
      "epoch 1  | loss: 0.49677 | val_0_auc: 0.86489 |  0:00:01s\n",
      "epoch 2  | loss: 0.41542 | val_0_auc: 0.90502 |  0:00:02s\n",
      "epoch 3  | loss: 0.33296 | val_0_auc: 0.94428 |  0:00:03s\n",
      "epoch 4  | loss: 0.27479 | val_0_auc: 0.95195 |  0:00:03s\n",
      "epoch 5  | loss: 0.23407 | val_0_auc: 0.95831 |  0:00:04s\n",
      "epoch 6  | loss: 0.2211  | val_0_auc: 0.96353 |  0:00:05s\n",
      "epoch 7  | loss: 0.21501 | val_0_auc: 0.96769 |  0:00:05s\n",
      "epoch 8  | loss: 0.20391 | val_0_auc: 0.97339 |  0:00:06s\n",
      "epoch 9  | loss: 0.20307 | val_0_auc: 0.96997 |  0:00:07s\n",
      "epoch 10 | loss: 0.19702 | val_0_auc: 0.96966 |  0:00:08s\n",
      "epoch 11 | loss: 0.18791 | val_0_auc: 0.97526 |  0:00:08s\n",
      "epoch 12 | loss: 0.17907 | val_0_auc: 0.98401 |  0:00:09s\n",
      "epoch 13 | loss: 0.17069 | val_0_auc: 0.98141 |  0:00:10s\n",
      "epoch 14 | loss: 0.15801 | val_0_auc: 0.98433 |  0:00:11s\n",
      "epoch 15 | loss: 0.14741 | val_0_auc: 0.98729 |  0:00:11s\n",
      "epoch 16 | loss: 0.13948 | val_0_auc: 0.9914  |  0:00:12s\n",
      "epoch 17 | loss: 0.13068 | val_0_auc: 0.98926 |  0:00:13s\n",
      "epoch 18 | loss: 0.13237 | val_0_auc: 0.98983 |  0:00:13s\n",
      "epoch 19 | loss: 0.12531 | val_0_auc: 0.99177 |  0:00:14s\n",
      "epoch 20 | loss: 0.12378 | val_0_auc: 0.99113 |  0:00:15s\n",
      "epoch 21 | loss: 0.11729 | val_0_auc: 0.99273 |  0:00:16s\n",
      "epoch 22 | loss: 0.12356 | val_0_auc: 0.99396 |  0:00:16s\n",
      "epoch 23 | loss: 0.11906 | val_0_auc: 0.99283 |  0:00:17s\n",
      "epoch 24 | loss: 0.11298 | val_0_auc: 0.99604 |  0:00:18s\n",
      "epoch 25 | loss: 0.10772 | val_0_auc: 0.99217 |  0:00:19s\n",
      "epoch 26 | loss: 0.12047 | val_0_auc: 0.9916  |  0:00:19s\n",
      "epoch 27 | loss: 0.10571 | val_0_auc: 0.99497 |  0:00:20s\n",
      "epoch 28 | loss: 0.10273 | val_0_auc: 0.99492 |  0:00:21s\n",
      "epoch 29 | loss: 0.09846 | val_0_auc: 0.99515 |  0:00:21s\n",
      "epoch 30 | loss: 0.09374 | val_0_auc: 0.99573 |  0:00:22s\n",
      "epoch 31 | loss: 0.09847 | val_0_auc: 0.99547 |  0:00:23s\n",
      "epoch 32 | loss: 0.09396 | val_0_auc: 0.99475 |  0:00:24s\n",
      "epoch 33 | loss: 0.09578 | val_0_auc: 0.99615 |  0:00:24s\n",
      "epoch 34 | loss: 0.09401 | val_0_auc: 0.99725 |  0:00:25s\n",
      "epoch 35 | loss: 0.08676 | val_0_auc: 0.99609 |  0:00:26s\n",
      "epoch 36 | loss: 0.0915  | val_0_auc: 0.99473 |  0:00:27s\n",
      "epoch 37 | loss: 0.08769 | val_0_auc: 0.99449 |  0:00:27s\n",
      "epoch 38 | loss: 0.08726 | val_0_auc: 0.99541 |  0:00:28s\n",
      "epoch 39 | loss: 0.08779 | val_0_auc: 0.99401 |  0:00:29s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.99725\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99725105  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 29.5497\n",
      "Function value obtained: -0.9973\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0339222613321868, 'lambda_sparse': 0.05754918820711136, 'n_steps': 7, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.30687 | val_0_auc: 0.88078 |  0:00:02s\n",
      "epoch 1  | loss: 0.44888 | val_0_auc: 0.89036 |  0:00:04s\n",
      "epoch 2  | loss: 0.35607 | val_0_auc: 0.94623 |  0:00:06s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.29747 | val_0_auc: 0.94666 |  0:00:08s\n",
      "epoch 4  | loss: 0.27547 | val_0_auc: 0.96455 |  0:00:11s\n",
      "epoch 5  | loss: 0.2475  | val_0_auc: 0.97412 |  0:00:13s\n",
      "epoch 6  | loss: 0.23313 | val_0_auc: 0.9747  |  0:00:15s\n",
      "epoch 7  | loss: 0.21342 | val_0_auc: 0.979   |  0:00:17s\n",
      "epoch 8  | loss: 0.20888 | val_0_auc: 0.98116 |  0:00:19s\n",
      "epoch 9  | loss: 0.20097 | val_0_auc: 0.98246 |  0:00:22s\n",
      "epoch 10 | loss: 0.22932 | val_0_auc: 0.98503 |  0:00:24s\n",
      "epoch 11 | loss: 0.19709 | val_0_auc: 0.98965 |  0:00:26s\n",
      "epoch 12 | loss: 0.18347 | val_0_auc: 0.99033 |  0:00:28s\n",
      "epoch 13 | loss: 0.16181 | val_0_auc: 0.99152 |  0:00:31s\n",
      "epoch 14 | loss: 0.14833 | val_0_auc: 0.99013 |  0:00:33s\n",
      "epoch 15 | loss: 0.15796 | val_0_auc: 0.99315 |  0:00:35s\n",
      "epoch 16 | loss: 0.15024 | val_0_auc: 0.98838 |  0:00:37s\n",
      "epoch 17 | loss: 0.1435  | val_0_auc: 0.99064 |  0:00:39s\n",
      "epoch 18 | loss: 0.14877 | val_0_auc: 0.99207 |  0:00:42s\n",
      "epoch 19 | loss: 0.14435 | val_0_auc: 0.99176 |  0:00:44s\n",
      "epoch 20 | loss: 0.14339 | val_0_auc: 0.99245 |  0:00:46s\n",
      "epoch 21 | loss: 0.14325 | val_0_auc: 0.99086 |  0:00:48s\n",
      "epoch 22 | loss: 0.14615 | val_0_auc: 0.99205 |  0:00:50s\n",
      "epoch 23 | loss: 0.13897 | val_0_auc: 0.9949  |  0:00:52s\n",
      "epoch 24 | loss: 0.12914 | val_0_auc: 0.99463 |  0:00:55s\n",
      "epoch 25 | loss: 0.13573 | val_0_auc: 0.99237 |  0:00:57s\n",
      "epoch 26 | loss: 0.13449 | val_0_auc: 0.9909  |  0:00:59s\n",
      "epoch 27 | loss: 0.12995 | val_0_auc: 0.9934  |  0:01:01s\n",
      "epoch 28 | loss: 0.12943 | val_0_auc: 0.99437 |  0:01:03s\n",
      "epoch 29 | loss: 0.12219 | val_0_auc: 0.99505 |  0:01:06s\n",
      "epoch 30 | loss: 0.11809 | val_0_auc: 0.99431 |  0:01:08s\n",
      "epoch 31 | loss: 0.1162  | val_0_auc: 0.99387 |  0:01:10s\n",
      "epoch 32 | loss: 0.12452 | val_0_auc: 0.9938  |  0:01:12s\n",
      "epoch 33 | loss: 0.1152  | val_0_auc: 0.99468 |  0:01:14s\n",
      "epoch 34 | loss: 0.11515 | val_0_auc: 0.99505 |  0:01:16s\n",
      "epoch 35 | loss: 0.11998 | val_0_auc: 0.99344 |  0:01:19s\n",
      "epoch 36 | loss: 0.13263 | val_0_auc: 0.99496 |  0:01:21s\n",
      "epoch 37 | loss: 0.12818 | val_0_auc: 0.99655 |  0:01:23s\n",
      "epoch 38 | loss: 0.11888 | val_0_auc: 0.99546 |  0:01:25s\n",
      "epoch 39 | loss: 0.12389 | val_0_auc: 0.99597 |  0:01:27s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99655\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99655498  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 88.7193\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6103131003811515, 'lambda_sparse': 0.09589514283951854, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.31786 | val_0_auc: 0.79924 |  0:00:01s\n",
      "epoch 1  | loss: 0.64491 | val_0_auc: 0.85918 |  0:00:03s\n",
      "epoch 2  | loss: 0.47504 | val_0_auc: 0.90611 |  0:00:05s\n",
      "epoch 3  | loss: 0.36354 | val_0_auc: 0.93742 |  0:00:06s\n",
      "epoch 4  | loss: 0.29974 | val_0_auc: 0.95402 |  0:00:08s\n",
      "epoch 5  | loss: 0.25701 | val_0_auc: 0.96377 |  0:00:10s\n",
      "epoch 6  | loss: 0.24602 | val_0_auc: 0.96933 |  0:00:11s\n",
      "epoch 7  | loss: 0.21469 | val_0_auc: 0.97593 |  0:00:13s\n",
      "epoch 8  | loss: 0.22313 | val_0_auc: 0.97645 |  0:00:15s\n",
      "epoch 9  | loss: 0.22457 | val_0_auc: 0.98054 |  0:00:16s\n",
      "epoch 10 | loss: 0.21138 | val_0_auc: 0.97276 |  0:00:18s\n",
      "epoch 11 | loss: 0.19734 | val_0_auc: 0.98197 |  0:00:20s\n",
      "epoch 12 | loss: 0.2023  | val_0_auc: 0.98167 |  0:00:21s\n",
      "epoch 13 | loss: 0.18815 | val_0_auc: 0.98631 |  0:00:23s\n",
      "epoch 14 | loss: 0.18666 | val_0_auc: 0.98841 |  0:00:25s\n",
      "epoch 15 | loss: 0.16915 | val_0_auc: 0.98588 |  0:00:26s\n",
      "epoch 16 | loss: 0.16022 | val_0_auc: 0.98943 |  0:00:28s\n",
      "epoch 17 | loss: 0.14112 | val_0_auc: 0.99035 |  0:00:30s\n",
      "epoch 18 | loss: 0.13024 | val_0_auc: 0.9872  |  0:00:31s\n",
      "epoch 19 | loss: 0.13064 | val_0_auc: 0.98829 |  0:00:33s\n",
      "epoch 20 | loss: 0.13689 | val_0_auc: 0.99346 |  0:00:35s\n",
      "epoch 21 | loss: 0.12932 | val_0_auc: 0.99383 |  0:00:36s\n",
      "epoch 22 | loss: 0.12522 | val_0_auc: 0.9945  |  0:00:38s\n",
      "epoch 23 | loss: 0.13319 | val_0_auc: 0.99474 |  0:00:40s\n",
      "epoch 24 | loss: 0.11856 | val_0_auc: 0.99329 |  0:00:41s\n",
      "epoch 25 | loss: 0.12276 | val_0_auc: 0.99402 |  0:00:43s\n",
      "epoch 26 | loss: 0.12555 | val_0_auc: 0.98945 |  0:00:45s\n",
      "epoch 27 | loss: 0.12017 | val_0_auc: 0.99391 |  0:00:46s\n",
      "epoch 28 | loss: 0.12933 | val_0_auc: 0.99494 |  0:00:48s\n",
      "epoch 29 | loss: 0.12089 | val_0_auc: 0.99608 |  0:00:50s\n",
      "epoch 30 | loss: 0.12256 | val_0_auc: 0.99426 |  0:00:51s\n",
      "epoch 31 | loss: 0.12564 | val_0_auc: 0.99457 |  0:00:53s\n",
      "epoch 32 | loss: 0.11061 | val_0_auc: 0.99391 |  0:00:55s\n",
      "epoch 33 | loss: 0.10942 | val_0_auc: 0.99592 |  0:00:57s\n",
      "epoch 34 | loss: 0.1157  | val_0_auc: 0.99307 |  0:00:58s\n",
      "epoch 35 | loss: 0.11762 | val_0_auc: 0.99173 |  0:01:00s\n",
      "epoch 36 | loss: 0.11371 | val_0_auc: 0.991   |  0:01:02s\n",
      "epoch 37 | loss: 0.11539 | val_0_auc: 0.99077 |  0:01:03s\n",
      "epoch 38 | loss: 0.11712 | val_0_auc: 0.99129 |  0:01:05s\n",
      "epoch 39 | loss: 0.11652 | val_0_auc: 0.99036 |  0:01:06s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_auc = 0.99608\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99607916  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 67.6030\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.644791442603186, 'lambda_sparse': 0.053118576013960764, 'n_steps': 5, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.22316 | val_0_auc: 0.84015 |  0:00:01s\n",
      "epoch 1  | loss: 0.63735 | val_0_auc: 0.83834 |  0:00:03s\n",
      "epoch 2  | loss: 0.48644 | val_0_auc: 0.87299 |  0:00:05s\n",
      "epoch 3  | loss: 0.41755 | val_0_auc: 0.90183 |  0:00:06s\n",
      "epoch 4  | loss: 0.39175 | val_0_auc: 0.90453 |  0:00:08s\n",
      "epoch 5  | loss: 0.35245 | val_0_auc: 0.92695 |  0:00:10s\n",
      "epoch 6  | loss: 0.32195 | val_0_auc: 0.93345 |  0:00:11s\n",
      "epoch 7  | loss: 0.31065 | val_0_auc: 0.93152 |  0:00:13s\n",
      "epoch 8  | loss: 0.33574 | val_0_auc: 0.93777 |  0:00:15s\n",
      "epoch 9  | loss: 0.33406 | val_0_auc: 0.93681 |  0:00:16s\n",
      "epoch 10 | loss: 0.30878 | val_0_auc: 0.9514  |  0:00:18s\n",
      "epoch 11 | loss: 0.28143 | val_0_auc: 0.94334 |  0:00:20s\n",
      "epoch 12 | loss: 0.25634 | val_0_auc: 0.95851 |  0:00:21s\n",
      "epoch 13 | loss: 0.24475 | val_0_auc: 0.95413 |  0:00:23s\n",
      "epoch 14 | loss: 0.22877 | val_0_auc: 0.9595  |  0:00:25s\n",
      "epoch 15 | loss: 0.22853 | val_0_auc: 0.96188 |  0:00:26s\n",
      "epoch 16 | loss: 0.24767 | val_0_auc: 0.96656 |  0:00:28s\n",
      "epoch 17 | loss: 0.24651 | val_0_auc: 0.95321 |  0:00:30s\n",
      "epoch 18 | loss: 0.24895 | val_0_auc: 0.94709 |  0:00:31s\n",
      "epoch 19 | loss: 0.25109 | val_0_auc: 0.95869 |  0:00:33s\n",
      "epoch 20 | loss: 0.26754 | val_0_auc: 0.95284 |  0:00:35s\n",
      "epoch 21 | loss: 0.24838 | val_0_auc: 0.96433 |  0:00:36s\n",
      "epoch 22 | loss: 0.23758 | val_0_auc: 0.97354 |  0:00:38s\n",
      "epoch 23 | loss: 0.23004 | val_0_auc: 0.966   |  0:00:40s\n",
      "epoch 24 | loss: 0.20522 | val_0_auc: 0.97806 |  0:00:41s\n",
      "epoch 25 | loss: 0.20071 | val_0_auc: 0.9743  |  0:00:43s\n",
      "epoch 26 | loss: 0.20251 | val_0_auc: 0.97317 |  0:00:45s\n",
      "epoch 27 | loss: 0.19742 | val_0_auc: 0.97675 |  0:00:46s\n",
      "epoch 28 | loss: 0.18715 | val_0_auc: 0.97302 |  0:00:48s\n",
      "epoch 29 | loss: 0.17687 | val_0_auc: 0.98156 |  0:00:50s\n",
      "epoch 30 | loss: 0.17282 | val_0_auc: 0.97772 |  0:00:51s\n",
      "epoch 31 | loss: 0.18114 | val_0_auc: 0.97921 |  0:00:53s\n",
      "epoch 32 | loss: 0.16815 | val_0_auc: 0.98353 |  0:00:55s\n",
      "epoch 33 | loss: 0.16057 | val_0_auc: 0.97957 |  0:00:56s\n",
      "epoch 34 | loss: 0.15876 | val_0_auc: 0.9859  |  0:00:58s\n",
      "epoch 35 | loss: 0.15373 | val_0_auc: 0.98704 |  0:01:00s\n",
      "epoch 36 | loss: 0.14391 | val_0_auc: 0.99037 |  0:01:01s\n",
      "epoch 37 | loss: 0.14521 | val_0_auc: 0.98798 |  0:01:03s\n",
      "epoch 38 | loss: 0.15432 | val_0_auc: 0.98473 |  0:01:05s\n",
      "epoch 39 | loss: 0.14156 | val_0_auc: 0.98918 |  0:01:06s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 36 and best_val_0_auc = 0.99037\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99036933  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 67.5904\n",
      "Function value obtained: -0.9904\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4947794960440408, 'lambda_sparse': 0.055747646006232165, 'n_steps': 4, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.76997 | val_0_auc: 0.88203 |  0:00:01s\n",
      "epoch 1  | loss: 0.3855  | val_0_auc: 0.91374 |  0:00:02s\n",
      "epoch 2  | loss: 0.28017 | val_0_auc: 0.94501 |  0:00:04s\n",
      "epoch 3  | loss: 0.24901 | val_0_auc: 0.96061 |  0:00:05s\n",
      "epoch 4  | loss: 0.21367 | val_0_auc: 0.96651 |  0:00:07s\n",
      "epoch 5  | loss: 0.18624 | val_0_auc: 0.96807 |  0:00:08s\n",
      "epoch 6  | loss: 0.18922 | val_0_auc: 0.97914 |  0:00:09s\n",
      "epoch 7  | loss: 0.17575 | val_0_auc: 0.9737  |  0:00:11s\n",
      "epoch 8  | loss: 0.16029 | val_0_auc: 0.98373 |  0:00:12s\n",
      "epoch 9  | loss: 0.15837 | val_0_auc: 0.98202 |  0:00:14s\n",
      "epoch 10 | loss: 0.15322 | val_0_auc: 0.98597 |  0:00:15s\n",
      "epoch 11 | loss: 0.13683 | val_0_auc: 0.98921 |  0:00:16s\n",
      "epoch 12 | loss: 0.12487 | val_0_auc: 0.99117 |  0:00:18s\n",
      "epoch 13 | loss: 0.12744 | val_0_auc: 0.98983 |  0:00:19s\n",
      "epoch 14 | loss: 0.12018 | val_0_auc: 0.99154 |  0:00:21s\n",
      "epoch 15 | loss: 0.13848 | val_0_auc: 0.9812  |  0:00:22s\n",
      "epoch 16 | loss: 0.12253 | val_0_auc: 0.99352 |  0:00:23s\n",
      "epoch 17 | loss: 0.11384 | val_0_auc: 0.99296 |  0:00:25s\n",
      "epoch 18 | loss: 0.12834 | val_0_auc: 0.99329 |  0:00:26s\n",
      "epoch 19 | loss: 0.1204  | val_0_auc: 0.99118 |  0:00:28s\n",
      "epoch 20 | loss: 0.11435 | val_0_auc: 0.99397 |  0:00:29s\n",
      "epoch 21 | loss: 0.10578 | val_0_auc: 0.9912  |  0:00:31s\n",
      "epoch 22 | loss: 0.11043 | val_0_auc: 0.99254 |  0:00:32s\n",
      "epoch 23 | loss: 0.10961 | val_0_auc: 0.99402 |  0:00:33s\n",
      "epoch 24 | loss: 0.11069 | val_0_auc: 0.9929  |  0:00:35s\n",
      "epoch 25 | loss: 0.12522 | val_0_auc: 0.99389 |  0:00:36s\n",
      "epoch 26 | loss: 0.10944 | val_0_auc: 0.99385 |  0:00:38s\n",
      "epoch 27 | loss: 0.11273 | val_0_auc: 0.98863 |  0:00:39s\n",
      "epoch 28 | loss: 0.10652 | val_0_auc: 0.99081 |  0:00:40s\n",
      "epoch 29 | loss: 0.10667 | val_0_auc: 0.99289 |  0:00:42s\n",
      "epoch 30 | loss: 0.09947 | val_0_auc: 0.9921  |  0:00:43s\n",
      "epoch 31 | loss: 0.09524 | val_0_auc: 0.98784 |  0:00:45s\n",
      "epoch 32 | loss: 0.10139 | val_0_auc: 0.99633 |  0:00:46s\n",
      "epoch 33 | loss: 0.10125 | val_0_auc: 0.99412 |  0:00:47s\n",
      "epoch 34 | loss: 0.09831 | val_0_auc: 0.99558 |  0:00:49s\n",
      "epoch 35 | loss: 0.10927 | val_0_auc: 0.99579 |  0:00:50s\n",
      "epoch 36 | loss: 0.10471 | val_0_auc: 0.99519 |  0:00:52s\n",
      "epoch 37 | loss: 0.09113 | val_0_auc: 0.99318 |  0:00:53s\n",
      "epoch 38 | loss: 0.09647 | val_0_auc: 0.99593 |  0:00:54s\n",
      "epoch 39 | loss: 0.0932  | val_0_auc: 0.99484 |  0:00:56s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 32 and best_val_0_auc = 0.99633\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9963285  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 56.9152\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.510226421310916, 'lambda_sparse': 0.04319340639419684, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.04539 | val_0_auc: 0.81191 |  0:00:01s\n",
      "epoch 1  | loss: 0.4837  | val_0_auc: 0.86067 |  0:00:03s\n",
      "epoch 2  | loss: 0.38878 | val_0_auc: 0.88136 |  0:00:05s\n",
      "epoch 3  | loss: 0.40649 | val_0_auc: 0.89096 |  0:00:06s\n",
      "epoch 4  | loss: 0.39551 | val_0_auc: 0.89042 |  0:00:08s\n",
      "epoch 5  | loss: 0.36412 | val_0_auc: 0.8952  |  0:00:10s\n",
      "epoch 6  | loss: 0.37101 | val_0_auc: 0.8995  |  0:00:12s\n",
      "epoch 7  | loss: 0.37614 | val_0_auc: 0.92516 |  0:00:13s\n",
      "epoch 8  | loss: 0.32752 | val_0_auc: 0.94045 |  0:00:15s\n",
      "epoch 9  | loss: 0.30803 | val_0_auc: 0.94626 |  0:00:17s\n",
      "epoch 10 | loss: 0.29617 | val_0_auc: 0.93352 |  0:00:18s\n",
      "epoch 11 | loss: 0.30884 | val_0_auc: 0.93704 |  0:00:20s\n",
      "epoch 12 | loss: 0.27518 | val_0_auc: 0.93955 |  0:00:22s\n",
      "epoch 13 | loss: 0.28129 | val_0_auc: 0.9413  |  0:00:24s\n",
      "epoch 14 | loss: 0.26    | val_0_auc: 0.92945 |  0:00:25s\n",
      "epoch 15 | loss: 0.26441 | val_0_auc: 0.9459  |  0:00:27s\n",
      "epoch 16 | loss: 0.26133 | val_0_auc: 0.941   |  0:00:29s\n",
      "epoch 17 | loss: 0.26352 | val_0_auc: 0.94764 |  0:00:30s\n",
      "epoch 18 | loss: 0.25795 | val_0_auc: 0.95253 |  0:00:32s\n",
      "epoch 19 | loss: 0.25412 | val_0_auc: 0.9439  |  0:00:34s\n",
      "epoch 20 | loss: 0.24924 | val_0_auc: 0.94897 |  0:00:36s\n",
      "epoch 21 | loss: 0.261   | val_0_auc: 0.9422  |  0:00:37s\n",
      "epoch 22 | loss: 0.26433 | val_0_auc: 0.93549 |  0:00:39s\n",
      "epoch 23 | loss: 0.27265 | val_0_auc: 0.94756 |  0:00:41s\n",
      "epoch 24 | loss: 0.25741 | val_0_auc: 0.95472 |  0:00:42s\n",
      "epoch 25 | loss: 0.24447 | val_0_auc: 0.94778 |  0:00:44s\n",
      "epoch 26 | loss: 0.23266 | val_0_auc: 0.96643 |  0:00:46s\n",
      "epoch 27 | loss: 0.22938 | val_0_auc: 0.9666  |  0:00:48s\n",
      "epoch 28 | loss: 0.32067 | val_0_auc: 0.96187 |  0:00:49s\n",
      "epoch 29 | loss: 0.29104 | val_0_auc: 0.96338 |  0:00:51s\n",
      "epoch 30 | loss: 0.22119 | val_0_auc: 0.9747  |  0:00:53s\n",
      "epoch 31 | loss: 0.19455 | val_0_auc: 0.97765 |  0:00:54s\n",
      "epoch 32 | loss: 0.18939 | val_0_auc: 0.9795  |  0:00:56s\n",
      "epoch 33 | loss: 0.1797  | val_0_auc: 0.97659 |  0:00:58s\n",
      "epoch 34 | loss: 0.17932 | val_0_auc: 0.97366 |  0:00:59s\n",
      "epoch 35 | loss: 0.17285 | val_0_auc: 0.98281 |  0:01:01s\n",
      "epoch 36 | loss: 0.15423 | val_0_auc: 0.98212 |  0:01:03s\n",
      "epoch 37 | loss: 0.1539  | val_0_auc: 0.98222 |  0:01:05s\n",
      "epoch 38 | loss: 0.14541 | val_0_auc: 0.98179 |  0:01:06s\n",
      "epoch 39 | loss: 0.14467 | val_0_auc: 0.98291 |  0:01:08s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.98291\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9829079  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 69.0200\n",
      "Function value obtained: -0.9829\n",
      "Current minimum: -0.9973\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7948676512987956, 'lambda_sparse': 0.0302566800754811, 'n_steps': 5, 'n_a': 8, 'n_d': 8, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.95082 | val_0_auc: 0.68393 |  0:00:00s\n",
      "epoch 1  | loss: 0.645   | val_0_auc: 0.78845 |  0:00:01s\n",
      "epoch 2  | loss: 0.54342 | val_0_auc: 0.84565 |  0:00:02s\n",
      "epoch 3  | loss: 0.47589 | val_0_auc: 0.87133 |  0:00:03s\n",
      "epoch 4  | loss: 0.40971 | val_0_auc: 0.87004 |  0:00:04s\n",
      "epoch 5  | loss: 0.40377 | val_0_auc: 0.88409 |  0:00:05s\n",
      "epoch 6  | loss: 0.36543 | val_0_auc: 0.89764 |  0:00:06s\n",
      "epoch 7  | loss: 0.36136 | val_0_auc: 0.89961 |  0:00:07s\n",
      "epoch 8  | loss: 0.33195 | val_0_auc: 0.9004  |  0:00:07s\n",
      "epoch 9  | loss: 0.31816 | val_0_auc: 0.91422 |  0:00:08s\n",
      "epoch 10 | loss: 0.31275 | val_0_auc: 0.91022 |  0:00:09s\n",
      "epoch 11 | loss: 0.28918 | val_0_auc: 0.92615 |  0:00:10s\n",
      "epoch 12 | loss: 0.28326 | val_0_auc: 0.93179 |  0:00:11s\n",
      "epoch 13 | loss: 0.27573 | val_0_auc: 0.92935 |  0:00:12s\n",
      "epoch 14 | loss: 0.27614 | val_0_auc: 0.93136 |  0:00:13s\n",
      "epoch 15 | loss: 0.2677  | val_0_auc: 0.93093 |  0:00:14s\n",
      "epoch 16 | loss: 0.25496 | val_0_auc: 0.93191 |  0:00:14s\n",
      "epoch 17 | loss: 0.24409 | val_0_auc: 0.93254 |  0:00:15s\n",
      "epoch 18 | loss: 0.23619 | val_0_auc: 0.93887 |  0:00:16s\n",
      "epoch 19 | loss: 0.23818 | val_0_auc: 0.93566 |  0:00:17s\n",
      "epoch 20 | loss: 0.24087 | val_0_auc: 0.93938 |  0:00:18s\n",
      "epoch 21 | loss: 0.24574 | val_0_auc: 0.93997 |  0:00:19s\n",
      "epoch 22 | loss: 0.25775 | val_0_auc: 0.93673 |  0:00:20s\n",
      "epoch 23 | loss: 0.23818 | val_0_auc: 0.94577 |  0:00:20s\n",
      "epoch 24 | loss: 0.22698 | val_0_auc: 0.95469 |  0:00:21s\n",
      "epoch 25 | loss: 0.23194 | val_0_auc: 0.9582  |  0:00:22s\n",
      "epoch 26 | loss: 0.22205 | val_0_auc: 0.95595 |  0:00:23s\n",
      "epoch 27 | loss: 0.23123 | val_0_auc: 0.95466 |  0:00:24s\n",
      "epoch 28 | loss: 0.22396 | val_0_auc: 0.96057 |  0:00:25s\n",
      "epoch 29 | loss: 0.21674 | val_0_auc: 0.96939 |  0:00:26s\n",
      "epoch 30 | loss: 0.20457 | val_0_auc: 0.96197 |  0:00:27s\n",
      "epoch 31 | loss: 0.20793 | val_0_auc: 0.96444 |  0:00:27s\n",
      "epoch 32 | loss: 0.20548 | val_0_auc: 0.96664 |  0:00:28s\n",
      "epoch 33 | loss: 0.20373 | val_0_auc: 0.95697 |  0:00:29s\n",
      "epoch 34 | loss: 0.21064 | val_0_auc: 0.96271 |  0:00:30s\n",
      "epoch 35 | loss: 0.21655 | val_0_auc: 0.9692  |  0:00:31s\n",
      "epoch 36 | loss: 0.20468 | val_0_auc: 0.96937 |  0:00:32s\n",
      "epoch 37 | loss: 0.20335 | val_0_auc: 0.97056 |  0:00:33s\n",
      "epoch 38 | loss: 0.20838 | val_0_auc: 0.97291 |  0:00:34s\n",
      "epoch 39 | loss: 0.20025 | val_0_auc: 0.96694 |  0:00:34s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.97291\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.97290738  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 37.8283\n",
      "Function value obtained: -0.9729\n",
      "Current minimum: -0.9973\n",
      "STARTED: syn6\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.30787245263576934, 'max_depth': 13, 'n_estimators': 533}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.9965322  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.0674\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.45595377463022, 'max_depth': 6, 'n_estimators': 695}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.996175  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.0454\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4658886704767876, 'max_depth': 6, 'n_estimators': 413}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99644055  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.0591\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9965\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3490336584679883, 'max_depth': 12, 'n_estimators': 344}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99667591  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.0878\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9967\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.47667387490720425, 'max_depth': 6, 'n_estimators': 210}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99690397  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.0495\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.37119342516837484, 'max_depth': 5, 'n_estimators': 527}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99605108  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.0587\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.461435147914139, 'max_depth': 8, 'n_estimators': 305}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99578032  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.0705\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.11352937944898184, 'max_depth': 14, 'n_estimators': 904}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99661551  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.1747\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48840684654259203, 'max_depth': 7, 'n_estimators': 328}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99610523  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.0643\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.272910510241714, 'max_depth': 13, 'n_estimators': 331}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99641973  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.1066\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48608509897365004, 'max_depth': 6, 'n_estimators': 954}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.996628  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.0682\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3896764145097532, 'max_depth': 5, 'n_estimators': 891}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99648221  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.0604\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.40028389170643963, 'max_depth': 14, 'n_estimators': 944}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99670611  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.0581\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4213462969572036, 'max_depth': 3, 'n_estimators': 204}\n",
      "AUC:  0.99582614  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.0450\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.46329296031850353, 'max_depth': 11, 'n_estimators': 609}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99560745  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.0521\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1911040149797038, 'max_depth': 6, 'n_estimators': 875}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99718827  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.1070\n",
      "Function value obtained: -0.9972\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.22584366097142822, 'max_depth': 5, 'n_estimators': 629}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99693  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.0839\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10670801982952494, 'max_depth': 13, 'n_estimators': 602}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99700498  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.1317\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.30378819388021105, 'max_depth': 14, 'n_estimators': 611}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99643222  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.0993\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4431562440993676, 'max_depth': 12, 'n_estimators': 117}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99515757  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.0464\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.27113407517175087, 'max_depth': 3, 'n_estimators': 618}\n",
      "AUC:  0.99646867  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.0610\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2478471344623816, 'max_depth': 9, 'n_estimators': 160}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99619271  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.0854\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4895589238672554, 'max_depth': 9, 'n_estimators': 973}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99574179  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.0472\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4448081007895651, 'max_depth': 14, 'n_estimators': 867}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99598235  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.0537\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10395676704673273, 'max_depth': 11, 'n_estimators': 258}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99664883  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.1689\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43256684496641273, 'max_depth': 15, 'n_estimators': 431}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99612189  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.0839\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19036711458334774, 'max_depth': 10, 'n_estimators': 698}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99685502  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.1118\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3561189369315304, 'max_depth': 9, 'n_estimators': 107}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99578865  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.0651\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.39426704644354715, 'max_depth': 5, 'n_estimators': 492}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99643847  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.0545\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3655564232551317, 'max_depth': 5, 'n_estimators': 986}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.99618229  of boosting iteration \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.9065\n",
      "Function value obtained: -0.9962\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6538187172750245, 'max_depth': 11, 'n_estimators': 282}\n",
      "[03:27:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9960365  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.2030\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9960\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.19800192875811898, 'max_depth': 5, 'n_estimators': 306}\n",
      "[03:27:07] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99717785  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.6087\n",
      "Function value obtained: -0.9972\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6428761984727557, 'max_depth': 14, 'n_estimators': 111}\n",
      "[03:27:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99544708  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.2379\n",
      "Function value obtained: -0.9954\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9679560315704135, 'max_depth': 12, 'n_estimators': 609}\n",
      "[03:27:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.9953471  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.1191\n",
      "Function value obtained: -0.9953\n",
      "Current minimum: -0.9972\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.103989684658078, 'max_depth': 5, 'n_estimators': 805}\n",
      "[03:27:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9976194  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.5484\n",
      "Function value obtained: -0.9976\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.30878575062832464, 'max_depth': 15, 'n_estimators': 601}\n",
      "[03:27:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99597193  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.2989\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3523204230675755, 'max_depth': 13, 'n_estimators': 548}\n",
      "[03:27:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99516174  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.2909\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9615836477378591, 'max_depth': 6, 'n_estimators': 666}\n",
      "[03:27:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.994212  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.0836\n",
      "Function value obtained: -0.9942\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14238769781032956, 'max_depth': 9, 'n_estimators': 453}\n",
      "[03:27:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99663842  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.3138\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1256558633506421, 'max_depth': 8, 'n_estimators': 905}\n",
      "[03:27:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99698624  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.3934\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5148422018809014, 'max_depth': 7, 'n_estimators': 768}\n",
      "[03:27:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99599276  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 0.1753\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.48915173004639456, 'max_depth': 7, 'n_estimators': 475}\n",
      "[03:27:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99673527  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 0.1778\n",
      "Function value obtained: -0.9967\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8701341404291698, 'max_depth': 4, 'n_estimators': 817}\n",
      "[03:27:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.9954981  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 0.0848\n",
      "Function value obtained: -0.9955\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4510894552578002, 'max_depth': 11, 'n_estimators': 614}\n",
      "[03:27:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99595527  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 0.1624\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4840661667767996, 'max_depth': 15, 'n_estimators': 242}\n",
      "[03:27:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99610731  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 0.2505\n",
      "Function value obtained: -0.9961\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.34228779047877683, 'max_depth': 13, 'n_estimators': 442}\n",
      "[03:27:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.996351  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 0.2847\n",
      "Function value obtained: -0.9964\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9935661638070498, 'max_depth': 10, 'n_estimators': 863}\n",
      "[03:27:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99541375  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 0.1320\n",
      "Function value obtained: -0.9954\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7843143836217543, 'max_depth': 10, 'n_estimators': 368}\n",
      "[03:27:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99584072  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 0.1626\n",
      "Function value obtained: -0.9958\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9464794280382905, 'max_depth': 7, 'n_estimators': 623}\n",
      "[03:27:11] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99472852  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 0.1729\n",
      "Function value obtained: -0.9947\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4516218269690544, 'max_depth': 8, 'n_estimators': 173}\n",
      "[03:27:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99629684  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 0.1659\n",
      "Function value obtained: -0.9963\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5174640057825435, 'max_depth': 11, 'n_estimators': 642}\n",
      "[03:27:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99645305  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 0.1790\n",
      "Function value obtained: -0.9965\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14400417435313456, 'max_depth': 8, 'n_estimators': 934}\n",
      "[03:27:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99703831  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 0.4273\n",
      "Function value obtained: -0.9970\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8091140361799009, 'max_depth': 11, 'n_estimators': 932}\n",
      "[03:27:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99585113  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 0.1576\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8616985087132987, 'max_depth': 5, 'n_estimators': 629}\n",
      "[03:27:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99482225  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 0.1409\n",
      "Function value obtained: -0.9948\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29202863784759203, 'max_depth': 4, 'n_estimators': 691}\n",
      "[03:27:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99690918  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 0.2619\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3644789171113981, 'max_depth': 8, 'n_estimators': 393}\n",
      "[03:27:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99559495  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 0.2174\n",
      "Function value obtained: -0.9956\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6972078182589933, 'max_depth': 5, 'n_estimators': 785}\n",
      "[03:27:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99589279  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 0.1525\n",
      "Function value obtained: -0.9959\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.14147573359129645, 'max_depth': 14, 'n_estimators': 206}\n",
      "[03:27:13] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99604899  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 0.4205\n",
      "Function value obtained: -0.9960\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9505177964318047, 'max_depth': 11, 'n_estimators': 740}\n",
      "[03:27:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.99544812  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 0.1437\n",
      "Function value obtained: -0.9954\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.44680119943203533, 'max_depth': 5, 'n_estimators': 557}\n",
      "[03:27:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99658843  of boosting iteration \n",
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 0.6134\n",
      "Function value obtained: -0.9966\n",
      "Current minimum: -0.9976\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0474779230157356, 'lambda_sparse': 0.024973109736170278, 'n_steps': 10, 'n_a': 16, 'n_d': 16, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.84009 | val_0_auc: 0.80438 |  0:00:01s\n",
      "epoch 1  | loss: 0.56379 | val_0_auc: 0.86008 |  0:00:03s\n",
      "epoch 2  | loss: 0.42653 | val_0_auc: 0.89975 |  0:00:05s\n",
      "epoch 3  | loss: 0.351   | val_0_auc: 0.89308 |  0:00:07s\n",
      "epoch 4  | loss: 0.30276 | val_0_auc: 0.91843 |  0:00:09s\n",
      "epoch 5  | loss: 0.28541 | val_0_auc: 0.93332 |  0:00:11s\n",
      "epoch 6  | loss: 0.27166 | val_0_auc: 0.94784 |  0:00:13s\n",
      "epoch 7  | loss: 0.2462  | val_0_auc: 0.9589  |  0:00:15s\n",
      "epoch 8  | loss: 0.24557 | val_0_auc: 0.96269 |  0:00:17s\n",
      "epoch 9  | loss: 0.25461 | val_0_auc: 0.96228 |  0:00:19s\n",
      "epoch 10 | loss: 0.23559 | val_0_auc: 0.96457 |  0:00:20s\n",
      "epoch 11 | loss: 0.23258 | val_0_auc: 0.95888 |  0:00:22s\n",
      "epoch 12 | loss: 0.22583 | val_0_auc: 0.96182 |  0:00:24s\n",
      "epoch 13 | loss: 0.24358 | val_0_auc: 0.96697 |  0:00:26s\n",
      "epoch 14 | loss: 0.21527 | val_0_auc: 0.97539 |  0:00:28s\n",
      "epoch 15 | loss: 0.22704 | val_0_auc: 0.97108 |  0:00:30s\n",
      "epoch 16 | loss: 0.21593 | val_0_auc: 0.97495 |  0:00:32s\n",
      "epoch 17 | loss: 0.19637 | val_0_auc: 0.96774 |  0:00:34s\n",
      "epoch 18 | loss: 0.20138 | val_0_auc: 0.96711 |  0:00:35s\n",
      "epoch 19 | loss: 0.18695 | val_0_auc: 0.97605 |  0:00:37s\n",
      "epoch 20 | loss: 0.19967 | val_0_auc: 0.97343 |  0:00:39s\n",
      "epoch 21 | loss: 0.19338 | val_0_auc: 0.97787 |  0:00:41s\n",
      "epoch 22 | loss: 0.18704 | val_0_auc: 0.9732  |  0:00:43s\n",
      "epoch 23 | loss: 0.18357 | val_0_auc: 0.97851 |  0:00:45s\n",
      "epoch 24 | loss: 0.18219 | val_0_auc: 0.97314 |  0:00:47s\n",
      "epoch 25 | loss: 0.17502 | val_0_auc: 0.98394 |  0:00:49s\n",
      "epoch 26 | loss: 0.15514 | val_0_auc: 0.97245 |  0:00:50s\n",
      "epoch 27 | loss: 0.15801 | val_0_auc: 0.98659 |  0:00:52s\n",
      "epoch 28 | loss: 0.15918 | val_0_auc: 0.98084 |  0:00:54s\n",
      "epoch 29 | loss: 0.15799 | val_0_auc: 0.98854 |  0:00:56s\n",
      "epoch 30 | loss: 0.15148 | val_0_auc: 0.98408 |  0:00:58s\n",
      "epoch 31 | loss: 0.15323 | val_0_auc: 0.9874  |  0:01:00s\n",
      "epoch 32 | loss: 0.14572 | val_0_auc: 0.98531 |  0:01:02s\n",
      "epoch 33 | loss: 0.142   | val_0_auc: 0.98823 |  0:01:04s\n",
      "epoch 34 | loss: 0.13552 | val_0_auc: 0.98727 |  0:01:06s\n",
      "epoch 35 | loss: 0.1337  | val_0_auc: 0.98819 |  0:01:07s\n",
      "epoch 36 | loss: 0.13651 | val_0_auc: 0.98712 |  0:01:09s\n",
      "epoch 37 | loss: 0.13849 | val_0_auc: 0.98253 |  0:01:11s\n",
      "epoch 38 | loss: 0.13434 | val_0_auc: 0.98962 |  0:01:13s\n",
      "epoch 39 | loss: 0.13086 | val_0_auc: 0.98649 |  0:01:15s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.98962\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9896195  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 75.9114\n",
      "Function value obtained: -0.9896\n",
      "Current minimum: -0.9896\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.823248524013499, 'lambda_sparse': 0.04771504357992963, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.8515  | val_0_auc: 0.69264 |  0:00:01s\n",
      "epoch 1  | loss: 0.54962 | val_0_auc: 0.82526 |  0:00:02s\n",
      "epoch 2  | loss: 0.42835 | val_0_auc: 0.89558 |  0:00:03s\n",
      "epoch 3  | loss: 0.38343 | val_0_auc: 0.89855 |  0:00:04s\n",
      "epoch 4  | loss: 0.36264 | val_0_auc: 0.90898 |  0:00:06s\n",
      "epoch 5  | loss: 0.37997 | val_0_auc: 0.88458 |  0:00:07s\n",
      "epoch 6  | loss: 0.34722 | val_0_auc: 0.92    |  0:00:08s\n",
      "epoch 7  | loss: 0.33624 | val_0_auc: 0.92335 |  0:00:09s\n",
      "epoch 8  | loss: 0.35303 | val_0_auc: 0.90721 |  0:00:11s\n",
      "epoch 9  | loss: 0.34763 | val_0_auc: 0.90748 |  0:00:12s\n",
      "epoch 10 | loss: 0.33128 | val_0_auc: 0.90888 |  0:00:13s\n",
      "epoch 11 | loss: 0.32946 | val_0_auc: 0.92188 |  0:00:14s\n",
      "epoch 12 | loss: 0.3269  | val_0_auc: 0.91564 |  0:00:16s\n",
      "epoch 13 | loss: 0.31694 | val_0_auc: 0.91727 |  0:00:17s\n",
      "epoch 14 | loss: 0.32216 | val_0_auc: 0.92352 |  0:00:18s\n",
      "epoch 15 | loss: 0.27309 | val_0_auc: 0.93444 |  0:00:19s\n",
      "epoch 16 | loss: 0.26088 | val_0_auc: 0.92895 |  0:00:20s\n",
      "epoch 17 | loss: 0.27049 | val_0_auc: 0.91282 |  0:00:22s\n",
      "epoch 18 | loss: 0.27767 | val_0_auc: 0.92446 |  0:00:23s\n",
      "epoch 19 | loss: 0.27088 | val_0_auc: 0.94058 |  0:00:24s\n",
      "epoch 20 | loss: 0.26384 | val_0_auc: 0.93356 |  0:00:25s\n",
      "epoch 21 | loss: 0.26991 | val_0_auc: 0.9271  |  0:00:27s\n",
      "epoch 22 | loss: 0.26464 | val_0_auc: 0.9362  |  0:00:28s\n",
      "epoch 23 | loss: 0.25588 | val_0_auc: 0.9316  |  0:00:29s\n",
      "epoch 24 | loss: 0.26187 | val_0_auc: 0.93841 |  0:00:30s\n",
      "epoch 25 | loss: 0.25292 | val_0_auc: 0.9424  |  0:00:32s\n",
      "epoch 26 | loss: 0.24965 | val_0_auc: 0.9451  |  0:00:33s\n",
      "epoch 27 | loss: 0.26561 | val_0_auc: 0.92202 |  0:00:34s\n",
      "epoch 28 | loss: 0.26438 | val_0_auc: 0.92999 |  0:00:35s\n",
      "epoch 29 | loss: 0.25894 | val_0_auc: 0.92478 |  0:00:36s\n",
      "epoch 30 | loss: 0.26351 | val_0_auc: 0.93052 |  0:00:38s\n",
      "epoch 31 | loss: 0.24246 | val_0_auc: 0.93525 |  0:00:39s\n",
      "epoch 32 | loss: 0.23548 | val_0_auc: 0.93876 |  0:00:40s\n",
      "epoch 33 | loss: 0.23367 | val_0_auc: 0.93888 |  0:00:41s\n",
      "epoch 34 | loss: 0.23574 | val_0_auc: 0.95119 |  0:00:43s\n",
      "epoch 35 | loss: 0.22534 | val_0_auc: 0.95242 |  0:00:44s\n",
      "epoch 36 | loss: 0.23284 | val_0_auc: 0.94922 |  0:00:45s\n",
      "epoch 37 | loss: 0.23662 | val_0_auc: 0.95097 |  0:00:46s\n",
      "epoch 38 | loss: 0.25876 | val_0_auc: 0.93873 |  0:00:47s\n",
      "epoch 39 | loss: 0.2581  | val_0_auc: 0.95039 |  0:00:49s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.95242\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95242132  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 49.5943\n",
      "Function value obtained: -0.9524\n",
      "Current minimum: -0.9896\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.54007112299511, 'lambda_sparse': 0.08139312376490491, 'n_steps': 7, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.92875 | val_0_auc: 0.6946  |  0:00:01s\n",
      "epoch 1  | loss: 0.63316 | val_0_auc: 0.77807 |  0:00:02s\n",
      "epoch 2  | loss: 0.52306 | val_0_auc: 0.84399 |  0:00:03s\n",
      "epoch 3  | loss: 0.48367 | val_0_auc: 0.86354 |  0:00:04s\n",
      "epoch 4  | loss: 0.43917 | val_0_auc: 0.88526 |  0:00:05s\n",
      "epoch 5  | loss: 0.42444 | val_0_auc: 0.89947 |  0:00:06s\n",
      "epoch 6  | loss: 0.38045 | val_0_auc: 0.9029  |  0:00:08s\n",
      "epoch 7  | loss: 0.36299 | val_0_auc: 0.90232 |  0:00:09s\n",
      "epoch 8  | loss: 0.33133 | val_0_auc: 0.92554 |  0:00:10s\n",
      "epoch 9  | loss: 0.31538 | val_0_auc: 0.93204 |  0:00:11s\n",
      "epoch 10 | loss: 0.3031  | val_0_auc: 0.91806 |  0:00:12s\n",
      "epoch 11 | loss: 0.31559 | val_0_auc: 0.93361 |  0:00:13s\n",
      "epoch 12 | loss: 0.30949 | val_0_auc: 0.94237 |  0:00:15s\n",
      "epoch 13 | loss: 0.29425 | val_0_auc: 0.94301 |  0:00:16s\n",
      "epoch 14 | loss: 0.2916  | val_0_auc: 0.94061 |  0:00:17s\n",
      "epoch 15 | loss: 0.27014 | val_0_auc: 0.95335 |  0:00:18s\n",
      "epoch 16 | loss: 0.26618 | val_0_auc: 0.95329 |  0:00:19s\n",
      "epoch 17 | loss: 0.25749 | val_0_auc: 0.96146 |  0:00:20s\n",
      "epoch 18 | loss: 0.244   | val_0_auc: 0.96414 |  0:00:22s\n",
      "epoch 19 | loss: 0.22652 | val_0_auc: 0.95403 |  0:00:23s\n",
      "epoch 20 | loss: 0.22845 | val_0_auc: 0.95597 |  0:00:24s\n",
      "epoch 21 | loss: 0.23186 | val_0_auc: 0.94873 |  0:00:25s\n",
      "epoch 22 | loss: 0.23846 | val_0_auc: 0.95972 |  0:00:26s\n",
      "epoch 23 | loss: 0.21733 | val_0_auc: 0.96467 |  0:00:27s\n",
      "epoch 24 | loss: 0.21001 | val_0_auc: 0.96207 |  0:00:28s\n",
      "epoch 25 | loss: 0.21592 | val_0_auc: 0.96344 |  0:00:30s\n",
      "epoch 26 | loss: 0.20992 | val_0_auc: 0.96502 |  0:00:31s\n",
      "epoch 27 | loss: 0.20754 | val_0_auc: 0.96869 |  0:00:32s\n",
      "epoch 28 | loss: 0.20193 | val_0_auc: 0.9643  |  0:00:33s\n",
      "epoch 29 | loss: 0.20634 | val_0_auc: 0.96264 |  0:00:34s\n",
      "epoch 30 | loss: 0.21336 | val_0_auc: 0.97324 |  0:00:35s\n",
      "epoch 31 | loss: 0.21182 | val_0_auc: 0.97256 |  0:00:36s\n",
      "epoch 32 | loss: 0.20376 | val_0_auc: 0.97489 |  0:00:38s\n",
      "epoch 33 | loss: 0.20715 | val_0_auc: 0.9763  |  0:00:39s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 34 | loss: 0.20156 | val_0_auc: 0.97658 |  0:00:40s\n",
      "epoch 35 | loss: 0.19382 | val_0_auc: 0.97486 |  0:00:41s\n",
      "epoch 36 | loss: 0.20094 | val_0_auc: 0.97341 |  0:00:42s\n",
      "epoch 37 | loss: 0.19109 | val_0_auc: 0.97275 |  0:00:43s\n",
      "epoch 38 | loss: 0.17249 | val_0_auc: 0.97069 |  0:00:44s\n",
      "epoch 39 | loss: 0.18092 | val_0_auc: 0.97511 |  0:00:46s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.97658\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97657723  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 46.4793\n",
      "Function value obtained: -0.9766\n",
      "Current minimum: -0.9896\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.4712542320379205, 'lambda_sparse': 0.05824164490772977, 'n_steps': 10, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.94252 | val_0_auc: 0.79651 |  0:00:02s\n",
      "epoch 1  | loss: 0.53055 | val_0_auc: 0.84227 |  0:00:04s\n",
      "epoch 2  | loss: 0.54594 | val_0_auc: 0.86139 |  0:00:06s\n",
      "epoch 3  | loss: 0.63285 | val_0_auc: 0.87046 |  0:00:09s\n",
      "epoch 4  | loss: 0.50928 | val_0_auc: 0.84963 |  0:00:11s\n",
      "epoch 5  | loss: 0.4733  | val_0_auc: 0.87501 |  0:00:13s\n",
      "epoch 6  | loss: 0.41163 | val_0_auc: 0.89113 |  0:00:16s\n",
      "epoch 7  | loss: 0.50525 | val_0_auc: 0.87038 |  0:00:18s\n",
      "epoch 8  | loss: 0.45151 | val_0_auc: 0.89563 |  0:00:20s\n",
      "epoch 9  | loss: 0.37851 | val_0_auc: 0.90372 |  0:00:23s\n",
      "epoch 10 | loss: 0.36928 | val_0_auc: 0.9066  |  0:00:25s\n",
      "epoch 11 | loss: 0.37924 | val_0_auc: 0.90841 |  0:00:27s\n",
      "epoch 12 | loss: 0.34705 | val_0_auc: 0.9307  |  0:00:30s\n",
      "epoch 13 | loss: 0.33351 | val_0_auc: 0.91821 |  0:00:32s\n",
      "epoch 14 | loss: 0.35139 | val_0_auc: 0.922   |  0:00:34s\n",
      "epoch 15 | loss: 0.33173 | val_0_auc: 0.92539 |  0:00:37s\n",
      "epoch 16 | loss: 0.3625  | val_0_auc: 0.93054 |  0:00:39s\n",
      "epoch 17 | loss: 0.38357 | val_0_auc: 0.92503 |  0:00:41s\n",
      "epoch 18 | loss: 0.36721 | val_0_auc: 0.90943 |  0:00:43s\n",
      "epoch 19 | loss: 0.33412 | val_0_auc: 0.92223 |  0:00:46s\n",
      "epoch 20 | loss: 0.30685 | val_0_auc: 0.92719 |  0:00:48s\n",
      "epoch 21 | loss: 0.31338 | val_0_auc: 0.92356 |  0:00:50s\n",
      "epoch 22 | loss: 0.30948 | val_0_auc: 0.93202 |  0:00:53s\n",
      "epoch 23 | loss: 0.29859 | val_0_auc: 0.93655 |  0:00:55s\n",
      "epoch 24 | loss: 0.28329 | val_0_auc: 0.92923 |  0:00:57s\n",
      "epoch 25 | loss: 0.31098 | val_0_auc: 0.93038 |  0:01:00s\n",
      "epoch 26 | loss: 0.28059 | val_0_auc: 0.93079 |  0:01:02s\n",
      "epoch 27 | loss: 0.28021 | val_0_auc: 0.93845 |  0:01:04s\n",
      "epoch 28 | loss: 0.29028 | val_0_auc: 0.93312 |  0:01:07s\n",
      "epoch 29 | loss: 0.3119  | val_0_auc: 0.93173 |  0:01:09s\n",
      "epoch 30 | loss: 0.29229 | val_0_auc: 0.93788 |  0:01:11s\n",
      "epoch 31 | loss: 0.31296 | val_0_auc: 0.93379 |  0:01:14s\n",
      "epoch 32 | loss: 0.28366 | val_0_auc: 0.93989 |  0:01:16s\n",
      "epoch 33 | loss: 0.26123 | val_0_auc: 0.94059 |  0:01:18s\n",
      "epoch 34 | loss: 0.25262 | val_0_auc: 0.94648 |  0:01:21s\n",
      "epoch 35 | loss: 0.24531 | val_0_auc: 0.95224 |  0:01:23s\n",
      "epoch 36 | loss: 0.24463 | val_0_auc: 0.94048 |  0:01:25s\n",
      "epoch 37 | loss: 0.26193 | val_0_auc: 0.95757 |  0:01:27s\n",
      "epoch 38 | loss: 0.24955 | val_0_auc: 0.964   |  0:01:30s\n",
      "epoch 39 | loss: 0.26644 | val_0_auc: 0.94396 |  0:01:32s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.964\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96399941  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 93.4266\n",
      "Function value obtained: -0.9640\n",
      "Current minimum: -0.9896\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5859396560772239, 'lambda_sparse': 0.0848263535472106, 'n_steps': 4, 'n_a': 24, 'n_d': 24, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.72743 | val_0_auc: 0.85527 |  0:00:01s\n",
      "epoch 1  | loss: 0.44066 | val_0_auc: 0.90637 |  0:00:02s\n",
      "epoch 2  | loss: 0.34992 | val_0_auc: 0.91929 |  0:00:03s\n",
      "epoch 3  | loss: 0.28955 | val_0_auc: 0.93774 |  0:00:04s\n",
      "epoch 4  | loss: 0.26884 | val_0_auc: 0.94523 |  0:00:05s\n",
      "epoch 5  | loss: 0.25206 | val_0_auc: 0.94549 |  0:00:06s\n",
      "epoch 6  | loss: 0.23218 | val_0_auc: 0.95676 |  0:00:07s\n",
      "epoch 7  | loss: 0.21688 | val_0_auc: 0.95996 |  0:00:08s\n",
      "epoch 8  | loss: 0.20482 | val_0_auc: 0.96736 |  0:00:09s\n",
      "epoch 9  | loss: 0.19508 | val_0_auc: 0.97316 |  0:00:10s\n",
      "epoch 10 | loss: 0.18565 | val_0_auc: 0.97619 |  0:00:11s\n",
      "epoch 11 | loss: 0.19714 | val_0_auc: 0.97608 |  0:00:12s\n",
      "epoch 12 | loss: 0.17333 | val_0_auc: 0.97583 |  0:00:13s\n",
      "epoch 13 | loss: 0.16924 | val_0_auc: 0.98014 |  0:00:14s\n",
      "epoch 14 | loss: 0.17713 | val_0_auc: 0.97999 |  0:00:15s\n",
      "epoch 15 | loss: 0.17027 | val_0_auc: 0.98458 |  0:00:16s\n",
      "epoch 16 | loss: 0.15724 | val_0_auc: 0.98292 |  0:00:17s\n",
      "epoch 17 | loss: 0.14951 | val_0_auc: 0.98527 |  0:00:18s\n",
      "epoch 18 | loss: 0.13377 | val_0_auc: 0.98245 |  0:00:19s\n",
      "epoch 19 | loss: 0.13663 | val_0_auc: 0.98964 |  0:00:20s\n",
      "epoch 20 | loss: 0.13321 | val_0_auc: 0.99058 |  0:00:21s\n",
      "epoch 21 | loss: 0.1258  | val_0_auc: 0.98653 |  0:00:22s\n",
      "epoch 22 | loss: 0.13012 | val_0_auc: 0.99069 |  0:00:23s\n",
      "epoch 23 | loss: 0.12454 | val_0_auc: 0.99182 |  0:00:24s\n",
      "epoch 24 | loss: 0.1236  | val_0_auc: 0.98597 |  0:00:25s\n",
      "epoch 25 | loss: 0.12645 | val_0_auc: 0.98906 |  0:00:26s\n",
      "epoch 26 | loss: 0.12424 | val_0_auc: 0.9892  |  0:00:27s\n",
      "epoch 27 | loss: 0.1261  | val_0_auc: 0.99163 |  0:00:28s\n",
      "epoch 28 | loss: 0.12144 | val_0_auc: 0.9908  |  0:00:29s\n",
      "epoch 29 | loss: 0.11783 | val_0_auc: 0.99286 |  0:00:30s\n",
      "epoch 30 | loss: 0.11036 | val_0_auc: 0.99256 |  0:00:31s\n",
      "epoch 31 | loss: 0.11028 | val_0_auc: 0.98966 |  0:00:32s\n",
      "epoch 32 | loss: 0.10318 | val_0_auc: 0.98975 |  0:00:33s\n",
      "epoch 33 | loss: 0.11549 | val_0_auc: 0.98705 |  0:00:34s\n",
      "epoch 34 | loss: 0.11485 | val_0_auc: 0.99214 |  0:00:35s\n",
      "epoch 35 | loss: 0.10497 | val_0_auc: 0.99293 |  0:00:37s\n",
      "epoch 36 | loss: 0.11519 | val_0_auc: 0.9882  |  0:00:38s\n",
      "epoch 37 | loss: 0.11491 | val_0_auc: 0.99294 |  0:00:39s\n",
      "epoch 38 | loss: 0.10639 | val_0_auc: 0.99395 |  0:00:40s\n",
      "epoch 39 | loss: 0.10136 | val_0_auc: 0.99166 |  0:00:41s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.99395\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9939454  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 41.4734\n",
      "Function value obtained: -0.9939\n",
      "Current minimum: -0.9939\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9278114971469482, 'lambda_sparse': 0.08753029010537075, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.84759 | val_0_auc: 0.79554 |  0:00:01s\n",
      "epoch 1  | loss: 0.48827 | val_0_auc: 0.88347 |  0:00:03s\n",
      "epoch 2  | loss: 0.4479  | val_0_auc: 0.86505 |  0:00:05s\n",
      "epoch 3  | loss: 0.51499 | val_0_auc: 0.87908 |  0:00:07s\n",
      "epoch 4  | loss: 0.41992 | val_0_auc: 0.88232 |  0:00:09s\n",
      "epoch 5  | loss: 0.42844 | val_0_auc: 0.87309 |  0:00:11s\n",
      "epoch 6  | loss: 0.39359 | val_0_auc: 0.90128 |  0:00:13s\n",
      "epoch 7  | loss: 0.35622 | val_0_auc: 0.91267 |  0:00:15s\n",
      "epoch 8  | loss: 0.34753 | val_0_auc: 0.91719 |  0:00:17s\n",
      "epoch 9  | loss: 0.34086 | val_0_auc: 0.94142 |  0:00:19s\n",
      "epoch 10 | loss: 0.32281 | val_0_auc: 0.93314 |  0:00:21s\n",
      "epoch 11 | loss: 0.35379 | val_0_auc: 0.91604 |  0:00:23s\n",
      "epoch 12 | loss: 0.36872 | val_0_auc: 0.90639 |  0:00:25s\n",
      "epoch 13 | loss: 0.5083  | val_0_auc: 0.91414 |  0:00:26s\n",
      "epoch 14 | loss: 0.46444 | val_0_auc: 0.92026 |  0:00:28s\n",
      "epoch 15 | loss: 0.41362 | val_0_auc: 0.87888 |  0:00:30s\n",
      "epoch 16 | loss: 0.50001 | val_0_auc: 0.89358 |  0:00:32s\n",
      "epoch 17 | loss: 0.85113 | val_0_auc: 0.8928  |  0:00:34s\n",
      "epoch 18 | loss: 0.45637 | val_0_auc: 0.87981 |  0:00:36s\n",
      "epoch 19 | loss: 0.40687 | val_0_auc: 0.91536 |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.94142\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.94141807  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 39.1033\n",
      "Function value obtained: -0.9414\n",
      "Current minimum: -0.9939\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.272404843875921, 'lambda_sparse': 0.08801465876651296, 'n_steps': 6, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.91247 | val_0_auc: 0.84216 |  0:00:01s\n",
      "epoch 1  | loss: 0.45044 | val_0_auc: 0.886   |  0:00:03s\n",
      "epoch 2  | loss: 0.36076 | val_0_auc: 0.91373 |  0:00:04s\n",
      "epoch 3  | loss: 0.30336 | val_0_auc: 0.93421 |  0:00:06s\n",
      "epoch 4  | loss: 0.29328 | val_0_auc: 0.93787 |  0:00:07s\n",
      "epoch 5  | loss: 0.2893  | val_0_auc: 0.9319  |  0:00:09s\n",
      "epoch 6  | loss: 0.28018 | val_0_auc: 0.929   |  0:00:10s\n",
      "epoch 7  | loss: 0.26751 | val_0_auc: 0.94625 |  0:00:12s\n",
      "epoch 8  | loss: 0.26241 | val_0_auc: 0.94807 |  0:00:13s\n",
      "epoch 9  | loss: 0.251   | val_0_auc: 0.96521 |  0:00:15s\n",
      "epoch 10 | loss: 0.22945 | val_0_auc: 0.97013 |  0:00:16s\n",
      "epoch 11 | loss: 0.22279 | val_0_auc: 0.961   |  0:00:18s\n",
      "epoch 12 | loss: 0.21186 | val_0_auc: 0.96998 |  0:00:19s\n",
      "epoch 13 | loss: 0.21025 | val_0_auc: 0.96899 |  0:00:21s\n",
      "epoch 14 | loss: 0.21214 | val_0_auc: 0.97033 |  0:00:22s\n",
      "epoch 15 | loss: 0.18889 | val_0_auc: 0.98003 |  0:00:24s\n",
      "epoch 16 | loss: 0.19438 | val_0_auc: 0.97748 |  0:00:25s\n",
      "epoch 17 | loss: 0.18865 | val_0_auc: 0.97685 |  0:00:27s\n",
      "epoch 18 | loss: 0.18105 | val_0_auc: 0.97842 |  0:00:28s\n",
      "epoch 19 | loss: 0.17638 | val_0_auc: 0.9801  |  0:00:30s\n",
      "epoch 20 | loss: 0.17989 | val_0_auc: 0.97515 |  0:00:31s\n",
      "epoch 21 | loss: 0.19532 | val_0_auc: 0.97203 |  0:00:33s\n",
      "epoch 22 | loss: 0.20781 | val_0_auc: 0.97775 |  0:00:34s\n",
      "epoch 23 | loss: 0.18545 | val_0_auc: 0.9816  |  0:00:36s\n",
      "epoch 24 | loss: 0.18389 | val_0_auc: 0.98348 |  0:00:37s\n",
      "epoch 25 | loss: 0.17288 | val_0_auc: 0.98449 |  0:00:39s\n",
      "epoch 26 | loss: 0.15997 | val_0_auc: 0.98079 |  0:00:40s\n",
      "epoch 27 | loss: 0.15455 | val_0_auc: 0.98136 |  0:00:42s\n",
      "epoch 28 | loss: 0.15141 | val_0_auc: 0.98572 |  0:00:43s\n",
      "epoch 29 | loss: 0.15712 | val_0_auc: 0.98703 |  0:00:45s\n",
      "epoch 30 | loss: 0.14891 | val_0_auc: 0.9879  |  0:00:46s\n",
      "epoch 31 | loss: 0.15191 | val_0_auc: 0.98895 |  0:00:48s\n",
      "epoch 32 | loss: 0.13916 | val_0_auc: 0.98727 |  0:00:49s\n",
      "epoch 33 | loss: 0.14197 | val_0_auc: 0.98509 |  0:00:51s\n",
      "epoch 34 | loss: 0.13961 | val_0_auc: 0.98736 |  0:00:52s\n",
      "epoch 35 | loss: 0.15102 | val_0_auc: 0.98197 |  0:00:54s\n",
      "epoch 36 | loss: 0.14844 | val_0_auc: 0.98316 |  0:00:55s\n",
      "epoch 37 | loss: 0.14416 | val_0_auc: 0.98112 |  0:00:56s\n",
      "epoch 38 | loss: 0.15162 | val_0_auc: 0.9827  |  0:00:58s\n",
      "epoch 39 | loss: 0.14728 | val_0_auc: 0.9863  |  0:00:59s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 31 and best_val_0_auc = 0.98895\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98895301  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 60.5061\n",
      "Function value obtained: -0.9890\n",
      "Current minimum: -0.9939\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6063868892935669, 'lambda_sparse': 0.09000423793755949, 'n_steps': 7, 'n_a': 64, 'n_d': 64, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.51596 | val_0_auc: 0.81959 |  0:00:02s\n",
      "epoch 1  | loss: 0.68593 | val_0_auc: 0.82139 |  0:00:04s\n",
      "epoch 2  | loss: 0.79094 | val_0_auc: 0.84225 |  0:00:06s\n",
      "epoch 3  | loss: 0.6919  | val_0_auc: 0.8728  |  0:00:09s\n",
      "epoch 4  | loss: 0.48159 | val_0_auc: 0.87864 |  0:00:11s\n",
      "epoch 5  | loss: 0.39617 | val_0_auc: 0.8917  |  0:00:13s\n",
      "epoch 6  | loss: 0.38228 | val_0_auc: 0.90562 |  0:00:15s\n",
      "epoch 7  | loss: 0.35645 | val_0_auc: 0.9017  |  0:00:18s\n",
      "epoch 8  | loss: 0.35351 | val_0_auc: 0.9112  |  0:00:20s\n",
      "epoch 9  | loss: 0.40584 | val_0_auc: 0.93104 |  0:00:22s\n",
      "epoch 10 | loss: 0.31608 | val_0_auc: 0.93658 |  0:00:24s\n",
      "epoch 11 | loss: 0.28848 | val_0_auc: 0.95519 |  0:00:26s\n",
      "epoch 12 | loss: 0.28786 | val_0_auc: 0.9512  |  0:00:29s\n",
      "epoch 13 | loss: 0.27918 | val_0_auc: 0.95899 |  0:00:31s\n",
      "epoch 14 | loss: 0.26172 | val_0_auc: 0.96071 |  0:00:33s\n",
      "epoch 15 | loss: 0.23141 | val_0_auc: 0.96706 |  0:00:35s\n",
      "epoch 16 | loss: 0.2338  | val_0_auc: 0.96624 |  0:00:38s\n",
      "epoch 17 | loss: 0.23866 | val_0_auc: 0.96238 |  0:00:40s\n",
      "epoch 18 | loss: 0.25066 | val_0_auc: 0.95877 |  0:00:42s\n",
      "epoch 19 | loss: 0.26654 | val_0_auc: 0.96862 |  0:00:44s\n",
      "epoch 20 | loss: 0.25438 | val_0_auc: 0.96353 |  0:00:46s\n",
      "epoch 21 | loss: 0.24571 | val_0_auc: 0.96221 |  0:00:49s\n",
      "epoch 22 | loss: 0.26566 | val_0_auc: 0.95036 |  0:00:51s\n",
      "epoch 23 | loss: 0.27151 | val_0_auc: 0.955   |  0:00:53s\n",
      "epoch 24 | loss: 0.26589 | val_0_auc: 0.95842 |  0:00:55s\n",
      "epoch 25 | loss: 0.26091 | val_0_auc: 0.96066 |  0:00:58s\n",
      "epoch 26 | loss: 0.24316 | val_0_auc: 0.95521 |  0:01:00s\n",
      "epoch 27 | loss: 0.23498 | val_0_auc: 0.9705  |  0:01:02s\n",
      "epoch 28 | loss: 0.22738 | val_0_auc: 0.96267 |  0:01:04s\n",
      "epoch 29 | loss: 0.23577 | val_0_auc: 0.95262 |  0:01:07s\n",
      "epoch 30 | loss: 0.23962 | val_0_auc: 0.95601 |  0:01:09s\n",
      "epoch 31 | loss: 0.22774 | val_0_auc: 0.96674 |  0:01:11s\n",
      "epoch 32 | loss: 0.21539 | val_0_auc: 0.97038 |  0:01:13s\n",
      "epoch 33 | loss: 0.20395 | val_0_auc: 0.96982 |  0:01:16s\n",
      "epoch 34 | loss: 0.20533 | val_0_auc: 0.9727  |  0:01:18s\n",
      "epoch 35 | loss: 0.20333 | val_0_auc: 0.97804 |  0:01:20s\n",
      "epoch 36 | loss: 0.18656 | val_0_auc: 0.97753 |  0:01:22s\n",
      "epoch 37 | loss: 0.19504 | val_0_auc: 0.97756 |  0:01:25s\n",
      "epoch 38 | loss: 0.16641 | val_0_auc: 0.98122 |  0:01:27s\n",
      "epoch 39 | loss: 0.16241 | val_0_auc: 0.98398 |  0:01:29s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.98398\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98398146  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 90.4076\n",
      "Function value obtained: -0.9840\n",
      "Current minimum: -0.9939\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8443401559801642, 'lambda_sparse': 0.0009995371195472458, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.4353  | val_0_auc: 0.62085 |  0:00:01s\n",
      "epoch 1  | loss: 0.59916 | val_0_auc: 0.79355 |  0:00:03s\n",
      "epoch 2  | loss: 0.51214 | val_0_auc: 0.82792 |  0:00:05s\n",
      "epoch 3  | loss: 0.4539  | val_0_auc: 0.86289 |  0:00:06s\n",
      "epoch 4  | loss: 0.43459 | val_0_auc: 0.84868 |  0:00:08s\n",
      "epoch 5  | loss: 0.41957 | val_0_auc: 0.8547  |  0:00:10s\n",
      "epoch 6  | loss: 0.43482 | val_0_auc: 0.87532 |  0:00:12s\n",
      "epoch 7  | loss: 0.40325 | val_0_auc: 0.90255 |  0:00:13s\n",
      "epoch 8  | loss: 0.43862 | val_0_auc: 0.87659 |  0:00:15s\n",
      "epoch 9  | loss: 0.38508 | val_0_auc: 0.90789 |  0:00:17s\n",
      "epoch 10 | loss: 0.32671 | val_0_auc: 0.91146 |  0:00:18s\n",
      "epoch 11 | loss: 0.32976 | val_0_auc: 0.91398 |  0:00:20s\n",
      "epoch 12 | loss: 0.32713 | val_0_auc: 0.933   |  0:00:22s\n",
      "epoch 13 | loss: 0.33874 | val_0_auc: 0.91975 |  0:00:24s\n",
      "epoch 14 | loss: 0.30474 | val_0_auc: 0.92795 |  0:00:25s\n",
      "epoch 15 | loss: 0.32543 | val_0_auc: 0.91012 |  0:00:27s\n",
      "epoch 16 | loss: 0.30396 | val_0_auc: 0.91096 |  0:00:29s\n",
      "epoch 17 | loss: 0.33437 | val_0_auc: 0.91213 |  0:00:30s\n",
      "epoch 18 | loss: 0.37337 | val_0_auc: 0.91111 |  0:00:32s\n",
      "epoch 19 | loss: 0.43485 | val_0_auc: 0.89803 |  0:00:34s\n",
      "epoch 20 | loss: 0.34517 | val_0_auc: 0.9099  |  0:00:36s\n",
      "epoch 21 | loss: 0.32424 | val_0_auc: 0.92054 |  0:00:37s\n",
      "epoch 22 | loss: 0.31908 | val_0_auc: 0.93084 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.933\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9330037  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 40.1687\n",
      "Function value obtained: -0.9330\n",
      "Current minimum: -0.9939\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.574933347686326, 'lambda_sparse': 0.08295459606351738, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.93292 | val_0_auc: 0.78118 |  0:00:01s\n",
      "epoch 1  | loss: 0.54654 | val_0_auc: 0.84409 |  0:00:02s\n",
      "epoch 2  | loss: 0.45789 | val_0_auc: 0.86819 |  0:00:04s\n",
      "epoch 3  | loss: 0.41136 | val_0_auc: 0.88629 |  0:00:05s\n",
      "epoch 4  | loss: 0.36884 | val_0_auc: 0.8911  |  0:00:06s\n",
      "epoch 5  | loss: 0.34523 | val_0_auc: 0.90472 |  0:00:08s\n",
      "epoch 6  | loss: 0.34848 | val_0_auc: 0.91485 |  0:00:09s\n",
      "epoch 7  | loss: 0.35279 | val_0_auc: 0.90911 |  0:00:10s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 0.33169 | val_0_auc: 0.93186 |  0:00:12s\n",
      "epoch 9  | loss: 0.30072 | val_0_auc: 0.94565 |  0:00:13s\n",
      "epoch 10 | loss: 0.27498 | val_0_auc: 0.9561  |  0:00:15s\n",
      "epoch 11 | loss: 0.25855 | val_0_auc: 0.95823 |  0:00:16s\n",
      "epoch 12 | loss: 0.26235 | val_0_auc: 0.95278 |  0:00:17s\n",
      "epoch 13 | loss: 0.2595  | val_0_auc: 0.95237 |  0:00:19s\n",
      "epoch 14 | loss: 0.24691 | val_0_auc: 0.96508 |  0:00:20s\n",
      "epoch 15 | loss: 0.23019 | val_0_auc: 0.96733 |  0:00:21s\n",
      "epoch 16 | loss: 0.22992 | val_0_auc: 0.97477 |  0:00:23s\n",
      "epoch 17 | loss: 0.20687 | val_0_auc: 0.97739 |  0:00:24s\n",
      "epoch 18 | loss: 0.21408 | val_0_auc: 0.96993 |  0:00:26s\n",
      "epoch 19 | loss: 0.21297 | val_0_auc: 0.96573 |  0:00:27s\n",
      "epoch 20 | loss: 0.2283  | val_0_auc: 0.96301 |  0:00:28s\n",
      "epoch 21 | loss: 0.22339 | val_0_auc: 0.96837 |  0:00:30s\n",
      "epoch 22 | loss: 0.22407 | val_0_auc: 0.9443  |  0:00:31s\n",
      "epoch 23 | loss: 0.23136 | val_0_auc: 0.96179 |  0:00:32s\n",
      "epoch 24 | loss: 0.22492 | val_0_auc: 0.95839 |  0:00:34s\n",
      "epoch 25 | loss: 0.22429 | val_0_auc: 0.96811 |  0:00:35s\n",
      "epoch 26 | loss: 0.21667 | val_0_auc: 0.97305 |  0:00:37s\n",
      "epoch 27 | loss: 0.21325 | val_0_auc: 0.97056 |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.97739\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97739159  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 38.8817\n",
      "Function value obtained: -0.9774\n",
      "Current minimum: -0.9939\n",
      "Iteration No: 11 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0682736252858298, 'lambda_sparse': 0.011394705648614379, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.70513 | val_0_auc: 0.88382 |  0:00:01s\n",
      "epoch 1  | loss: 0.35204 | val_0_auc: 0.91199 |  0:00:02s\n",
      "epoch 2  | loss: 0.29299 | val_0_auc: 0.92594 |  0:00:03s\n",
      "epoch 3  | loss: 0.24749 | val_0_auc: 0.95814 |  0:00:04s\n",
      "epoch 4  | loss: 0.2125  | val_0_auc: 0.96513 |  0:00:05s\n",
      "epoch 5  | loss: 0.19103 | val_0_auc: 0.97687 |  0:00:07s\n",
      "epoch 6  | loss: 0.18158 | val_0_auc: 0.98003 |  0:00:08s\n",
      "epoch 7  | loss: 0.15791 | val_0_auc: 0.98562 |  0:00:09s\n",
      "epoch 8  | loss: 0.146   | val_0_auc: 0.98681 |  0:00:10s\n",
      "epoch 9  | loss: 0.13292 | val_0_auc: 0.99038 |  0:00:11s\n",
      "epoch 10 | loss: 0.12062 | val_0_auc: 0.99096 |  0:00:12s\n",
      "epoch 11 | loss: 0.11352 | val_0_auc: 0.99122 |  0:00:14s\n",
      "epoch 12 | loss: 0.1118  | val_0_auc: 0.99167 |  0:00:15s\n",
      "epoch 13 | loss: 0.1161  | val_0_auc: 0.99317 |  0:00:16s\n",
      "epoch 14 | loss: 0.11501 | val_0_auc: 0.98876 |  0:00:17s\n",
      "epoch 15 | loss: 0.11541 | val_0_auc: 0.99111 |  0:00:18s\n",
      "epoch 16 | loss: 0.11399 | val_0_auc: 0.99446 |  0:00:20s\n",
      "epoch 17 | loss: 0.10448 | val_0_auc: 0.99391 |  0:00:21s\n",
      "epoch 18 | loss: 0.10687 | val_0_auc: 0.99081 |  0:00:22s\n",
      "epoch 19 | loss: 0.10234 | val_0_auc: 0.99219 |  0:00:23s\n",
      "epoch 20 | loss: 0.09888 | val_0_auc: 0.99332 |  0:00:24s\n",
      "epoch 21 | loss: 0.09416 | val_0_auc: 0.99474 |  0:00:25s\n",
      "epoch 22 | loss: 0.0934  | val_0_auc: 0.99505 |  0:00:27s\n",
      "epoch 23 | loss: 0.10043 | val_0_auc: 0.99249 |  0:00:28s\n",
      "epoch 24 | loss: 0.10459 | val_0_auc: 0.99102 |  0:00:29s\n",
      "epoch 25 | loss: 0.10298 | val_0_auc: 0.99175 |  0:00:30s\n",
      "epoch 26 | loss: 0.10306 | val_0_auc: 0.99515 |  0:00:31s\n",
      "epoch 27 | loss: 0.09729 | val_0_auc: 0.99518 |  0:00:33s\n",
      "epoch 28 | loss: 0.10577 | val_0_auc: 0.98706 |  0:00:34s\n",
      "epoch 29 | loss: 0.09846 | val_0_auc: 0.99381 |  0:00:35s\n",
      "epoch 30 | loss: 0.09691 | val_0_auc: 0.99364 |  0:00:36s\n",
      "epoch 31 | loss: 0.10886 | val_0_auc: 0.99354 |  0:00:37s\n",
      "epoch 32 | loss: 0.09802 | val_0_auc: 0.99469 |  0:00:38s\n",
      "epoch 33 | loss: 0.093   | val_0_auc: 0.9945  |  0:00:40s\n",
      "epoch 34 | loss: 0.09583 | val_0_auc: 0.99342 |  0:00:41s\n",
      "epoch 35 | loss: 0.09075 | val_0_auc: 0.99271 |  0:00:42s\n",
      "epoch 36 | loss: 0.09685 | val_0_auc: 0.99433 |  0:00:43s\n",
      "epoch 37 | loss: 0.09641 | val_0_auc: 0.9946  |  0:00:44s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.99518\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99518256  of boosting iteration \n",
      "Iteration No: 11 ended. Evaluation done at random point.\n",
      "Time taken: 45.2145\n",
      "Function value obtained: -0.9952\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 12 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.198429761193292, 'lambda_sparse': 0.00021586264951466714, 'n_steps': 8, 'n_a': 32, 'n_d': 32, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.00252 | val_0_auc: 0.81135 |  0:00:01s\n",
      "epoch 1  | loss: 0.46269 | val_0_auc: 0.87426 |  0:00:03s\n",
      "epoch 2  | loss: 0.37899 | val_0_auc: 0.89549 |  0:00:05s\n",
      "epoch 3  | loss: 0.35361 | val_0_auc: 0.90968 |  0:00:07s\n",
      "epoch 4  | loss: 0.30531 | val_0_auc: 0.92494 |  0:00:09s\n",
      "epoch 5  | loss: 0.28042 | val_0_auc: 0.92984 |  0:00:11s\n",
      "epoch 6  | loss: 0.27388 | val_0_auc: 0.93547 |  0:00:13s\n",
      "epoch 7  | loss: 0.30113 | val_0_auc: 0.94234 |  0:00:15s\n",
      "epoch 8  | loss: 0.26802 | val_0_auc: 0.95031 |  0:00:17s\n",
      "epoch 9  | loss: 0.29238 | val_0_auc: 0.93793 |  0:00:19s\n",
      "epoch 10 | loss: 0.30262 | val_0_auc: 0.94704 |  0:00:21s\n",
      "epoch 11 | loss: 0.24609 | val_0_auc: 0.94922 |  0:00:23s\n",
      "epoch 12 | loss: 0.22376 | val_0_auc: 0.94813 |  0:00:25s\n",
      "epoch 13 | loss: 0.2188  | val_0_auc: 0.95776 |  0:00:26s\n",
      "epoch 14 | loss: 0.19675 | val_0_auc: 0.96445 |  0:00:28s\n",
      "epoch 15 | loss: 0.19915 | val_0_auc: 0.96623 |  0:00:30s\n",
      "epoch 16 | loss: 0.20192 | val_0_auc: 0.96516 |  0:00:32s\n",
      "epoch 17 | loss: 0.18332 | val_0_auc: 0.96868 |  0:00:34s\n",
      "epoch 18 | loss: 0.18154 | val_0_auc: 0.96555 |  0:00:36s\n",
      "epoch 19 | loss: 0.18073 | val_0_auc: 0.9651  |  0:00:38s\n",
      "epoch 20 | loss: 0.19319 | val_0_auc: 0.9708  |  0:00:40s\n",
      "epoch 21 | loss: 0.17699 | val_0_auc: 0.96595 |  0:00:42s\n",
      "epoch 22 | loss: 0.16482 | val_0_auc: 0.97922 |  0:00:44s\n",
      "epoch 23 | loss: 0.16674 | val_0_auc: 0.97708 |  0:00:46s\n",
      "epoch 24 | loss: 0.15975 | val_0_auc: 0.97754 |  0:00:47s\n",
      "epoch 25 | loss: 0.16623 | val_0_auc: 0.97711 |  0:00:49s\n",
      "epoch 26 | loss: 0.1613  | val_0_auc: 0.98066 |  0:00:51s\n",
      "epoch 27 | loss: 0.15546 | val_0_auc: 0.97868 |  0:00:53s\n",
      "epoch 28 | loss: 0.15269 | val_0_auc: 0.98542 |  0:00:55s\n",
      "epoch 29 | loss: 0.1625  | val_0_auc: 0.98528 |  0:00:57s\n",
      "epoch 30 | loss: 0.15037 | val_0_auc: 0.9844  |  0:00:59s\n",
      "epoch 31 | loss: 0.14786 | val_0_auc: 0.98085 |  0:01:01s\n",
      "epoch 32 | loss: 0.13754 | val_0_auc: 0.98404 |  0:01:03s\n",
      "epoch 33 | loss: 0.13096 | val_0_auc: 0.98946 |  0:01:04s\n",
      "epoch 34 | loss: 0.13743 | val_0_auc: 0.98605 |  0:01:06s\n",
      "epoch 35 | loss: 0.13675 | val_0_auc: 0.98384 |  0:01:08s\n",
      "epoch 36 | loss: 0.13921 | val_0_auc: 0.98308 |  0:01:10s\n",
      "epoch 37 | loss: 0.13815 | val_0_auc: 0.98468 |  0:01:12s\n",
      "epoch 38 | loss: 0.14048 | val_0_auc: 0.98644 |  0:01:14s\n",
      "epoch 39 | loss: 0.13895 | val_0_auc: 0.98016 |  0:01:16s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.98946\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.98945704  of boosting iteration \n",
      "Iteration No: 12 ended. Evaluation done at random point.\n",
      "Time taken: 77.1600\n",
      "Function value obtained: -0.9895\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 13 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9495535733620986, 'lambda_sparse': 0.09418901283024785, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.86028 | val_0_auc: 0.78778 |  0:00:01s\n",
      "epoch 1  | loss: 0.54092 | val_0_auc: 0.79437 |  0:00:02s\n",
      "epoch 2  | loss: 0.48498 | val_0_auc: 0.8112  |  0:00:03s\n",
      "epoch 3  | loss: 0.43964 | val_0_auc: 0.87433 |  0:00:04s\n",
      "epoch 4  | loss: 0.41143 | val_0_auc: 0.87144 |  0:00:06s\n",
      "epoch 5  | loss: 0.38746 | val_0_auc: 0.88208 |  0:00:07s\n",
      "epoch 6  | loss: 0.38227 | val_0_auc: 0.89694 |  0:00:08s\n",
      "epoch 7  | loss: 0.39399 | val_0_auc: 0.921   |  0:00:09s\n",
      "epoch 8  | loss: 0.34528 | val_0_auc: 0.92469 |  0:00:11s\n",
      "epoch 9  | loss: 0.31237 | val_0_auc: 0.9354  |  0:00:12s\n",
      "epoch 10 | loss: 0.29779 | val_0_auc: 0.92956 |  0:00:13s\n",
      "epoch 11 | loss: 0.27445 | val_0_auc: 0.92963 |  0:00:14s\n",
      "epoch 12 | loss: 0.27433 | val_0_auc: 0.94162 |  0:00:15s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13 | loss: 0.26518 | val_0_auc: 0.95396 |  0:00:17s\n",
      "epoch 14 | loss: 0.25442 | val_0_auc: 0.95152 |  0:00:18s\n",
      "epoch 15 | loss: 0.24411 | val_0_auc: 0.95832 |  0:00:19s\n",
      "epoch 16 | loss: 0.23803 | val_0_auc: 0.95137 |  0:00:20s\n",
      "epoch 17 | loss: 0.23498 | val_0_auc: 0.95101 |  0:00:22s\n",
      "epoch 18 | loss: 0.23615 | val_0_auc: 0.96187 |  0:00:23s\n",
      "epoch 19 | loss: 0.23362 | val_0_auc: 0.95906 |  0:00:24s\n",
      "epoch 20 | loss: 0.22697 | val_0_auc: 0.96221 |  0:00:25s\n",
      "epoch 21 | loss: 0.22444 | val_0_auc: 0.96964 |  0:00:26s\n",
      "epoch 22 | loss: 0.22457 | val_0_auc: 0.96496 |  0:00:28s\n",
      "epoch 23 | loss: 0.22326 | val_0_auc: 0.96794 |  0:00:29s\n",
      "epoch 24 | loss: 0.21088 | val_0_auc: 0.96336 |  0:00:30s\n",
      "epoch 25 | loss: 0.21147 | val_0_auc: 0.95946 |  0:00:31s\n",
      "epoch 26 | loss: 0.22767 | val_0_auc: 0.94917 |  0:00:33s\n",
      "epoch 27 | loss: 0.2511  | val_0_auc: 0.95622 |  0:00:34s\n",
      "epoch 28 | loss: 0.24611 | val_0_auc: 0.95657 |  0:00:35s\n",
      "epoch 29 | loss: 0.23422 | val_0_auc: 0.96051 |  0:00:36s\n",
      "epoch 30 | loss: 0.22553 | val_0_auc: 0.95995 |  0:00:38s\n",
      "epoch 31 | loss: 0.20567 | val_0_auc: 0.9657  |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_val_0_auc = 0.96964\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96964162  of boosting iteration \n",
      "Iteration No: 13 ended. Evaluation done at random point.\n",
      "Time taken: 39.6216\n",
      "Function value obtained: -0.9696\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 14 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.719240027024554, 'lambda_sparse': 0.009786957735871325, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.82755 | val_0_auc: 0.7882  |  0:00:01s\n",
      "epoch 1  | loss: 0.49456 | val_0_auc: 0.79749 |  0:00:02s\n",
      "epoch 2  | loss: 0.44576 | val_0_auc: 0.85197 |  0:00:03s\n",
      "epoch 3  | loss: 0.418   | val_0_auc: 0.87034 |  0:00:04s\n",
      "epoch 4  | loss: 0.36473 | val_0_auc: 0.90771 |  0:00:05s\n",
      "epoch 5  | loss: 0.29914 | val_0_auc: 0.92476 |  0:00:07s\n",
      "epoch 6  | loss: 0.27232 | val_0_auc: 0.94773 |  0:00:08s\n",
      "epoch 7  | loss: 0.2725  | val_0_auc: 0.92775 |  0:00:09s\n",
      "epoch 8  | loss: 0.2735  | val_0_auc: 0.95425 |  0:00:10s\n",
      "epoch 9  | loss: 0.24805 | val_0_auc: 0.95177 |  0:00:11s\n",
      "epoch 10 | loss: 0.22966 | val_0_auc: 0.95833 |  0:00:13s\n",
      "epoch 11 | loss: 0.23592 | val_0_auc: 0.95683 |  0:00:14s\n",
      "epoch 12 | loss: 0.22973 | val_0_auc: 0.95988 |  0:00:15s\n",
      "epoch 13 | loss: 0.21078 | val_0_auc: 0.95177 |  0:00:16s\n",
      "epoch 14 | loss: 0.20073 | val_0_auc: 0.97154 |  0:00:17s\n",
      "epoch 15 | loss: 0.1927  | val_0_auc: 0.97606 |  0:00:18s\n",
      "epoch 16 | loss: 0.19262 | val_0_auc: 0.97534 |  0:00:20s\n",
      "epoch 17 | loss: 0.20203 | val_0_auc: 0.97265 |  0:00:21s\n",
      "epoch 18 | loss: 0.20374 | val_0_auc: 0.97058 |  0:00:22s\n",
      "epoch 19 | loss: 0.17971 | val_0_auc: 0.96966 |  0:00:23s\n",
      "epoch 20 | loss: 0.18073 | val_0_auc: 0.97409 |  0:00:24s\n",
      "epoch 21 | loss: 0.16285 | val_0_auc: 0.97761 |  0:00:25s\n",
      "epoch 22 | loss: 0.15735 | val_0_auc: 0.97969 |  0:00:27s\n",
      "epoch 23 | loss: 0.1631  | val_0_auc: 0.97912 |  0:00:28s\n",
      "epoch 24 | loss: 0.15098 | val_0_auc: 0.9842  |  0:00:29s\n",
      "epoch 25 | loss: 0.13903 | val_0_auc: 0.97998 |  0:00:30s\n",
      "epoch 26 | loss: 0.14578 | val_0_auc: 0.98783 |  0:00:31s\n",
      "epoch 27 | loss: 0.13821 | val_0_auc: 0.98659 |  0:00:33s\n",
      "epoch 28 | loss: 0.13505 | val_0_auc: 0.9822  |  0:00:34s\n",
      "epoch 29 | loss: 0.14219 | val_0_auc: 0.98621 |  0:00:35s\n",
      "epoch 30 | loss: 0.13895 | val_0_auc: 0.98602 |  0:00:36s\n",
      "epoch 31 | loss: 0.13317 | val_0_auc: 0.98629 |  0:00:37s\n",
      "epoch 32 | loss: 0.1253  | val_0_auc: 0.98414 |  0:00:38s\n",
      "epoch 33 | loss: 0.1326  | val_0_auc: 0.98372 |  0:00:40s\n",
      "epoch 34 | loss: 0.13209 | val_0_auc: 0.98253 |  0:00:41s\n",
      "epoch 35 | loss: 0.11646 | val_0_auc: 0.98685 |  0:00:42s\n",
      "epoch 36 | loss: 0.11953 | val_0_auc: 0.98891 |  0:00:43s\n",
      "epoch 37 | loss: 0.12113 | val_0_auc: 0.99183 |  0:00:44s\n",
      "epoch 38 | loss: 0.13183 | val_0_auc: 0.98995 |  0:00:46s\n",
      "epoch 39 | loss: 0.12173 | val_0_auc: 0.99327 |  0:00:47s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99327\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99327475  of boosting iteration \n",
      "Iteration No: 14 ended. Evaluation done at random point.\n",
      "Time taken: 47.7055\n",
      "Function value obtained: -0.9933\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 15 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.9943679510926045, 'lambda_sparse': 0.05263788628509853, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.80245 | val_0_auc: 0.77313 |  0:00:01s\n",
      "epoch 1  | loss: 0.51948 | val_0_auc: 0.8031  |  0:00:03s\n",
      "epoch 2  | loss: 0.50559 | val_0_auc: 0.84408 |  0:00:05s\n",
      "epoch 3  | loss: 0.43002 | val_0_auc: 0.87978 |  0:00:07s\n",
      "epoch 4  | loss: 0.40598 | val_0_auc: 0.88228 |  0:00:09s\n",
      "epoch 5  | loss: 0.44504 | val_0_auc: 0.88186 |  0:00:11s\n",
      "epoch 6  | loss: 0.51912 | val_0_auc: 0.88771 |  0:00:13s\n",
      "epoch 7  | loss: 0.5861  | val_0_auc: 0.89166 |  0:00:15s\n",
      "epoch 8  | loss: 0.52812 | val_0_auc: 0.88989 |  0:00:17s\n",
      "epoch 9  | loss: 0.51762 | val_0_auc: 0.89726 |  0:00:19s\n",
      "epoch 10 | loss: 0.50317 | val_0_auc: 0.89587 |  0:00:21s\n",
      "epoch 11 | loss: 0.41579 | val_0_auc: 0.90545 |  0:00:23s\n",
      "epoch 12 | loss: 0.38615 | val_0_auc: 0.91799 |  0:00:24s\n",
      "epoch 13 | loss: 0.35551 | val_0_auc: 0.90471 |  0:00:27s\n",
      "epoch 14 | loss: 0.35468 | val_0_auc: 0.88681 |  0:00:28s\n",
      "epoch 15 | loss: 0.35091 | val_0_auc: 0.90325 |  0:00:30s\n",
      "epoch 16 | loss: 0.33135 | val_0_auc: 0.92051 |  0:00:32s\n",
      "epoch 17 | loss: 0.30165 | val_0_auc: 0.92379 |  0:00:34s\n",
      "epoch 18 | loss: 0.2862  | val_0_auc: 0.92841 |  0:00:36s\n",
      "epoch 19 | loss: 0.27385 | val_0_auc: 0.93035 |  0:00:38s\n",
      "epoch 20 | loss: 0.27845 | val_0_auc: 0.92754 |  0:00:40s\n",
      "epoch 21 | loss: 0.29119 | val_0_auc: 0.92618 |  0:00:42s\n",
      "epoch 22 | loss: 0.2846  | val_0_auc: 0.92744 |  0:00:44s\n",
      "epoch 23 | loss: 0.26857 | val_0_auc: 0.93569 |  0:00:46s\n",
      "epoch 24 | loss: 0.25427 | val_0_auc: 0.9329  |  0:00:48s\n",
      "epoch 25 | loss: 0.26077 | val_0_auc: 0.92777 |  0:00:50s\n",
      "epoch 26 | loss: 0.26339 | val_0_auc: 0.93874 |  0:00:51s\n",
      "epoch 27 | loss: 0.25838 | val_0_auc: 0.93689 |  0:00:53s\n",
      "epoch 28 | loss: 0.24625 | val_0_auc: 0.94378 |  0:00:55s\n",
      "epoch 29 | loss: 0.23418 | val_0_auc: 0.94688 |  0:00:57s\n",
      "epoch 30 | loss: 0.23299 | val_0_auc: 0.94503 |  0:00:59s\n",
      "epoch 31 | loss: 0.2251  | val_0_auc: 0.95279 |  0:01:01s\n",
      "epoch 32 | loss: 0.20908 | val_0_auc: 0.94724 |  0:01:03s\n",
      "epoch 33 | loss: 0.21389 | val_0_auc: 0.95006 |  0:01:05s\n",
      "epoch 34 | loss: 0.21247 | val_0_auc: 0.95381 |  0:01:07s\n",
      "epoch 35 | loss: 0.214   | val_0_auc: 0.95398 |  0:01:09s\n",
      "epoch 36 | loss: 0.22527 | val_0_auc: 0.95243 |  0:01:11s\n",
      "epoch 37 | loss: 0.21433 | val_0_auc: 0.95331 |  0:01:13s\n",
      "epoch 38 | loss: 0.21245 | val_0_auc: 0.96145 |  0:01:14s\n",
      "epoch 39 | loss: 0.2102  | val_0_auc: 0.96619 |  0:01:16s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.96619\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96619256  of boosting iteration \n",
      "Iteration No: 15 ended. Evaluation done at random point.\n",
      "Time taken: 77.5560\n",
      "Function value obtained: -0.9662\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 16 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7150199010346694, 'lambda_sparse': 0.05772616050488577, 'n_steps': 8, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.79367 | val_0_auc: 0.63655 |  0:00:01s\n",
      "epoch 1  | loss: 0.83011 | val_0_auc: 0.77696 |  0:00:03s\n",
      "epoch 2  | loss: 0.59846 | val_0_auc: 0.80033 |  0:00:05s\n",
      "epoch 3  | loss: 0.5147  | val_0_auc: 0.82345 |  0:00:07s\n",
      "epoch 4  | loss: 0.55504 | val_0_auc: 0.8381  |  0:00:08s\n",
      "epoch 5  | loss: 0.91775 | val_0_auc: 0.83877 |  0:00:10s\n",
      "epoch 6  | loss: 0.53467 | val_0_auc: 0.8417  |  0:00:12s\n",
      "epoch 7  | loss: 0.45255 | val_0_auc: 0.86104 |  0:00:14s\n",
      "epoch 8  | loss: 0.41949 | val_0_auc: 0.88519 |  0:00:15s\n",
      "epoch 9  | loss: 0.41393 | val_0_auc: 0.8821  |  0:00:17s\n",
      "epoch 10 | loss: 0.37486 | val_0_auc: 0.89142 |  0:00:19s\n",
      "epoch 11 | loss: 0.36174 | val_0_auc: 0.89552 |  0:00:21s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12 | loss: 0.36887 | val_0_auc: 0.88307 |  0:00:22s\n",
      "epoch 13 | loss: 0.38599 | val_0_auc: 0.86867 |  0:00:24s\n",
      "epoch 14 | loss: 0.36761 | val_0_auc: 0.89271 |  0:00:26s\n",
      "epoch 15 | loss: 0.3623  | val_0_auc: 0.89122 |  0:00:27s\n",
      "epoch 16 | loss: 0.34521 | val_0_auc: 0.91244 |  0:00:29s\n",
      "epoch 17 | loss: 0.3435  | val_0_auc: 0.9101  |  0:00:31s\n",
      "epoch 18 | loss: 0.34174 | val_0_auc: 0.91736 |  0:00:33s\n",
      "epoch 19 | loss: 0.33267 | val_0_auc: 0.92778 |  0:00:34s\n",
      "epoch 20 | loss: 0.33349 | val_0_auc: 0.91639 |  0:00:36s\n",
      "epoch 21 | loss: 0.32258 | val_0_auc: 0.92716 |  0:00:38s\n",
      "epoch 22 | loss: 0.32825 | val_0_auc: 0.92766 |  0:00:40s\n",
      "epoch 23 | loss: 0.31845 | val_0_auc: 0.93576 |  0:00:41s\n",
      "epoch 24 | loss: 0.31041 | val_0_auc: 0.9288  |  0:00:43s\n",
      "epoch 25 | loss: 0.2798  | val_0_auc: 0.94525 |  0:00:45s\n",
      "epoch 26 | loss: 0.27706 | val_0_auc: 0.95489 |  0:00:46s\n",
      "epoch 27 | loss: 0.26811 | val_0_auc: 0.93846 |  0:00:48s\n",
      "epoch 28 | loss: 0.26046 | val_0_auc: 0.9448  |  0:00:50s\n",
      "epoch 29 | loss: 0.28448 | val_0_auc: 0.94958 |  0:00:52s\n",
      "epoch 30 | loss: 0.27523 | val_0_auc: 0.93649 |  0:00:53s\n",
      "epoch 31 | loss: 0.26287 | val_0_auc: 0.93476 |  0:00:55s\n",
      "epoch 32 | loss: 0.25479 | val_0_auc: 0.93244 |  0:00:57s\n",
      "epoch 33 | loss: 0.24065 | val_0_auc: 0.94855 |  0:00:59s\n",
      "epoch 34 | loss: 0.23584 | val_0_auc: 0.9523  |  0:01:00s\n",
      "epoch 35 | loss: 0.22871 | val_0_auc: 0.96531 |  0:01:02s\n",
      "epoch 36 | loss: 0.22522 | val_0_auc: 0.95412 |  0:01:04s\n",
      "epoch 37 | loss: 0.24187 | val_0_auc: 0.94904 |  0:01:06s\n",
      "epoch 38 | loss: 0.2418  | val_0_auc: 0.94036 |  0:01:07s\n",
      "epoch 39 | loss: 0.23572 | val_0_auc: 0.94974 |  0:01:09s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.96531\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96530738  of boosting iteration \n",
      "Iteration No: 16 ended. Evaluation done at random point.\n",
      "Time taken: 70.1740\n",
      "Function value obtained: -0.9653\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 17 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.596674324766612, 'lambda_sparse': 0.04731198733854139, 'n_steps': 10, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.51191 | val_0_auc: 0.68932 |  0:00:02s\n",
      "epoch 1  | loss: 0.92813 | val_0_auc: 0.79258 |  0:00:04s\n",
      "epoch 2  | loss: 1.13977 | val_0_auc: 0.81143 |  0:00:06s\n",
      "epoch 3  | loss: 0.95886 | val_0_auc: 0.84975 |  0:00:08s\n",
      "epoch 4  | loss: 0.67376 | val_0_auc: 0.8554  |  0:00:10s\n",
      "epoch 5  | loss: 0.83978 | val_0_auc: 0.85165 |  0:00:12s\n",
      "epoch 6  | loss: 1.58237 | val_0_auc: 0.85108 |  0:00:14s\n",
      "epoch 7  | loss: 0.91093 | val_0_auc: 0.81158 |  0:00:16s\n",
      "epoch 8  | loss: 1.0117  | val_0_auc: 0.84112 |  0:00:19s\n",
      "epoch 9  | loss: 0.57257 | val_0_auc: 0.86371 |  0:00:21s\n",
      "epoch 10 | loss: 0.60502 | val_0_auc: 0.86447 |  0:00:23s\n",
      "epoch 11 | loss: 0.66772 | val_0_auc: 0.87778 |  0:00:25s\n",
      "epoch 12 | loss: 0.49205 | val_0_auc: 0.90288 |  0:00:27s\n",
      "epoch 13 | loss: 0.39467 | val_0_auc: 0.90581 |  0:00:29s\n",
      "epoch 14 | loss: 0.368   | val_0_auc: 0.89543 |  0:00:32s\n",
      "epoch 15 | loss: 0.39498 | val_0_auc: 0.90949 |  0:00:34s\n",
      "epoch 16 | loss: 0.49567 | val_0_auc: 0.88805 |  0:00:36s\n",
      "epoch 17 | loss: 0.49764 | val_0_auc: 0.9049  |  0:00:38s\n",
      "epoch 18 | loss: 0.44    | val_0_auc: 0.90155 |  0:00:40s\n",
      "epoch 19 | loss: 0.37219 | val_0_auc: 0.90464 |  0:00:42s\n",
      "epoch 20 | loss: 0.37692 | val_0_auc: 0.90713 |  0:00:44s\n",
      "epoch 21 | loss: 0.37893 | val_0_auc: 0.91797 |  0:00:46s\n",
      "epoch 22 | loss: 0.38559 | val_0_auc: 0.90068 |  0:00:48s\n",
      "epoch 23 | loss: 0.39392 | val_0_auc: 0.92431 |  0:00:51s\n",
      "epoch 24 | loss: 0.35876 | val_0_auc: 0.89844 |  0:00:53s\n",
      "epoch 25 | loss: 0.3785  | val_0_auc: 0.90772 |  0:00:55s\n",
      "epoch 26 | loss: 0.39267 | val_0_auc: 0.90586 |  0:00:57s\n",
      "epoch 27 | loss: 0.3687  | val_0_auc: 0.90899 |  0:00:59s\n",
      "epoch 28 | loss: 0.34295 | val_0_auc: 0.92373 |  0:01:01s\n",
      "epoch 29 | loss: 0.34471 | val_0_auc: 0.92042 |  0:01:03s\n",
      "epoch 30 | loss: 0.35229 | val_0_auc: 0.92341 |  0:01:05s\n",
      "epoch 31 | loss: 0.36324 | val_0_auc: 0.92355 |  0:01:07s\n",
      "epoch 32 | loss: 0.33205 | val_0_auc: 0.93202 |  0:01:09s\n",
      "epoch 33 | loss: 0.33238 | val_0_auc: 0.92591 |  0:01:12s\n",
      "epoch 34 | loss: 0.34667 | val_0_auc: 0.93112 |  0:01:14s\n",
      "epoch 35 | loss: 0.33499 | val_0_auc: 0.92451 |  0:01:16s\n",
      "epoch 36 | loss: 0.29766 | val_0_auc: 0.93717 |  0:01:18s\n",
      "epoch 37 | loss: 0.29262 | val_0_auc: 0.93936 |  0:01:20s\n",
      "epoch 38 | loss: 0.29284 | val_0_auc: 0.93451 |  0:01:22s\n",
      "epoch 39 | loss: 0.27792 | val_0_auc: 0.95433 |  0:01:24s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.95433\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.95432913  of boosting iteration \n",
      "Iteration No: 17 ended. Evaluation done at random point.\n",
      "Time taken: 85.4235\n",
      "Function value obtained: -0.9543\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 18 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.182016988509308, 'lambda_sparse': 0.003470537988565155, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.70345 | val_0_auc: 0.85322 |  0:00:01s\n",
      "epoch 1  | loss: 0.36895 | val_0_auc: 0.87943 |  0:00:02s\n",
      "epoch 2  | loss: 0.30458 | val_0_auc: 0.91476 |  0:00:03s\n",
      "epoch 3  | loss: 0.26867 | val_0_auc: 0.93151 |  0:00:04s\n",
      "epoch 4  | loss: 0.24565 | val_0_auc: 0.94067 |  0:00:06s\n",
      "epoch 5  | loss: 0.22094 | val_0_auc: 0.92928 |  0:00:07s\n",
      "epoch 6  | loss: 0.21033 | val_0_auc: 0.9583  |  0:00:08s\n",
      "epoch 7  | loss: 0.19538 | val_0_auc: 0.96396 |  0:00:09s\n",
      "epoch 8  | loss: 0.18854 | val_0_auc: 0.97034 |  0:00:10s\n",
      "epoch 9  | loss: 0.17366 | val_0_auc: 0.97841 |  0:00:11s\n",
      "epoch 10 | loss: 0.15889 | val_0_auc: 0.97887 |  0:00:13s\n",
      "epoch 11 | loss: 0.15062 | val_0_auc: 0.98414 |  0:00:14s\n",
      "epoch 12 | loss: 0.13284 | val_0_auc: 0.98366 |  0:00:15s\n",
      "epoch 13 | loss: 0.14276 | val_0_auc: 0.98728 |  0:00:16s\n",
      "epoch 14 | loss: 0.13054 | val_0_auc: 0.9884  |  0:00:17s\n",
      "epoch 15 | loss: 0.12929 | val_0_auc: 0.98837 |  0:00:19s\n",
      "epoch 16 | loss: 0.11999 | val_0_auc: 0.99092 |  0:00:20s\n",
      "epoch 17 | loss: 0.11324 | val_0_auc: 0.98834 |  0:00:21s\n",
      "epoch 18 | loss: 0.1141  | val_0_auc: 0.98931 |  0:00:22s\n",
      "epoch 19 | loss: 0.11532 | val_0_auc: 0.9904  |  0:00:23s\n",
      "epoch 20 | loss: 0.11459 | val_0_auc: 0.99248 |  0:00:24s\n",
      "epoch 21 | loss: 0.10978 | val_0_auc: 0.99171 |  0:00:26s\n",
      "epoch 22 | loss: 0.10693 | val_0_auc: 0.99377 |  0:00:27s\n",
      "epoch 23 | loss: 0.11161 | val_0_auc: 0.98766 |  0:00:28s\n",
      "epoch 24 | loss: 0.12037 | val_0_auc: 0.99032 |  0:00:29s\n",
      "epoch 25 | loss: 0.11091 | val_0_auc: 0.98754 |  0:00:30s\n",
      "epoch 26 | loss: 0.11499 | val_0_auc: 0.98879 |  0:00:31s\n",
      "epoch 27 | loss: 0.10829 | val_0_auc: 0.99075 |  0:00:33s\n",
      "epoch 28 | loss: 0.10524 | val_0_auc: 0.99278 |  0:00:34s\n",
      "epoch 29 | loss: 0.10938 | val_0_auc: 0.99191 |  0:00:35s\n",
      "epoch 30 | loss: 0.10848 | val_0_auc: 0.99344 |  0:00:36s\n",
      "epoch 31 | loss: 0.10722 | val_0_auc: 0.9938  |  0:00:37s\n",
      "epoch 32 | loss: 0.10028 | val_0_auc: 0.99336 |  0:00:39s\n",
      "epoch 33 | loss: 0.10472 | val_0_auc: 0.9926  |  0:00:40s\n",
      "epoch 34 | loss: 0.09273 | val_0_auc: 0.993   |  0:00:41s\n",
      "epoch 35 | loss: 0.09302 | val_0_auc: 0.9928  |  0:00:42s\n",
      "epoch 36 | loss: 0.0942  | val_0_auc: 0.99385 |  0:00:43s\n",
      "epoch 37 | loss: 0.09568 | val_0_auc: 0.98997 |  0:00:44s\n",
      "epoch 38 | loss: 0.10045 | val_0_auc: 0.9946  |  0:00:46s\n",
      "epoch 39 | loss: 0.09599 | val_0_auc: 0.99278 |  0:00:47s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 38 and best_val_0_auc = 0.9946\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99459523  of boosting iteration \n",
      "Iteration No: 18 ended. Evaluation done at random point.\n",
      "Time taken: 47.7693\n",
      "Function value obtained: -0.9946\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 19 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0291220895311914, 'lambda_sparse': 0.09519334060161502, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.07975 | val_0_auc: 0.84581 |  0:00:01s\n",
      "epoch 1  | loss: 0.45621 | val_0_auc: 0.89486 |  0:00:03s\n",
      "epoch 2  | loss: 0.37477 | val_0_auc: 0.92128 |  0:00:05s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.31888 | val_0_auc: 0.94275 |  0:00:06s\n",
      "epoch 4  | loss: 0.29373 | val_0_auc: 0.9471  |  0:00:08s\n",
      "epoch 5  | loss: 0.28096 | val_0_auc: 0.94792 |  0:00:10s\n",
      "epoch 6  | loss: 0.26608 | val_0_auc: 0.95187 |  0:00:11s\n",
      "epoch 7  | loss: 0.24909 | val_0_auc: 0.9681  |  0:00:13s\n",
      "epoch 8  | loss: 0.24875 | val_0_auc: 0.96896 |  0:00:15s\n",
      "epoch 9  | loss: 0.22297 | val_0_auc: 0.98098 |  0:00:17s\n",
      "epoch 10 | loss: 0.22217 | val_0_auc: 0.97976 |  0:00:18s\n",
      "epoch 11 | loss: 0.21577 | val_0_auc: 0.97341 |  0:00:20s\n",
      "epoch 12 | loss: 0.21678 | val_0_auc: 0.9744  |  0:00:22s\n",
      "epoch 13 | loss: 0.21449 | val_0_auc: 0.97674 |  0:00:23s\n",
      "epoch 14 | loss: 0.20387 | val_0_auc: 0.98035 |  0:00:25s\n",
      "epoch 15 | loss: 0.20512 | val_0_auc: 0.97196 |  0:00:27s\n",
      "epoch 16 | loss: 0.20998 | val_0_auc: 0.9772  |  0:00:29s\n",
      "epoch 17 | loss: 0.18902 | val_0_auc: 0.97978 |  0:00:30s\n",
      "epoch 18 | loss: 0.19324 | val_0_auc: 0.98373 |  0:00:32s\n",
      "epoch 19 | loss: 0.18464 | val_0_auc: 0.9853  |  0:00:34s\n",
      "epoch 20 | loss: 0.18453 | val_0_auc: 0.98204 |  0:00:35s\n",
      "epoch 21 | loss: 0.20336 | val_0_auc: 0.98228 |  0:00:37s\n",
      "epoch 22 | loss: 0.20519 | val_0_auc: 0.98467 |  0:00:39s\n",
      "epoch 23 | loss: 0.19846 | val_0_auc: 0.98958 |  0:00:40s\n",
      "epoch 24 | loss: 0.17624 | val_0_auc: 0.98616 |  0:00:42s\n",
      "epoch 25 | loss: 0.17796 | val_0_auc: 0.98626 |  0:00:44s\n",
      "epoch 26 | loss: 0.16421 | val_0_auc: 0.99052 |  0:00:45s\n",
      "epoch 27 | loss: 0.16044 | val_0_auc: 0.98588 |  0:00:47s\n",
      "epoch 28 | loss: 0.15726 | val_0_auc: 0.98412 |  0:00:49s\n",
      "epoch 29 | loss: 0.16117 | val_0_auc: 0.98718 |  0:00:51s\n",
      "epoch 30 | loss: 0.16765 | val_0_auc: 0.98629 |  0:00:52s\n",
      "epoch 31 | loss: 0.16733 | val_0_auc: 0.98693 |  0:00:54s\n",
      "epoch 32 | loss: 0.15654 | val_0_auc: 0.98929 |  0:00:56s\n",
      "epoch 33 | loss: 0.15932 | val_0_auc: 0.98946 |  0:00:57s\n",
      "epoch 34 | loss: 0.15668 | val_0_auc: 0.98977 |  0:00:59s\n",
      "epoch 35 | loss: 0.15212 | val_0_auc: 0.98794 |  0:01:01s\n",
      "epoch 36 | loss: 0.15387 | val_0_auc: 0.98969 |  0:01:02s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.99052\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99052342  of boosting iteration \n",
      "Iteration No: 19 ended. Evaluation done at random point.\n",
      "Time taken: 63.4692\n",
      "Function value obtained: -0.9905\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 20 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1057326754714087, 'lambda_sparse': 0.08600329283784976, 'n_steps': 9, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.00872 | val_0_auc: 0.86906 |  0:00:02s\n",
      "epoch 1  | loss: 0.46114 | val_0_auc: 0.90379 |  0:00:04s\n",
      "epoch 2  | loss: 0.40578 | val_0_auc: 0.92313 |  0:00:06s\n",
      "epoch 3  | loss: 0.34509 | val_0_auc: 0.9162  |  0:00:08s\n",
      "epoch 4  | loss: 0.33046 | val_0_auc: 0.93743 |  0:00:10s\n",
      "epoch 5  | loss: 0.346   | val_0_auc: 0.94977 |  0:00:12s\n",
      "epoch 6  | loss: 0.36024 | val_0_auc: 0.95196 |  0:00:14s\n",
      "epoch 7  | loss: 0.35172 | val_0_auc: 0.96187 |  0:00:17s\n",
      "epoch 8  | loss: 0.29011 | val_0_auc: 0.97094 |  0:00:19s\n",
      "epoch 9  | loss: 0.28378 | val_0_auc: 0.96195 |  0:00:21s\n",
      "epoch 10 | loss: 0.23551 | val_0_auc: 0.96685 |  0:00:23s\n",
      "epoch 11 | loss: 0.23694 | val_0_auc: 0.96899 |  0:00:25s\n",
      "epoch 12 | loss: 0.22713 | val_0_auc: 0.97969 |  0:00:27s\n",
      "epoch 13 | loss: 0.22024 | val_0_auc: 0.97169 |  0:00:29s\n",
      "epoch 14 | loss: 0.21778 | val_0_auc: 0.98313 |  0:00:31s\n",
      "epoch 15 | loss: 0.20759 | val_0_auc: 0.97623 |  0:00:33s\n",
      "epoch 16 | loss: 0.20139 | val_0_auc: 0.98611 |  0:00:36s\n",
      "epoch 17 | loss: 0.1986  | val_0_auc: 0.98256 |  0:00:38s\n",
      "epoch 18 | loss: 0.17954 | val_0_auc: 0.98815 |  0:00:40s\n",
      "epoch 19 | loss: 0.17738 | val_0_auc: 0.97824 |  0:00:42s\n",
      "epoch 20 | loss: 0.18472 | val_0_auc: 0.98388 |  0:00:44s\n",
      "epoch 21 | loss: 0.18428 | val_0_auc: 0.98468 |  0:00:46s\n",
      "epoch 22 | loss: 0.18518 | val_0_auc: 0.98704 |  0:00:48s\n",
      "epoch 23 | loss: 0.17303 | val_0_auc: 0.98588 |  0:00:50s\n",
      "epoch 24 | loss: 0.17202 | val_0_auc: 0.98519 |  0:00:52s\n",
      "epoch 25 | loss: 0.16632 | val_0_auc: 0.98178 |  0:00:54s\n",
      "epoch 26 | loss: 0.16607 | val_0_auc: 0.98282 |  0:00:56s\n",
      "epoch 27 | loss: 0.16372 | val_0_auc: 0.98483 |  0:00:59s\n",
      "epoch 28 | loss: 0.16103 | val_0_auc: 0.99195 |  0:01:01s\n",
      "epoch 29 | loss: 0.16883 | val_0_auc: 0.99161 |  0:01:03s\n",
      "epoch 30 | loss: 0.15767 | val_0_auc: 0.99114 |  0:01:05s\n",
      "epoch 31 | loss: 0.15862 | val_0_auc: 0.98957 |  0:01:07s\n",
      "epoch 32 | loss: 0.14539 | val_0_auc: 0.99199 |  0:01:09s\n",
      "epoch 33 | loss: 0.14247 | val_0_auc: 0.98827 |  0:01:11s\n",
      "epoch 34 | loss: 0.13966 | val_0_auc: 0.99409 |  0:01:13s\n",
      "epoch 35 | loss: 0.14209 | val_0_auc: 0.99191 |  0:01:15s\n",
      "epoch 36 | loss: 0.13186 | val_0_auc: 0.99267 |  0:01:18s\n",
      "epoch 37 | loss: 0.14154 | val_0_auc: 0.99365 |  0:01:20s\n",
      "epoch 38 | loss: 0.132   | val_0_auc: 0.99036 |  0:01:22s\n",
      "epoch 39 | loss: 0.12934 | val_0_auc: 0.99047 |  0:01:24s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 34 and best_val_0_auc = 0.99409\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99409328  of boosting iteration \n",
      "Iteration No: 20 ended. Evaluation done at random point.\n",
      "Time taken: 85.0251\n",
      "Function value obtained: -0.9941\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 21 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1049038654983714, 'lambda_sparse': 0.05005154685693937, 'n_steps': 6, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.79937 | val_0_auc: 0.80947 |  0:00:01s\n",
      "epoch 1  | loss: 0.48766 | val_0_auc: 0.86059 |  0:00:02s\n",
      "epoch 2  | loss: 0.36674 | val_0_auc: 0.92223 |  0:00:03s\n",
      "epoch 3  | loss: 0.32297 | val_0_auc: 0.92846 |  0:00:04s\n",
      "epoch 4  | loss: 0.28748 | val_0_auc: 0.92712 |  0:00:06s\n",
      "epoch 5  | loss: 0.27574 | val_0_auc: 0.94153 |  0:00:07s\n",
      "epoch 6  | loss: 0.26013 | val_0_auc: 0.94022 |  0:00:08s\n",
      "epoch 7  | loss: 0.23863 | val_0_auc: 0.95897 |  0:00:09s\n",
      "epoch 8  | loss: 0.22773 | val_0_auc: 0.96342 |  0:00:11s\n",
      "epoch 9  | loss: 0.22103 | val_0_auc: 0.97664 |  0:00:12s\n",
      "epoch 10 | loss: 0.21972 | val_0_auc: 0.97555 |  0:00:13s\n",
      "epoch 11 | loss: 0.21703 | val_0_auc: 0.97244 |  0:00:14s\n",
      "epoch 12 | loss: 0.20746 | val_0_auc: 0.97755 |  0:00:15s\n",
      "epoch 13 | loss: 0.20856 | val_0_auc: 0.97595 |  0:00:17s\n",
      "epoch 14 | loss: 0.20574 | val_0_auc: 0.9668  |  0:00:18s\n",
      "epoch 15 | loss: 0.19966 | val_0_auc: 0.96844 |  0:00:19s\n",
      "epoch 16 | loss: 0.19418 | val_0_auc: 0.97606 |  0:00:20s\n",
      "epoch 17 | loss: 0.17941 | val_0_auc: 0.97344 |  0:00:22s\n",
      "epoch 18 | loss: 0.17615 | val_0_auc: 0.98209 |  0:00:23s\n",
      "epoch 19 | loss: 0.17737 | val_0_auc: 0.97684 |  0:00:24s\n",
      "epoch 20 | loss: 0.17642 | val_0_auc: 0.98402 |  0:00:25s\n",
      "epoch 21 | loss: 0.18279 | val_0_auc: 0.97408 |  0:00:27s\n",
      "epoch 22 | loss: 0.16945 | val_0_auc: 0.98279 |  0:00:28s\n",
      "epoch 23 | loss: 0.17746 | val_0_auc: 0.98514 |  0:00:29s\n",
      "epoch 24 | loss: 0.15955 | val_0_auc: 0.98623 |  0:00:30s\n",
      "epoch 25 | loss: 0.15147 | val_0_auc: 0.98329 |  0:00:31s\n",
      "epoch 26 | loss: 0.14401 | val_0_auc: 0.9875  |  0:00:33s\n",
      "epoch 27 | loss: 0.14948 | val_0_auc: 0.98682 |  0:00:34s\n",
      "epoch 28 | loss: 0.14836 | val_0_auc: 0.98797 |  0:00:35s\n",
      "epoch 29 | loss: 0.14357 | val_0_auc: 0.98811 |  0:00:36s\n",
      "epoch 30 | loss: 0.13827 | val_0_auc: 0.98709 |  0:00:37s\n",
      "epoch 31 | loss: 0.14337 | val_0_auc: 0.9895  |  0:00:39s\n",
      "epoch 32 | loss: 0.15059 | val_0_auc: 0.98844 |  0:00:40s\n",
      "epoch 33 | loss: 0.13581 | val_0_auc: 0.98847 |  0:00:41s\n",
      "epoch 34 | loss: 0.13585 | val_0_auc: 0.98985 |  0:00:42s\n",
      "epoch 35 | loss: 0.13213 | val_0_auc: 0.98319 |  0:00:44s\n",
      "epoch 36 | loss: 0.13143 | val_0_auc: 0.98917 |  0:00:45s\n",
      "epoch 37 | loss: 0.1276  | val_0_auc: 0.98761 |  0:00:46s\n",
      "epoch 38 | loss: 0.13203 | val_0_auc: 0.99107 |  0:00:47s\n",
      "epoch 39 | loss: 0.13153 | val_0_auc: 0.99126 |  0:00:48s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99126\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99125655  of boosting iteration \n",
      "Iteration No: 21 ended. Evaluation done at random point.\n",
      "Time taken: 49.2689\n",
      "Function value obtained: -0.9913\n",
      "Current minimum: -0.9952\n",
      "Iteration No: 22 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6092557133043461, 'lambda_sparse': 0.04471862206640546, 'n_steps': 4, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.64125 | val_0_auc: 0.83815 |  0:00:01s\n",
      "epoch 1  | loss: 0.42039 | val_0_auc: 0.91203 |  0:00:02s\n",
      "epoch 2  | loss: 0.31881 | val_0_auc: 0.93009 |  0:00:02s\n",
      "epoch 3  | loss: 0.27184 | val_0_auc: 0.93889 |  0:00:04s\n",
      "epoch 4  | loss: 0.23832 | val_0_auc: 0.95987 |  0:00:05s\n",
      "epoch 5  | loss: 0.20963 | val_0_auc: 0.96531 |  0:00:06s\n",
      "epoch 6  | loss: 0.20592 | val_0_auc: 0.97391 |  0:00:07s\n",
      "epoch 7  | loss: 0.20231 | val_0_auc: 0.97    |  0:00:08s\n",
      "epoch 8  | loss: 0.19054 | val_0_auc: 0.9779  |  0:00:09s\n",
      "epoch 9  | loss: 0.18683 | val_0_auc: 0.97752 |  0:00:10s\n",
      "epoch 10 | loss: 0.17644 | val_0_auc: 0.97702 |  0:00:11s\n",
      "epoch 11 | loss: 0.18165 | val_0_auc: 0.97981 |  0:00:12s\n",
      "epoch 12 | loss: 0.16832 | val_0_auc: 0.97849 |  0:00:13s\n",
      "epoch 13 | loss: 0.15455 | val_0_auc: 0.98645 |  0:00:14s\n",
      "epoch 14 | loss: 0.15459 | val_0_auc: 0.98425 |  0:00:15s\n",
      "epoch 15 | loss: 0.14215 | val_0_auc: 0.99153 |  0:00:16s\n",
      "epoch 16 | loss: 0.13364 | val_0_auc: 0.99151 |  0:00:17s\n",
      "epoch 17 | loss: 0.12613 | val_0_auc: 0.9894  |  0:00:17s\n",
      "epoch 18 | loss: 0.1273  | val_0_auc: 0.98813 |  0:00:18s\n",
      "epoch 19 | loss: 0.12641 | val_0_auc: 0.9885  |  0:00:19s\n",
      "epoch 20 | loss: 0.11828 | val_0_auc: 0.99462 |  0:00:20s\n",
      "epoch 21 | loss: 0.12044 | val_0_auc: 0.99266 |  0:00:21s\n",
      "epoch 22 | loss: 0.12916 | val_0_auc: 0.9941  |  0:00:22s\n",
      "epoch 23 | loss: 0.11905 | val_0_auc: 0.99105 |  0:00:23s\n",
      "epoch 24 | loss: 0.12207 | val_0_auc: 0.99316 |  0:00:24s\n",
      "epoch 25 | loss: 0.11792 | val_0_auc: 0.99344 |  0:00:25s\n",
      "epoch 26 | loss: 0.11237 | val_0_auc: 0.99574 |  0:00:26s\n",
      "epoch 27 | loss: 0.1131  | val_0_auc: 0.99481 |  0:00:27s\n",
      "epoch 28 | loss: 0.10665 | val_0_auc: 0.9943  |  0:00:28s\n",
      "epoch 29 | loss: 0.10877 | val_0_auc: 0.99567 |  0:00:29s\n",
      "epoch 30 | loss: 0.10826 | val_0_auc: 0.99422 |  0:00:30s\n",
      "epoch 31 | loss: 0.11009 | val_0_auc: 0.994   |  0:00:31s\n",
      "epoch 32 | loss: 0.11041 | val_0_auc: 0.99428 |  0:00:32s\n",
      "epoch 33 | loss: 0.10728 | val_0_auc: 0.99004 |  0:00:33s\n",
      "epoch 34 | loss: 0.1035  | val_0_auc: 0.99413 |  0:00:34s\n",
      "epoch 35 | loss: 0.10598 | val_0_auc: 0.99426 |  0:00:35s\n",
      "epoch 36 | loss: 0.11926 | val_0_auc: 0.99325 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_val_0_auc = 0.99574\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99574075  of boosting iteration \n",
      "Iteration No: 22 ended. Evaluation done at random point.\n",
      "Time taken: 37.2558\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9957\n",
      "Iteration No: 23 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.595496549887517, 'lambda_sparse': 0.08691860857137684, 'n_steps': 3, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.75566 | val_0_auc: 0.80278 |  0:00:00s\n",
      "epoch 1  | loss: 0.4739  | val_0_auc: 0.87929 |  0:00:01s\n",
      "epoch 2  | loss: 0.40002 | val_0_auc: 0.89963 |  0:00:02s\n",
      "epoch 3  | loss: 0.33508 | val_0_auc: 0.91556 |  0:00:02s\n",
      "epoch 4  | loss: 0.28524 | val_0_auc: 0.91806 |  0:00:03s\n",
      "epoch 5  | loss: 0.26309 | val_0_auc: 0.92931 |  0:00:04s\n",
      "epoch 6  | loss: 0.24667 | val_0_auc: 0.93412 |  0:00:05s\n",
      "epoch 7  | loss: 0.23892 | val_0_auc: 0.94075 |  0:00:05s\n",
      "epoch 8  | loss: 0.22135 | val_0_auc: 0.95499 |  0:00:06s\n",
      "epoch 9  | loss: 0.22294 | val_0_auc: 0.96575 |  0:00:07s\n",
      "epoch 10 | loss: 0.2072  | val_0_auc: 0.96218 |  0:00:08s\n",
      "epoch 11 | loss: 0.1991  | val_0_auc: 0.96355 |  0:00:08s\n",
      "epoch 12 | loss: 0.2006  | val_0_auc: 0.95948 |  0:00:09s\n",
      "epoch 13 | loss: 0.18894 | val_0_auc: 0.96873 |  0:00:10s\n",
      "epoch 14 | loss: 0.18226 | val_0_auc: 0.97085 |  0:00:10s\n",
      "epoch 15 | loss: 0.17631 | val_0_auc: 0.97384 |  0:00:11s\n",
      "epoch 16 | loss: 0.16878 | val_0_auc: 0.97484 |  0:00:12s\n",
      "epoch 17 | loss: 0.16183 | val_0_auc: 0.98415 |  0:00:13s\n",
      "epoch 18 | loss: 0.14937 | val_0_auc: 0.99013 |  0:00:13s\n",
      "epoch 19 | loss: 0.14825 | val_0_auc: 0.98797 |  0:00:14s\n",
      "epoch 20 | loss: 0.15138 | val_0_auc: 0.98796 |  0:00:15s\n",
      "epoch 21 | loss: 0.1413  | val_0_auc: 0.98983 |  0:00:15s\n",
      "epoch 22 | loss: 0.12859 | val_0_auc: 0.99048 |  0:00:16s\n",
      "epoch 23 | loss: 0.11863 | val_0_auc: 0.98948 |  0:00:17s\n",
      "epoch 24 | loss: 0.12633 | val_0_auc: 0.98953 |  0:00:18s\n",
      "epoch 25 | loss: 0.11639 | val_0_auc: 0.99059 |  0:00:18s\n",
      "epoch 26 | loss: 0.10638 | val_0_auc: 0.99244 |  0:00:19s\n",
      "epoch 27 | loss: 0.1153  | val_0_auc: 0.99409 |  0:00:20s\n",
      "epoch 28 | loss: 0.11005 | val_0_auc: 0.99504 |  0:00:21s\n",
      "epoch 29 | loss: 0.10893 | val_0_auc: 0.99416 |  0:00:21s\n",
      "epoch 30 | loss: 0.10862 | val_0_auc: 0.99529 |  0:00:22s\n",
      "epoch 31 | loss: 0.0988  | val_0_auc: 0.99634 |  0:00:23s\n",
      "epoch 32 | loss: 0.09788 | val_0_auc: 0.99479 |  0:00:24s\n",
      "epoch 33 | loss: 0.10056 | val_0_auc: 0.99694 |  0:00:24s\n",
      "epoch 34 | loss: 0.0976  | val_0_auc: 0.99475 |  0:00:25s\n",
      "epoch 35 | loss: 0.09864 | val_0_auc: 0.99492 |  0:00:26s\n",
      "epoch 36 | loss: 0.09708 | val_0_auc: 0.99652 |  0:00:26s\n",
      "epoch 37 | loss: 0.09595 | val_0_auc: 0.99575 |  0:00:27s\n",
      "epoch 38 | loss: 0.09681 | val_0_auc: 0.99428 |  0:00:28s\n",
      "epoch 39 | loss: 0.08656 | val_0_auc: 0.99487 |  0:00:29s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.99694\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99694458  of boosting iteration \n",
      "Iteration No: 23 ended. Evaluation done at random point.\n",
      "Time taken: 29.4306\n",
      "Function value obtained: -0.9969\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 24 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0305901470468117, 'lambda_sparse': 0.007633503826980348, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.9259  | val_0_auc: 0.82905 |  0:00:01s\n",
      "epoch 1  | loss: 0.41285 | val_0_auc: 0.88017 |  0:00:03s\n",
      "epoch 2  | loss: 0.33287 | val_0_auc: 0.92134 |  0:00:05s\n",
      "epoch 3  | loss: 0.2895  | val_0_auc: 0.94041 |  0:00:06s\n",
      "epoch 4  | loss: 0.26497 | val_0_auc: 0.93901 |  0:00:08s\n",
      "epoch 5  | loss: 0.24507 | val_0_auc: 0.95904 |  0:00:10s\n",
      "epoch 6  | loss: 0.21936 | val_0_auc: 0.96733 |  0:00:11s\n",
      "epoch 7  | loss: 0.19919 | val_0_auc: 0.98185 |  0:00:13s\n",
      "epoch 8  | loss: 0.17969 | val_0_auc: 0.9767  |  0:00:15s\n",
      "epoch 9  | loss: 0.16746 | val_0_auc: 0.97771 |  0:00:17s\n",
      "epoch 10 | loss: 0.16715 | val_0_auc: 0.97847 |  0:00:18s\n",
      "epoch 11 | loss: 0.15312 | val_0_auc: 0.98151 |  0:00:20s\n",
      "epoch 12 | loss: 0.14479 | val_0_auc: 0.98267 |  0:00:22s\n",
      "epoch 13 | loss: 0.14449 | val_0_auc: 0.98092 |  0:00:24s\n",
      "epoch 14 | loss: 0.13922 | val_0_auc: 0.98235 |  0:00:25s\n",
      "epoch 15 | loss: 0.1309  | val_0_auc: 0.98311 |  0:00:27s\n",
      "epoch 16 | loss: 0.13218 | val_0_auc: 0.983   |  0:00:29s\n",
      "epoch 17 | loss: 0.13916 | val_0_auc: 0.98475 |  0:00:30s\n",
      "epoch 18 | loss: 0.13607 | val_0_auc: 0.98366 |  0:00:32s\n",
      "epoch 19 | loss: 0.13212 | val_0_auc: 0.98824 |  0:00:34s\n",
      "epoch 20 | loss: 0.13042 | val_0_auc: 0.97967 |  0:00:35s\n",
      "epoch 21 | loss: 0.13142 | val_0_auc: 0.98721 |  0:00:37s\n",
      "epoch 22 | loss: 0.11966 | val_0_auc: 0.9897  |  0:00:39s\n",
      "epoch 23 | loss: 0.11582 | val_0_auc: 0.99261 |  0:00:41s\n",
      "epoch 24 | loss: 0.11822 | val_0_auc: 0.99203 |  0:00:42s\n",
      "epoch 25 | loss: 0.11121 | val_0_auc: 0.98767 |  0:00:44s\n",
      "epoch 26 | loss: 0.10884 | val_0_auc: 0.99015 |  0:00:46s\n",
      "epoch 27 | loss: 0.10727 | val_0_auc: 0.98922 |  0:00:47s\n",
      "epoch 28 | loss: 0.11238 | val_0_auc: 0.98972 |  0:00:49s\n",
      "epoch 29 | loss: 0.10789 | val_0_auc: 0.99123 |  0:00:51s\n",
      "epoch 30 | loss: 0.11409 | val_0_auc: 0.99013 |  0:00:52s\n",
      "epoch 31 | loss: 0.11706 | val_0_auc: 0.99397 |  0:00:54s\n",
      "epoch 32 | loss: 0.10508 | val_0_auc: 0.98851 |  0:00:56s\n",
      "epoch 33 | loss: 0.10651 | val_0_auc: 0.99292 |  0:00:58s\n",
      "epoch 34 | loss: 0.10706 | val_0_auc: 0.9923  |  0:00:59s\n",
      "epoch 35 | loss: 0.11494 | val_0_auc: 0.99141 |  0:01:01s\n",
      "epoch 36 | loss: 0.10873 | val_0_auc: 0.99214 |  0:01:03s\n",
      "epoch 37 | loss: 0.1093  | val_0_auc: 0.9919  |  0:01:04s\n",
      "epoch 38 | loss: 0.09831 | val_0_auc: 0.99276 |  0:01:06s\n",
      "epoch 39 | loss: 0.09906 | val_0_auc: 0.99247 |  0:01:08s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 31 and best_val_0_auc = 0.99397\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9939704  of boosting iteration \n",
      "Iteration No: 24 ended. Evaluation done at random point.\n",
      "Time taken: 68.8647\n",
      "Function value obtained: -0.9940\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 25 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5817175510317876, 'lambda_sparse': 0.08493359961251172, 'n_steps': 7, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.3638  | val_0_auc: 0.70842 |  0:00:01s\n",
      "epoch 1  | loss: 0.61249 | val_0_auc: 0.8052  |  0:00:03s\n",
      "epoch 2  | loss: 0.49337 | val_0_auc: 0.85257 |  0:00:04s\n",
      "epoch 3  | loss: 0.44795 | val_0_auc: 0.87845 |  0:00:06s\n",
      "epoch 4  | loss: 0.42952 | val_0_auc: 0.8876  |  0:00:07s\n",
      "epoch 5  | loss: 0.38414 | val_0_auc: 0.90124 |  0:00:09s\n",
      "epoch 6  | loss: 0.36703 | val_0_auc: 0.92286 |  0:00:11s\n",
      "epoch 7  | loss: 0.40037 | val_0_auc: 0.91993 |  0:00:12s\n",
      "epoch 8  | loss: 0.362   | val_0_auc: 0.92801 |  0:00:14s\n",
      "epoch 9  | loss: 0.32578 | val_0_auc: 0.91499 |  0:00:15s\n",
      "epoch 10 | loss: 0.31033 | val_0_auc: 0.92122 |  0:00:17s\n",
      "epoch 11 | loss: 0.30675 | val_0_auc: 0.93287 |  0:00:18s\n",
      "epoch 12 | loss: 0.2998  | val_0_auc: 0.93374 |  0:00:20s\n",
      "epoch 13 | loss: 0.29143 | val_0_auc: 0.93186 |  0:00:21s\n",
      "epoch 14 | loss: 0.28659 | val_0_auc: 0.92055 |  0:00:23s\n",
      "epoch 15 | loss: 0.29822 | val_0_auc: 0.92128 |  0:00:25s\n",
      "epoch 16 | loss: 0.29865 | val_0_auc: 0.91461 |  0:00:26s\n",
      "epoch 17 | loss: 0.28816 | val_0_auc: 0.92133 |  0:00:28s\n",
      "epoch 18 | loss: 0.28376 | val_0_auc: 0.94077 |  0:00:29s\n",
      "epoch 19 | loss: 0.26246 | val_0_auc: 0.94732 |  0:00:31s\n",
      "epoch 20 | loss: 0.25586 | val_0_auc: 0.95382 |  0:00:32s\n",
      "epoch 21 | loss: 0.24693 | val_0_auc: 0.94962 |  0:00:34s\n",
      "epoch 22 | loss: 0.25277 | val_0_auc: 0.95172 |  0:00:35s\n",
      "epoch 23 | loss: 0.26    | val_0_auc: 0.95363 |  0:00:37s\n",
      "epoch 24 | loss: 0.25931 | val_0_auc: 0.9569  |  0:00:39s\n",
      "epoch 25 | loss: 0.23929 | val_0_auc: 0.951   |  0:00:40s\n",
      "epoch 26 | loss: 0.21696 | val_0_auc: 0.96288 |  0:00:42s\n",
      "epoch 27 | loss: 0.233   | val_0_auc: 0.95427 |  0:00:43s\n",
      "epoch 28 | loss: 0.22595 | val_0_auc: 0.96863 |  0:00:45s\n",
      "epoch 29 | loss: 0.22198 | val_0_auc: 0.96149 |  0:00:46s\n",
      "epoch 30 | loss: 0.23717 | val_0_auc: 0.9492  |  0:00:48s\n",
      "epoch 31 | loss: 0.23484 | val_0_auc: 0.96245 |  0:00:50s\n",
      "epoch 32 | loss: 0.21966 | val_0_auc: 0.96439 |  0:00:51s\n",
      "epoch 33 | loss: 0.21366 | val_0_auc: 0.9773  |  0:00:53s\n",
      "epoch 34 | loss: 0.20691 | val_0_auc: 0.96795 |  0:00:54s\n",
      "epoch 35 | loss: 0.20846 | val_0_auc: 0.9779  |  0:00:56s\n",
      "epoch 36 | loss: 0.20886 | val_0_auc: 0.96965 |  0:00:58s\n",
      "epoch 37 | loss: 0.20549 | val_0_auc: 0.97655 |  0:00:59s\n",
      "epoch 38 | loss: 0.18389 | val_0_auc: 0.97332 |  0:01:01s\n",
      "epoch 39 | loss: 0.18054 | val_0_auc: 0.97739 |  0:01:02s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.9779\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97790395  of boosting iteration \n",
      "Iteration No: 25 ended. Evaluation done at random point.\n",
      "Time taken: 63.2386\n",
      "Function value obtained: -0.9779\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 26 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.079617674389677, 'lambda_sparse': 0.0606082726230558, 'n_steps': 3, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.68524 | val_0_auc: 0.89489 |  0:00:01s\n",
      "epoch 1  | loss: 0.33128 | val_0_auc: 0.93604 |  0:00:02s\n",
      "epoch 2  | loss: 0.26671 | val_0_auc: 0.94599 |  0:00:03s\n",
      "epoch 3  | loss: 0.24491 | val_0_auc: 0.95906 |  0:00:04s\n",
      "epoch 4  | loss: 0.22308 | val_0_auc: 0.96442 |  0:00:05s\n",
      "epoch 5  | loss: 0.2062  | val_0_auc: 0.97055 |  0:00:06s\n",
      "epoch 6  | loss: 0.18729 | val_0_auc: 0.97316 |  0:00:07s\n",
      "epoch 7  | loss: 0.18917 | val_0_auc: 0.97313 |  0:00:09s\n",
      "epoch 8  | loss: 0.19064 | val_0_auc: 0.97984 |  0:00:10s\n",
      "epoch 9  | loss: 0.174   | val_0_auc: 0.98499 |  0:00:11s\n",
      "epoch 10 | loss: 0.17591 | val_0_auc: 0.98549 |  0:00:12s\n",
      "epoch 11 | loss: 0.15972 | val_0_auc: 0.98355 |  0:00:13s\n",
      "epoch 12 | loss: 0.15195 | val_0_auc: 0.98415 |  0:00:14s\n",
      "epoch 13 | loss: 0.14817 | val_0_auc: 0.98672 |  0:00:15s\n",
      "epoch 14 | loss: 0.14352 | val_0_auc: 0.98751 |  0:00:17s\n",
      "epoch 15 | loss: 0.14581 | val_0_auc: 0.98564 |  0:00:18s\n",
      "epoch 16 | loss: 0.14914 | val_0_auc: 0.9848  |  0:00:19s\n",
      "epoch 17 | loss: 0.14411 | val_0_auc: 0.98989 |  0:00:20s\n",
      "epoch 18 | loss: 0.13473 | val_0_auc: 0.98966 |  0:00:21s\n",
      "epoch 19 | loss: 0.12478 | val_0_auc: 0.9919  |  0:00:22s\n",
      "epoch 20 | loss: 0.12278 | val_0_auc: 0.99285 |  0:00:23s\n",
      "epoch 21 | loss: 0.11713 | val_0_auc: 0.99315 |  0:00:25s\n",
      "epoch 22 | loss: 0.1039  | val_0_auc: 0.99524 |  0:00:26s\n",
      "epoch 23 | loss: 0.11444 | val_0_auc: 0.99546 |  0:00:27s\n",
      "epoch 24 | loss: 0.10542 | val_0_auc: 0.99556 |  0:00:28s\n",
      "epoch 25 | loss: 0.10504 | val_0_auc: 0.99556 |  0:00:29s\n",
      "epoch 26 | loss: 0.10179 | val_0_auc: 0.99316 |  0:00:30s\n",
      "epoch 27 | loss: 0.10336 | val_0_auc: 0.99416 |  0:00:31s\n",
      "epoch 28 | loss: 0.11454 | val_0_auc: 0.99474 |  0:00:33s\n",
      "epoch 29 | loss: 0.09821 | val_0_auc: 0.99224 |  0:00:34s\n",
      "epoch 30 | loss: 0.09709 | val_0_auc: 0.99506 |  0:00:35s\n",
      "epoch 31 | loss: 0.10263 | val_0_auc: 0.99498 |  0:00:36s\n",
      "epoch 32 | loss: 0.09305 | val_0_auc: 0.99497 |  0:00:37s\n",
      "epoch 33 | loss: 0.0922  | val_0_auc: 0.99571 |  0:00:38s\n",
      "epoch 34 | loss: 0.09397 | val_0_auc: 0.99441 |  0:00:39s\n",
      "epoch 35 | loss: 0.0894  | val_0_auc: 0.99561 |  0:00:40s\n",
      "epoch 36 | loss: 0.09289 | val_0_auc: 0.99316 |  0:00:42s\n",
      "epoch 37 | loss: 0.09156 | val_0_auc: 0.99328 |  0:00:43s\n",
      "epoch 38 | loss: 0.10142 | val_0_auc: 0.99264 |  0:00:44s\n",
      "epoch 39 | loss: 0.09967 | val_0_auc: 0.99415 |  0:00:45s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_val_0_auc = 0.99571\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.9957095  of boosting iteration \n",
      "Iteration No: 26 ended. Evaluation done at random point.\n",
      "Time taken: 45.9186\n",
      "Function value obtained: -0.9957\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 27 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3894676164457267, 'lambda_sparse': 0.04838719078440819, 'n_steps': 4, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.84267 | val_0_auc: 0.86743 |  0:00:01s\n",
      "epoch 1  | loss: 0.43415 | val_0_auc: 0.88981 |  0:00:02s\n",
      "epoch 2  | loss: 0.34372 | val_0_auc: 0.9134  |  0:00:04s\n",
      "epoch 3  | loss: 0.28614 | val_0_auc: 0.93272 |  0:00:05s\n",
      "epoch 4  | loss: 0.2682  | val_0_auc: 0.95234 |  0:00:07s\n",
      "epoch 5  | loss: 0.23374 | val_0_auc: 0.95788 |  0:00:08s\n",
      "epoch 6  | loss: 0.2119  | val_0_auc: 0.96039 |  0:00:09s\n",
      "epoch 7  | loss: 0.21695 | val_0_auc: 0.96117 |  0:00:11s\n",
      "epoch 8  | loss: 0.21546 | val_0_auc: 0.96454 |  0:00:12s\n",
      "epoch 9  | loss: 0.20256 | val_0_auc: 0.9725  |  0:00:14s\n",
      "epoch 10 | loss: 0.20368 | val_0_auc: 0.9772  |  0:00:15s\n",
      "epoch 11 | loss: 0.18681 | val_0_auc: 0.97623 |  0:00:16s\n",
      "epoch 12 | loss: 0.1802  | val_0_auc: 0.9734  |  0:00:18s\n",
      "epoch 13 | loss: 0.17283 | val_0_auc: 0.97487 |  0:00:19s\n",
      "epoch 14 | loss: 0.17274 | val_0_auc: 0.9781  |  0:00:21s\n",
      "epoch 15 | loss: 0.17463 | val_0_auc: 0.98093 |  0:00:22s\n",
      "epoch 16 | loss: 0.16569 | val_0_auc: 0.97459 |  0:00:23s\n",
      "epoch 17 | loss: 0.16483 | val_0_auc: 0.98293 |  0:00:25s\n",
      "epoch 18 | loss: 0.16226 | val_0_auc: 0.98671 |  0:00:26s\n",
      "epoch 19 | loss: 0.15685 | val_0_auc: 0.98722 |  0:00:28s\n",
      "epoch 20 | loss: 0.14164 | val_0_auc: 0.98733 |  0:00:29s\n",
      "epoch 21 | loss: 0.14741 | val_0_auc: 0.98861 |  0:00:30s\n",
      "epoch 22 | loss: 0.13277 | val_0_auc: 0.98708 |  0:00:32s\n",
      "epoch 23 | loss: 0.12618 | val_0_auc: 0.99163 |  0:00:33s\n",
      "epoch 24 | loss: 0.12154 | val_0_auc: 0.98965 |  0:00:35s\n",
      "epoch 25 | loss: 0.12211 | val_0_auc: 0.98928 |  0:00:36s\n",
      "epoch 26 | loss: 0.12037 | val_0_auc: 0.99097 |  0:00:37s\n",
      "epoch 27 | loss: 0.12488 | val_0_auc: 0.99365 |  0:00:39s\n",
      "epoch 28 | loss: 0.12253 | val_0_auc: 0.99135 |  0:00:40s\n",
      "epoch 29 | loss: 0.12001 | val_0_auc: 0.99249 |  0:00:42s\n",
      "epoch 30 | loss: 0.11755 | val_0_auc: 0.99172 |  0:00:43s\n",
      "epoch 31 | loss: 0.11398 | val_0_auc: 0.99402 |  0:00:44s\n",
      "epoch 32 | loss: 0.11091 | val_0_auc: 0.99365 |  0:00:46s\n",
      "epoch 33 | loss: 0.1112  | val_0_auc: 0.99478 |  0:00:47s\n",
      "epoch 34 | loss: 0.11261 | val_0_auc: 0.9932  |  0:00:49s\n",
      "epoch 35 | loss: 0.11243 | val_0_auc: 0.99201 |  0:00:50s\n",
      "epoch 36 | loss: 0.11588 | val_0_auc: 0.99416 |  0:00:51s\n",
      "epoch 37 | loss: 0.11053 | val_0_auc: 0.99538 |  0:00:53s\n",
      "epoch 38 | loss: 0.1064  | val_0_auc: 0.99497 |  0:00:54s\n",
      "epoch 39 | loss: 0.1056  | val_0_auc: 0.993   |  0:00:56s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.99538\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.99537834  of boosting iteration \n",
      "Iteration No: 27 ended. Evaluation done at random point.\n",
      "Time taken: 56.6536\n",
      "Function value obtained: -0.9954\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 28 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5121485163376178, 'lambda_sparse': 0.029888848783104376, 'n_steps': 8, 'n_a': 64, 'n_d': 64, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 1.06977 | val_0_auc: 0.83145 |  0:00:02s\n",
      "epoch 1  | loss: 0.62523 | val_0_auc: 0.85129 |  0:00:05s\n",
      "epoch 2  | loss: 0.63209 | val_0_auc: 0.88152 |  0:00:07s\n",
      "epoch 3  | loss: 0.84974 | val_0_auc: 0.87519 |  0:00:10s\n",
      "epoch 4  | loss: 0.62883 | val_0_auc: 0.88803 |  0:00:12s\n",
      "epoch 5  | loss: 0.73182 | val_0_auc: 0.87791 |  0:00:15s\n",
      "epoch 6  | loss: 0.5772  | val_0_auc: 0.89705 |  0:00:17s\n",
      "epoch 7  | loss: 0.41896 | val_0_auc: 0.88842 |  0:00:20s\n",
      "epoch 8  | loss: 0.39532 | val_0_auc: 0.90952 |  0:00:22s\n",
      "epoch 9  | loss: 0.32833 | val_0_auc: 0.92086 |  0:00:25s\n",
      "epoch 10 | loss: 0.30873 | val_0_auc: 0.926   |  0:00:27s\n",
      "epoch 11 | loss: 0.30398 | val_0_auc: 0.91066 |  0:00:30s\n",
      "epoch 12 | loss: 0.29223 | val_0_auc: 0.93473 |  0:00:33s\n",
      "epoch 13 | loss: 0.30344 | val_0_auc: 0.94237 |  0:00:35s\n",
      "epoch 14 | loss: 0.27714 | val_0_auc: 0.94052 |  0:00:38s\n",
      "epoch 15 | loss: 0.27561 | val_0_auc: 0.94361 |  0:00:40s\n",
      "epoch 16 | loss: 0.28738 | val_0_auc: 0.93622 |  0:00:43s\n",
      "epoch 17 | loss: 0.27901 | val_0_auc: 0.93994 |  0:00:46s\n",
      "epoch 18 | loss: 0.29604 | val_0_auc: 0.94022 |  0:00:48s\n",
      "epoch 19 | loss: 0.27285 | val_0_auc: 0.96246 |  0:00:51s\n",
      "epoch 20 | loss: 0.25587 | val_0_auc: 0.95167 |  0:00:54s\n",
      "epoch 21 | loss: 0.27882 | val_0_auc: 0.94985 |  0:00:56s\n",
      "epoch 22 | loss: 0.35456 | val_0_auc: 0.92993 |  0:00:59s\n",
      "epoch 23 | loss: 0.35208 | val_0_auc: 0.93032 |  0:01:01s\n",
      "epoch 24 | loss: 0.30138 | val_0_auc: 0.94521 |  0:01:04s\n",
      "epoch 25 | loss: 0.25455 | val_0_auc: 0.94507 |  0:01:06s\n",
      "epoch 26 | loss: 0.23818 | val_0_auc: 0.95511 |  0:01:09s\n",
      "epoch 27 | loss: 0.23394 | val_0_auc: 0.96044 |  0:01:11s\n",
      "epoch 28 | loss: 0.24458 | val_0_auc: 0.96031 |  0:01:14s\n",
      "epoch 29 | loss: 0.23829 | val_0_auc: 0.96166 |  0:01:16s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.96246\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.96246233  of boosting iteration \n",
      "Iteration No: 28 ended. Evaluation done at random point.\n",
      "Time taken: 77.8253\n",
      "Function value obtained: -0.9625\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 29 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5704051610091352, 'lambda_sparse': 0.06112902828286277, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.89749 | val_0_auc: 0.74768 |  0:00:01s\n",
      "epoch 1  | loss: 0.7281  | val_0_auc: 0.83098 |  0:00:03s\n",
      "epoch 2  | loss: 0.65504 | val_0_auc: 0.84596 |  0:00:05s\n",
      "epoch 3  | loss: 0.42575 | val_0_auc: 0.88138 |  0:00:07s\n",
      "epoch 4  | loss: 0.34187 | val_0_auc: 0.9228  |  0:00:09s\n",
      "epoch 5  | loss: 0.33548 | val_0_auc: 0.91162 |  0:00:11s\n",
      "epoch 6  | loss: 0.33208 | val_0_auc: 0.93389 |  0:00:13s\n",
      "epoch 7  | loss: 0.29153 | val_0_auc: 0.93497 |  0:00:15s\n",
      "epoch 8  | loss: 0.29719 | val_0_auc: 0.93098 |  0:00:17s\n",
      "epoch 9  | loss: 0.29629 | val_0_auc: 0.93529 |  0:00:19s\n",
      "epoch 10 | loss: 0.30807 | val_0_auc: 0.93531 |  0:00:21s\n",
      "epoch 11 | loss: 0.30407 | val_0_auc: 0.92552 |  0:00:23s\n",
      "epoch 12 | loss: 0.28071 | val_0_auc: 0.9342  |  0:00:25s\n",
      "epoch 13 | loss: 0.27943 | val_0_auc: 0.93545 |  0:00:27s\n",
      "epoch 14 | loss: 0.27582 | val_0_auc: 0.9353  |  0:00:28s\n",
      "epoch 15 | loss: 0.2718  | val_0_auc: 0.94611 |  0:00:30s\n",
      "epoch 16 | loss: 0.29089 | val_0_auc: 0.94086 |  0:00:32s\n",
      "epoch 17 | loss: 0.28118 | val_0_auc: 0.94237 |  0:00:34s\n",
      "epoch 18 | loss: 0.28246 | val_0_auc: 0.94943 |  0:00:36s\n",
      "epoch 19 | loss: 0.26002 | val_0_auc: 0.94703 |  0:00:38s\n",
      "epoch 20 | loss: 0.26655 | val_0_auc: 0.948   |  0:00:40s\n",
      "epoch 21 | loss: 0.25716 | val_0_auc: 0.95774 |  0:00:42s\n",
      "epoch 22 | loss: 0.25151 | val_0_auc: 0.94734 |  0:00:44s\n",
      "epoch 23 | loss: 0.23812 | val_0_auc: 0.95021 |  0:00:46s\n",
      "epoch 24 | loss: 0.2737  | val_0_auc: 0.94446 |  0:00:48s\n",
      "epoch 25 | loss: 0.27652 | val_0_auc: 0.94535 |  0:00:50s\n",
      "epoch 26 | loss: 0.2785  | val_0_auc: 0.94753 |  0:00:52s\n",
      "epoch 27 | loss: 0.25925 | val_0_auc: 0.95178 |  0:00:53s\n",
      "epoch 28 | loss: 0.2538  | val_0_auc: 0.95595 |  0:00:55s\n",
      "epoch 29 | loss: 0.25416 | val_0_auc: 0.95848 |  0:00:57s\n",
      "epoch 30 | loss: 0.26692 | val_0_auc: 0.94845 |  0:00:59s\n",
      "epoch 31 | loss: 0.25698 | val_0_auc: 0.95529 |  0:01:01s\n",
      "epoch 32 | loss: 0.266   | val_0_auc: 0.94722 |  0:01:03s\n",
      "epoch 33 | loss: 0.26668 | val_0_auc: 0.95588 |  0:01:05s\n",
      "epoch 34 | loss: 0.25736 | val_0_auc: 0.95573 |  0:01:07s\n",
      "epoch 35 | loss: 0.24124 | val_0_auc: 0.96185 |  0:01:09s\n",
      "epoch 36 | loss: 0.22624 | val_0_auc: 0.96346 |  0:01:11s\n",
      "epoch 37 | loss: 0.21922 | val_0_auc: 0.97143 |  0:01:13s\n",
      "epoch 38 | loss: 0.19786 | val_0_auc: 0.96649 |  0:01:15s\n",
      "epoch 39 | loss: 0.20685 | val_0_auc: 0.9686  |  0:01:16s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.97143\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.97142863  of boosting iteration \n",
      "Iteration No: 29 ended. Evaluation done at random point.\n",
      "Time taken: 77.7033\n",
      "Function value obtained: -0.9714\n",
      "Current minimum: -0.9969\n",
      "Iteration No: 30 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0574697539673081, 'lambda_sparse': 0.09426805995337947, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.73081 | val_0_auc: 0.83862 |  0:00:00s\n",
      "epoch 1  | loss: 0.47619 | val_0_auc: 0.88782 |  0:00:01s\n",
      "epoch 2  | loss: 0.38272 | val_0_auc: 0.92454 |  0:00:02s\n",
      "epoch 3  | loss: 0.31753 | val_0_auc: 0.92834 |  0:00:03s\n",
      "epoch 4  | loss: 0.29258 | val_0_auc: 0.93984 |  0:00:04s\n",
      "epoch 5  | loss: 0.27731 | val_0_auc: 0.93245 |  0:00:05s\n",
      "epoch 6  | loss: 0.26007 | val_0_auc: 0.9381  |  0:00:06s\n",
      "epoch 7  | loss: 0.25313 | val_0_auc: 0.94327 |  0:00:07s\n",
      "epoch 8  | loss: 0.2485  | val_0_auc: 0.95015 |  0:00:08s\n",
      "epoch 9  | loss: 0.24351 | val_0_auc: 0.94244 |  0:00:09s\n",
      "epoch 10 | loss: 0.23557 | val_0_auc: 0.94944 |  0:00:09s\n",
      "epoch 11 | loss: 0.23374 | val_0_auc: 0.94743 |  0:00:10s\n",
      "epoch 12 | loss: 0.21953 | val_0_auc: 0.95101 |  0:00:11s\n",
      "epoch 13 | loss: 0.22281 | val_0_auc: 0.95988 |  0:00:12s\n",
      "epoch 14 | loss: 0.20892 | val_0_auc: 0.96334 |  0:00:13s\n",
      "epoch 15 | loss: 0.20035 | val_0_auc: 0.96196 |  0:00:14s\n",
      "epoch 16 | loss: 0.19892 | val_0_auc: 0.96718 |  0:00:15s\n",
      "epoch 17 | loss: 0.19736 | val_0_auc: 0.96587 |  0:00:16s\n",
      "epoch 18 | loss: 0.19502 | val_0_auc: 0.95791 |  0:00:17s\n",
      "epoch 19 | loss: 0.19411 | val_0_auc: 0.96131 |  0:00:17s\n",
      "epoch 20 | loss: 0.1864  | val_0_auc: 0.9641  |  0:00:18s\n",
      "epoch 21 | loss: 0.18362 | val_0_auc: 0.96313 |  0:00:19s\n",
      "epoch 22 | loss: 0.19059 | val_0_auc: 0.96432 |  0:00:20s\n",
      "epoch 23 | loss: 0.18743 | val_0_auc: 0.9682  |  0:00:21s\n",
      "epoch 24 | loss: 0.18747 | val_0_auc: 0.95977 |  0:00:22s\n",
      "epoch 25 | loss: 0.18017 | val_0_auc: 0.9638  |  0:00:23s\n",
      "epoch 26 | loss: 0.18703 | val_0_auc: 0.96528 |  0:00:24s\n",
      "epoch 27 | loss: 0.18577 | val_0_auc: 0.96455 |  0:00:24s\n",
      "epoch 28 | loss: 0.18494 | val_0_auc: 0.96539 |  0:00:25s\n",
      "epoch 29 | loss: 0.16148 | val_0_auc: 0.98075 |  0:00:26s\n",
      "epoch 30 | loss: 0.15186 | val_0_auc: 0.98152 |  0:00:27s\n",
      "epoch 31 | loss: 0.14581 | val_0_auc: 0.98411 |  0:00:28s\n",
      "epoch 32 | loss: 0.13576 | val_0_auc: 0.9841  |  0:00:29s\n",
      "epoch 33 | loss: 0.13982 | val_0_auc: 0.9874  |  0:00:30s\n",
      "epoch 34 | loss: 0.12879 | val_0_auc: 0.98189 |  0:00:31s\n",
      "epoch 35 | loss: 0.13164 | val_0_auc: 0.98756 |  0:00:32s\n",
      "epoch 36 | loss: 0.13641 | val_0_auc: 0.98393 |  0:00:33s\n",
      "epoch 37 | loss: 0.13145 | val_0_auc: 0.98862 |  0:00:33s\n",
      "epoch 38 | loss: 0.12468 | val_0_auc: 0.9896  |  0:00:34s\n",
      "epoch 39 | loss: 0.1222  | val_0_auc: 0.99188 |  0:00:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.99188\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.99188138  of boosting iteration \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 30 ended. Evaluation done at random point.\n",
      "Time taken: 38.6395\n",
      "Function value obtained: -0.9919\n",
      "Current minimum: -0.9969\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"STARTED:\", dataset)\n",
    "    train = pd.read_csv(\"data/synthetic/{}/train.csv\".format(dataset))\n",
    "    valid = pd.read_csv(\"data/synthetic/{}/val.csv\".format(dataset))\n",
    "    \n",
    "    train_x = train.drop([\"TARGET\"], axis=1).values\n",
    "    train_y = train[\"TARGET\"].values\n",
    "    \n",
    "    valid_x = valid.drop([\"TARGET\"], axis=1).values\n",
    "    valid_y = valid[\"TARGET\"].values\n",
    "\n",
    "    results = dict()\n",
    "    for clf_name, clf, search_range, params in classificators:\n",
    "        res_gp = gp_minimize(BayesianOptimization(clf, params, search_range, clf_name), search_range, n_jobs=-1, verbose=True, n_initial_points=30, n_calls=30)\n",
    "        results[clf_name] = res_gp\n",
    "        with open('{}_hp_kdd_{}.pickle'.format(clf_name, dataset), 'wb') as f:\n",
    "            pickle.dump(res_gp.x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(results[\"LGBMClassifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(results[\"TABNETClassifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1911040149797038, 6, 875]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"LGBMClassifier\"].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.103989684658078, 5, 805]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"XGBClassifier\"].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0131059206061017, 0.0051151725754103195, 10, 54]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"TABNETClassifier\"].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: syn1\n",
      "\tLGBMClassifier [0.12084853580348885, 14, 630]\n",
      "\tXGBClassifier [0.6407388649081306, 11, 145]\n",
      "\tTABNETClassifier [1.8294789242800777, 0.024825767831183742, 3, 8, 0.7]\n",
      "Dataset: syn2\n",
      "\tLGBMClassifier [0.23995731404370924, 13, 483]\n",
      "\tXGBClassifier [0.29615234675507385, 12, 386]\n",
      "\tTABNETClassifier [1.0726430938336577, 0.033960576396166633, 5, 32, 0.7]\n",
      "Dataset: syn3\n",
      "\tLGBMClassifier [0.4540837973031734, 3, 315]\n",
      "\tXGBClassifier [0.8880765022899698, 4, 675]\n",
      "\tTABNETClassifier [1.5677018396290885, 0.04841913603075304, 4, 32, 0.6]\n",
      "Dataset: syn4\n",
      "\tLGBMClassifier [0.2618422323665017, 11, 965]\n",
      "\tXGBClassifier [0.3535422004334621, 7, 969]\n",
      "\tTABNETClassifier [1.0840374047235146, 0.07756898575730785, 8, 16, 0.6]\n",
      "Dataset: syn5\n",
      "\tLGBMClassifier [0.14244374976727375, 5, 198]\n",
      "\tXGBClassifier [0.437344281810245, 5, 920]\n",
      "\tTABNETClassifier [1.1227966185155351, 0.05232096763520789, 4, 24, 0.7]\n",
      "Dataset: syn6\n",
      "\tLGBMClassifier [0.1911040149797038, 6, 875]\n",
      "\tXGBClassifier [0.103989684658078, 5, 805]\n",
      "\tTABNETClassifier [1.595496549887517, 0.08691860857137684, 3, 16, 0.6]\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"Dataset:\", dataset)\n",
    "    for classifier in classificators:\n",
    "        with open(\"{}_hp_kdd_{}.pickle\".format(classifier[0], dataset), \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "            print('\\t' + classifier[0], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
