{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, log_loss\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "from bo_parameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_tuned = {\n",
    "    \"learning_rate\" : 0.23995731404370924, \n",
    "    \"max_depth\" : 13, \n",
    "    \"n_estimators\" : 483\n",
    "}\n",
    "# lgbm_tuned += LIGHTGBM_PARAMS\n",
    "\n",
    "xgb_tuned = {\n",
    "    \"learning_rate\" : 0.29615234675507385, \n",
    "    \"max_depth\" : 12, \n",
    "    \"n_estimators\" : 386\n",
    "}\n",
    "# xgb_tuned += XGBOOST_PARAMS\n",
    "\n",
    "tabnet_tuned = {\n",
    "    \"gamma\" : 1.0726430938336577, \n",
    "    \"lambda_sparse\" : 0.033960576396166633, \n",
    "    \"n_steps\" : 5,\n",
    "    \"n_a\" : 32,\n",
    "    \"momentum\" : 0.7,\n",
    "}\n",
    "tabnet_tuned[\"n_d\"] = tabnet_tuned[\"n_a\"]\n",
    "\n",
    "tabnet_paper = {\n",
    "    \"gamma\" : 2.0, \n",
    "    \"lambda_sparse\" : 0.01, \n",
    "    \"n_steps\" : 4,\n",
    "    \"n_a\" : 16,\n",
    "    \"momentum\" : 0.7,\n",
    "}\n",
    "tabnet_paper[\"n_d\"] = tabnet_paper[\"n_a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/synthetic/syn2/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/synthetic/syn2/train.csv\")\n",
    "valid = pd.read_csv(\"data/synthetic/syn2/val.csv\")\n",
    "test = pd.read_csv(\"data/synthetic/syn2/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.624345</td>\n",
       "      <td>-0.611756</td>\n",
       "      <td>-0.528172</td>\n",
       "      <td>-1.072969</td>\n",
       "      <td>0.865408</td>\n",
       "      <td>-2.301539</td>\n",
       "      <td>1.744812</td>\n",
       "      <td>-0.761207</td>\n",
       "      <td>0.319039</td>\n",
       "      <td>-0.249370</td>\n",
       "      <td>1.462108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.060141</td>\n",
       "      <td>-0.322417</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>1.133769</td>\n",
       "      <td>-1.099891</td>\n",
       "      <td>-0.172428</td>\n",
       "      <td>-0.877858</td>\n",
       "      <td>0.042214</td>\n",
       "      <td>0.582815</td>\n",
       "      <td>-1.100619</td>\n",
       "      <td>1.144724</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.901591</td>\n",
       "      <td>0.502494</td>\n",
       "      <td>0.900856</td>\n",
       "      <td>-0.683728</td>\n",
       "      <td>-0.122890</td>\n",
       "      <td>-0.935769</td>\n",
       "      <td>-0.267888</td>\n",
       "      <td>0.530355</td>\n",
       "      <td>-0.691661</td>\n",
       "      <td>-0.396754</td>\n",
       "      <td>-0.687173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.845206</td>\n",
       "      <td>-0.671246</td>\n",
       "      <td>-0.012665</td>\n",
       "      <td>-1.117310</td>\n",
       "      <td>0.234416</td>\n",
       "      <td>1.659802</td>\n",
       "      <td>0.742044</td>\n",
       "      <td>-0.191836</td>\n",
       "      <td>-0.887629</td>\n",
       "      <td>-0.747158</td>\n",
       "      <td>1.692455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050808</td>\n",
       "      <td>-0.636996</td>\n",
       "      <td>0.190915</td>\n",
       "      <td>2.100255</td>\n",
       "      <td>0.120159</td>\n",
       "      <td>0.617203</td>\n",
       "      <td>0.300170</td>\n",
       "      <td>-0.352250</td>\n",
       "      <td>-1.142518</td>\n",
       "      <td>-0.349343</td>\n",
       "      <td>-0.208894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0  1.624345 -0.611756 -0.528172 -1.072969  0.865408 -2.301539  1.744812   \n",
       "1 -2.060141 -0.322417 -0.384054  1.133769 -1.099891 -0.172428 -0.877858   \n",
       "2  0.901591  0.502494  0.900856 -0.683728 -0.122890 -0.935769 -0.267888   \n",
       "3 -0.845206 -0.671246 -0.012665 -1.117310  0.234416  1.659802  0.742044   \n",
       "4  0.050808 -0.636996  0.190915  2.100255  0.120159  0.617203  0.300170   \n",
       "\n",
       "         X7        X8        X9       X10  TARGET  \n",
       "0 -0.761207  0.319039 -0.249370  1.462108       1  \n",
       "1  0.042214  0.582815 -1.100619  1.144724       1  \n",
       "2  0.530355 -0.691661 -0.396754 -0.687173       0  \n",
       "3 -0.191836 -0.887629 -0.747158  1.692455       0  \n",
       "4 -0.352250 -1.142518 -0.349343 -0.208894       1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train.drop([\"TARGET\"], axis=1)\n",
    "train_y = train[\"TARGET\"]\n",
    "\n",
    "test_X = test.drop([\"TARGET\"], axis=1)\n",
    "test_y = test[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.23995731404370924, max_depth=13, metric='auc',\n",
       "               n_estimators=483, objective='binary', random_state=42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgbm = LGBMClassifier(**{**lgbm_tuned, **LIGHTGBM_PARAMS})\n",
    "model_lgbm.fit(train_X, train_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:39:42] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[23:39:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.29615234675507385, max_delta_step=0, max_depth=12,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=386, n_jobs=-1, num_parallel_tree=1, random_state=42,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, silent=True,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_xgb = XGBClassifier(**{**xgb_tuned, **XGBOOST_PARAMS})\n",
    "model_xgb.fit(train_X, train_y, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABNET_PARAMS[\"verbose\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.86991 |  0:00:01s\n",
      "epoch 1  | loss: 0.52465 |  0:00:02s\n",
      "epoch 2  | loss: 0.46098 |  0:00:03s\n",
      "epoch 3  | loss: 0.41241 |  0:00:04s\n",
      "epoch 4  | loss: 0.37964 |  0:00:05s\n",
      "epoch 5  | loss: 0.33534 |  0:00:06s\n",
      "epoch 6  | loss: 0.31966 |  0:00:08s\n",
      "epoch 7  | loss: 0.28648 |  0:00:09s\n",
      "epoch 8  | loss: 0.27581 |  0:00:10s\n",
      "epoch 9  | loss: 0.25976 |  0:00:11s\n",
      "epoch 10 | loss: 0.23066 |  0:00:12s\n",
      "epoch 11 | loss: 0.21562 |  0:00:13s\n",
      "epoch 12 | loss: 0.20839 |  0:00:15s\n",
      "epoch 13 | loss: 0.19879 |  0:00:16s\n",
      "epoch 14 | loss: 0.19995 |  0:00:17s\n",
      "epoch 15 | loss: 0.18933 |  0:00:18s\n",
      "epoch 16 | loss: 0.17786 |  0:00:19s\n",
      "epoch 17 | loss: 0.19188 |  0:00:20s\n",
      "epoch 18 | loss: 0.18363 |  0:00:22s\n",
      "epoch 19 | loss: 0.19132 |  0:00:23s\n",
      "epoch 20 | loss: 0.16377 |  0:00:24s\n",
      "epoch 21 | loss: 0.17128 |  0:00:25s\n",
      "epoch 22 | loss: 0.15748 |  0:00:26s\n",
      "epoch 23 | loss: 0.16595 |  0:00:27s\n",
      "epoch 24 | loss: 0.14962 |  0:00:28s\n",
      "epoch 25 | loss: 0.15749 |  0:00:30s\n",
      "epoch 26 | loss: 0.15578 |  0:00:31s\n",
      "epoch 27 | loss: 0.14538 |  0:00:32s\n",
      "epoch 28 | loss: 0.13941 |  0:00:33s\n",
      "epoch 29 | loss: 0.15458 |  0:00:34s\n",
      "epoch 30 | loss: 0.14662 |  0:00:35s\n",
      "epoch 31 | loss: 0.1475  |  0:00:37s\n",
      "epoch 32 | loss: 0.14887 |  0:00:38s\n",
      "epoch 33 | loss: 0.15139 |  0:00:39s\n",
      "epoch 34 | loss: 0.15308 |  0:00:40s\n",
      "epoch 35 | loss: 0.13922 |  0:00:41s\n",
      "epoch 36 | loss: 0.13937 |  0:00:42s\n",
      "epoch 37 | loss: 0.14403 |  0:00:43s\n",
      "epoch 38 | loss: 0.1528  |  0:00:45s\n",
      "epoch 39 | loss: 0.15821 |  0:00:46s\n",
      "epoch 40 | loss: 0.15033 |  0:00:47s\n",
      "epoch 41 | loss: 0.13897 |  0:00:48s\n",
      "epoch 42 | loss: 0.1512  |  0:00:49s\n",
      "epoch 43 | loss: 0.14313 |  0:00:50s\n",
      "epoch 44 | loss: 0.14922 |  0:00:52s\n",
      "epoch 45 | loss: 0.14356 |  0:00:53s\n",
      "epoch 46 | loss: 0.1332  |  0:00:54s\n",
      "epoch 47 | loss: 0.13052 |  0:00:55s\n",
      "epoch 48 | loss: 0.12749 |  0:00:56s\n",
      "epoch 49 | loss: 0.12957 |  0:00:57s\n",
      "epoch 50 | loss: 0.13011 |  0:00:58s\n",
      "epoch 51 | loss: 0.14282 |  0:01:00s\n",
      "epoch 52 | loss: 0.13301 |  0:01:01s\n",
      "epoch 53 | loss: 0.14705 |  0:01:02s\n",
      "epoch 54 | loss: 0.13532 |  0:01:03s\n",
      "epoch 55 | loss: 0.13988 |  0:01:04s\n",
      "epoch 56 | loss: 0.14342 |  0:01:06s\n",
      "epoch 57 | loss: 0.13492 |  0:01:07s\n",
      "epoch 58 | loss: 0.15715 |  0:01:08s\n",
      "epoch 59 | loss: 0.14306 |  0:01:09s\n"
     ]
    }
   ],
   "source": [
    "model_tabnet = TabNetClassifier(**{**tabnet_tuned, **TABNET_PARAMS})\n",
    "model_tabnet.fit(train_X.values, train_y.values, max_epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.87068 |  0:00:00s\n",
      "epoch 1  | loss: 0.6699  |  0:00:01s\n",
      "epoch 2  | loss: 0.60373 |  0:00:02s\n",
      "epoch 3  | loss: 0.56591 |  0:00:02s\n",
      "epoch 4  | loss: 0.55347 |  0:00:03s\n",
      "epoch 5  | loss: 0.5325  |  0:00:04s\n",
      "epoch 6  | loss: 0.52385 |  0:00:04s\n",
      "epoch 7  | loss: 0.51923 |  0:00:05s\n",
      "epoch 8  | loss: 0.50602 |  0:00:06s\n",
      "epoch 9  | loss: 0.49472 |  0:00:06s\n",
      "epoch 10 | loss: 0.50206 |  0:00:07s\n",
      "epoch 11 | loss: 0.49528 |  0:00:08s\n",
      "epoch 12 | loss: 0.49779 |  0:00:09s\n",
      "epoch 13 | loss: 0.48459 |  0:00:09s\n",
      "epoch 14 | loss: 0.47983 |  0:00:10s\n",
      "epoch 15 | loss: 0.46011 |  0:00:11s\n",
      "epoch 16 | loss: 0.45829 |  0:00:11s\n",
      "epoch 17 | loss: 0.43751 |  0:00:12s\n",
      "epoch 18 | loss: 0.44063 |  0:00:13s\n",
      "epoch 19 | loss: 0.43581 |  0:00:13s\n",
      "epoch 20 | loss: 0.41919 |  0:00:14s\n",
      "epoch 21 | loss: 0.41313 |  0:00:15s\n",
      "epoch 22 | loss: 0.40584 |  0:00:15s\n",
      "epoch 23 | loss: 0.39397 |  0:00:16s\n",
      "epoch 24 | loss: 0.38784 |  0:00:17s\n",
      "epoch 25 | loss: 0.37715 |  0:00:17s\n",
      "epoch 26 | loss: 0.37292 |  0:00:18s\n",
      "epoch 27 | loss: 0.36809 |  0:00:19s\n",
      "epoch 28 | loss: 0.36844 |  0:00:19s\n",
      "epoch 29 | loss: 0.36399 |  0:00:20s\n",
      "epoch 30 | loss: 0.36259 |  0:00:21s\n",
      "epoch 31 | loss: 0.36723 |  0:00:22s\n",
      "epoch 32 | loss: 0.36059 |  0:00:22s\n",
      "epoch 33 | loss: 0.34921 |  0:00:23s\n",
      "epoch 34 | loss: 0.35218 |  0:00:24s\n",
      "epoch 35 | loss: 0.34259 |  0:00:24s\n",
      "epoch 36 | loss: 0.33284 |  0:00:25s\n",
      "epoch 37 | loss: 0.32888 |  0:00:26s\n",
      "epoch 38 | loss: 0.31803 |  0:00:26s\n",
      "epoch 39 | loss: 0.32474 |  0:00:27s\n",
      "epoch 40 | loss: 0.32324 |  0:00:28s\n",
      "epoch 41 | loss: 0.31758 |  0:00:28s\n",
      "epoch 42 | loss: 0.31682 |  0:00:29s\n",
      "epoch 43 | loss: 0.30524 |  0:00:30s\n",
      "epoch 44 | loss: 0.29688 |  0:00:31s\n",
      "epoch 45 | loss: 0.29691 |  0:00:31s\n",
      "epoch 46 | loss: 0.2884  |  0:00:32s\n",
      "epoch 47 | loss: 0.28414 |  0:00:33s\n",
      "epoch 48 | loss: 0.30019 |  0:00:33s\n",
      "epoch 49 | loss: 0.29298 |  0:00:34s\n",
      "epoch 50 | loss: 0.29276 |  0:00:35s\n",
      "epoch 51 | loss: 0.29013 |  0:00:35s\n",
      "epoch 52 | loss: 0.28285 |  0:00:36s\n",
      "epoch 53 | loss: 0.27118 |  0:00:37s\n",
      "epoch 54 | loss: 0.27059 |  0:00:37s\n",
      "epoch 55 | loss: 0.27583 |  0:00:38s\n",
      "epoch 56 | loss: 0.27146 |  0:00:39s\n",
      "epoch 57 | loss: 0.27041 |  0:00:39s\n",
      "epoch 58 | loss: 0.2633  |  0:00:40s\n",
      "epoch 59 | loss: 0.26476 |  0:00:41s\n"
     ]
    }
   ],
   "source": [
    "model_tabnet_paper = TabNetClassifier(**{**tabnet_paper, **TABNET_PARAMS})\n",
    "model_tabnet_paper.fit(train_X.values, train_y.values, max_epochs=60, batch_size=3000, virtual_batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_tabnet.pickle', 'wb') as f:\n",
    "    pickle.dump(model_tabnet, f)\n",
    "with open('model_tabnet_paper.pickle', 'wb') as f:\n",
    "    pickle.dump(model_tabnet_paper, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_tabnet.pickle', 'rb') as f:\n",
    "    model_tabnet = pickle.load(f)\n",
    "with open('model_tabnet_paper.pickle', 'rb') as f:\n",
    "    model_tabnet_paper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM auc:  0.997261\n",
      "XGBoost auc:  0.99759\n",
      "TabNet auc:  0.994865\n",
      "TabNet Paper auc:  0.966575\n"
     ]
    }
   ],
   "source": [
    "print(\"LightGBM auc: \", round(roc_auc_score(test_y, model_lgbm.predict_proba(test_X)[:, 1]), 6))\n",
    "print(\"XGBoost auc: \", round(roc_auc_score(test_y, model_xgb.predict_proba(test_X)[:, 1]), 6))\n",
    "print(\"TabNet auc: \", round(roc_auc_score(test_y, model_tabnet.predict_proba(test_X.values)[:, 1]), 6))\n",
    "print(\"TabNet Paper auc: \", round(roc_auc_score(test_y, model_tabnet_paper.predict_proba(test_X.values)[:, 1]), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM acc:  0.971352\n",
      "XGBoost acc:  0.974017\n",
      "TabNet acc:  0.952698\n",
      "TabNet Paper acc:  0.907395\n"
     ]
    }
   ],
   "source": [
    "print(\"LightGBM acc: \", round(accuracy_score(test_y, model_lgbm.predict(test_X)), 6))\n",
    "print(\"XGBoost acc: \", round(accuracy_score(test_y, model_xgb.predict(test_X)), 6))\n",
    "print(\"TabNet acc: \", round(accuracy_score(test_y, model_tabnet.predict(test_X.values)), 6))\n",
    "print(\"TabNet Paper acc: \", round(accuracy_score(test_y, model_tabnet_paper.predict(test_X.values)), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM log_loss:  0.091779\n",
      "XGBoost log_loss:  0.06412\n",
      "TabNet log_loss:  0.105463\n",
      "TabNet Paper log_loss:  0.232512\n"
     ]
    }
   ],
   "source": [
    "print(\"LightGBM log_loss: \", round(log_loss(test_y, model_lgbm.predict_proba(test_X)), 6))\n",
    "print(\"XGBoost log_loss: \", round(log_loss(test_y, model_xgb.predict_proba(test_X)), 6))\n",
    "print(\"TabNet log_loss: \", round(log_loss(test_y, model_tabnet.predict_proba(test_X.values)), 6))\n",
    "print(\"TabNet Paper log_loss: \", round(log_loss(test_y, model_tabnet_paper.predict_proba(test_X.values)), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_lgbm = list(enumerate(model_lgbm.feature_importances_))\n",
    "importance_xgb = model_xgb.get_booster().get_score(importance_type='weight').items()\n",
    "importance_tabnet = list(enumerate(model_tabnet.feature_importances_))\n",
    "importance_tabnet_paper = list(enumerate(model_tabnet_paper.feature_importances_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_lgbm = sorted(importance_lgbm, key=lambda x: x[1], reverse=True)\n",
    "importance_xgb = sorted(importance_xgb, key=lambda x: x[1], reverse=True)\n",
    "importance_tabnet = sorted(importance_tabnet, key=lambda x: x[1], reverse=True)\n",
    "importance_tabnet_paper = sorted(importance_tabnet_paper, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x424.8 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shap_values = shap.TreeExplainer(model_xgb).shap_values(train_X)\n",
    "shap.summary_plot(shap_values, train_X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.DataFrame([train.columns[x[0]] for x in importance_lgbm], columns=[\"LightGBM\"])\n",
    "df_compare[\"XGBoost\"] = [x[0] for x in importance_xgb]\n",
    "df_compare[\"SHAP_XGBoost\"] = train_X.columns[np.argsort(np.abs(shap_values).mean(0))][::-1]\n",
    "df_compare[\"TabNet\"] = [train.columns[x[0]] for x in importance_tabnet]\n",
    "df_compare[\"TabNet_Paper\"] = [train.columns[x[0]] for x in importance_tabnet_paper]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <th>TabNet</th>\n",
       "      <th>TabNet_Paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X1</td>\n",
       "      <td>X1</td>\n",
       "      <td>X3</td>\n",
       "      <td>X3</td>\n",
       "      <td>X1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X3</td>\n",
       "      <td>X3</td>\n",
       "      <td>X0</td>\n",
       "      <td>X1</td>\n",
       "      <td>X3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X0</td>\n",
       "      <td>X2</td>\n",
       "      <td>X2</td>\n",
       "      <td>X0</td>\n",
       "      <td>X0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X2</td>\n",
       "      <td>X0</td>\n",
       "      <td>X1</td>\n",
       "      <td>X2</td>\n",
       "      <td>X2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X9</td>\n",
       "      <td>X10</td>\n",
       "      <td>X10</td>\n",
       "      <td>X10</td>\n",
       "      <td>X10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>X5</td>\n",
       "      <td>X5</td>\n",
       "      <td>X5</td>\n",
       "      <td>X5</td>\n",
       "      <td>X7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>X7</td>\n",
       "      <td>X9</td>\n",
       "      <td>X4</td>\n",
       "      <td>X6</td>\n",
       "      <td>X8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>X6</td>\n",
       "      <td>X7</td>\n",
       "      <td>X8</td>\n",
       "      <td>X7</td>\n",
       "      <td>X9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X10</td>\n",
       "      <td>X4</td>\n",
       "      <td>X6</td>\n",
       "      <td>X8</td>\n",
       "      <td>X4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>X4</td>\n",
       "      <td>X6</td>\n",
       "      <td>X9</td>\n",
       "      <td>X9</td>\n",
       "      <td>X6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>X8</td>\n",
       "      <td>X8</td>\n",
       "      <td>X7</td>\n",
       "      <td>X4</td>\n",
       "      <td>X5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LightGBM XGBoost SHAP_XGBoost TabNet TabNet_Paper\n",
       "0        X1      X1           X3     X3           X1\n",
       "1        X3      X3           X0     X1           X3\n",
       "2        X0      X2           X2     X0           X0\n",
       "3        X2      X0           X1     X2           X2\n",
       "4        X9     X10          X10    X10          X10\n",
       "5        X5      X5           X5     X5           X7\n",
       "6        X7      X9           X4     X6           X8\n",
       "7        X6      X7           X8     X7           X9\n",
       "8       X10      X4           X6     X8           X4\n",
       "9        X4      X6           X9     X9           X6\n",
       "10       X8      X8           X7     X4           X5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3 = []\n",
    "\n",
    "for i in range(len(df_compare.columns)):\n",
    "    for j in range(len(df_compare.columns)):\n",
    "        col1 = df_compare.columns[i]\n",
    "        col2 = df_compare.columns[j]\n",
    "        d = []\n",
    "        d.append(col1)\n",
    "        d.append(col2)\n",
    "        d.append(len(set(df_compare.loc[:2, col1]) & set(df_compare.loc[:2, col2])))\n",
    "        top_3.append(d)\n",
    "top_3_data = pd.DataFrame(top_3, columns=[\"Model1\", \"Model2\", \"Sim\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model2</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <th>TabNet</th>\n",
       "      <th>TabNet_Paper</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet_Paper</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model2        LightGBM  SHAP_XGBoost  TabNet  TabNet_Paper  XGBoost\n",
       "Model1                                                             \n",
       "LightGBM             3             2       3             3        2\n",
       "SHAP_XGBoost         2             3       2             2        2\n",
       "TabNet               3             2       3             3        2\n",
       "TabNet_Paper         3             2       3             3        2\n",
       "XGBoost              2             2       2             2        3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(top_3_data, values='Sim', index=['Model1'], columns=['Model2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 3 признака из каждой и обучить бустинги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5 = []\n",
    "\n",
    "for i in range(len(df_compare.columns)):\n",
    "    for j in range(len(df_compare.columns)):\n",
    "        col1 = df_compare.columns[i]\n",
    "        col2 = df_compare.columns[j]\n",
    "        d = []\n",
    "        d.append(col1)\n",
    "        d.append(col2)\n",
    "        d.append(len(set(df_compare.loc[:4, col1]) & set(df_compare.loc[:4, col2])))\n",
    "        top_5.append(d)\n",
    "top_5_data = pd.DataFrame(top_5, columns=[\"Model1\", \"Model2\", \"Sim\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model2</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <th>TabNet</th>\n",
       "      <th>TabNet_Paper</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet_Paper</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model2        LightGBM  SHAP_XGBoost  TabNet  TabNet_Paper  XGBoost\n",
       "Model1                                                             \n",
       "LightGBM             5             4       4             4        4\n",
       "SHAP_XGBoost         4             5       5             5        5\n",
       "TabNet               4             5       5             5        5\n",
       "TabNet_Paper         4             5       5             5        5\n",
       "XGBoost              4             5       5             5        5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(top_5_data, values='Sim', index=['Model1'], columns=['Model2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = []\n",
    "\n",
    "for i in range(len(df_compare.columns)):\n",
    "    for j in range(len(df_compare.columns)):\n",
    "        col1 = df_compare.columns[i]\n",
    "        col2 = df_compare.columns[j]\n",
    "        d = []\n",
    "        d.append(col1)\n",
    "        d.append(col2)\n",
    "        d.append(len(set(df_compare.loc[:9, col1]) & set(df_compare.loc[:9, col2])))\n",
    "        top_10.append(d)\n",
    "top_10_data = pd.DataFrame(top_10, columns=[\"Model1\", \"Model2\", \"Sim\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Model2</th>\n",
       "      <th>LightGBM</th>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <th>TabNet</th>\n",
       "      <th>TabNet_Paper</th>\n",
       "      <th>XGBoost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LightGBM</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHAP_XGBoost</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TabNet_Paper</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model2        LightGBM  SHAP_XGBoost  TabNet  TabNet_Paper  XGBoost\n",
       "Model1                                                             \n",
       "LightGBM            10             9       9             9       10\n",
       "SHAP_XGBoost         9            10       9             9        9\n",
       "TabNet               9             9      10             9        9\n",
       "TabNet_Paper         9             9       9            10        9\n",
       "XGBoost             10             9       9             9       10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(top_10_data, values='Sim', index=['Model1'], columns=['Model2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_feats = {}\n",
    "top3_feats[\"xgboost\"] = [\"fnlwgt\", \"age\", \"hours_per_week\"]\n",
    "top3_feats[\"shap\"] = [\"age\", \"martial_status\", \"capital_gain\"]\n",
    "top3_feats[\"tabnet\"] = [\"relationship\", \"age\", \"occupation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_feats = {}\n",
    "top10_feats[\"xgboost\"] = [\"fnlwgt\", \"age\", \"hours_per_week\", \"occupation\", \"education_num\", \"capital_gain\", \"education\", \"workclass\", \"capital_loss\", \"native_country\"]\n",
    "top10_feats[\"shap\"] = [\"age\", \"martial_status\", \"capital_gain\", \"relationship\", \"education_num\", \"occupation\", \"fnlwgt\", \"hours_per_week\", \"sex\", \"capital_loss\"]\n",
    "top10_feats[\"tabnet\"] = [\"relationship\", \"age\", \"occupation\", \"hours_per_week\", \"education_num\", \"capital_gain\", \"education\", \"sex\", \"workclass\", \"capital_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['fnlwgt', 'age', 'hours_per_week'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-817f43cedb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtop3_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop3_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_X_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest_X_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcat_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X_slice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['fnlwgt', 'age', 'hours_per_week'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "top3_results = []\n",
    "for model, feats in top3_feats.items():\n",
    "    train_X_slice = train_X[feats]\n",
    "    test_X_slice = test_X[feats]\n",
    "    cat_feats = set(train_X_slice.columns) & set([train.columns[x] for x in categorical_idx])\n",
    "    cat_idx = sorted([list(train_X_slice.columns).index(x) for x in cat_feats])\n",
    "    cat_dims = [len(set(list(train_X_slice.iloc[:, x].unique()) + list(test_X_slice.iloc[:, x].unique()))) for x in cat_idx]\n",
    "    \n",
    "    model_xgb = XGBClassifier(**{**xgb_tuned, **XGBOOST_PARAMS})\n",
    "    model_xgb.fit(train_X_slice, train_y, verbose=1)\n",
    "    top3_results.append([model, \"XGBoost\", \"acc\", round(accuracy_score(test_y, model_xgb.predict(test_X_slice)), 6)])\n",
    "    top3_results.append([model, \"XGBoost\", \"auc\", round(roc_auc_score(test_y, model_xgb.predict_proba(test_X_slice)[:, 1]), 6)])\n",
    "    result = []\n",
    "    model_tabnet = TabNetClassifier(**{**tabnet_tuned, **TABNET_PARAMS}, cat_idxs=cat_idx, cat_dims=cat_dims)\n",
    "    model_tabnet.fit(train_X_slice.values, train_y.values, max_epochs=60)\n",
    "    top3_results.append([model, \"TabNet\", \"acc\", round(accuracy_score(test_y, model_tabnet.predict(test_X_slice.values)), 6)])\n",
    "    top3_results.append([model, \"TabNet\", \"auc\", round(roc_auc_score(test_y, model_tabnet.predict_proba(test_X_slice.values)[:, 1]), 6)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_scores = pd.DataFrame(top3_results, columns=[\"Features\", \"Model\", \"Score\", \"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">TabNet</th>\n",
       "      <th colspan=\"2\" halign=\"left\">XGBoost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shap</th>\n",
       "      <td>0.805527</td>\n",
       "      <td>0.851631</td>\n",
       "      <td>0.819652</td>\n",
       "      <td>0.864625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabnet</th>\n",
       "      <td>0.824770</td>\n",
       "      <td>0.864272</td>\n",
       "      <td>0.819652</td>\n",
       "      <td>0.857539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.769089</td>\n",
       "      <td>0.751890</td>\n",
       "      <td>0.760491</td>\n",
       "      <td>0.727796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model       TabNet             XGBoost          \n",
       "Score          acc       auc       acc       auc\n",
       "Features                                        \n",
       "shap      0.805527  0.851631  0.819652  0.864625\n",
       "tabnet    0.824770  0.864272  0.819652  0.857539\n",
       "xgboost   0.769089  0.751890  0.760491  0.727796"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(top_3_scores, values='Value', index=['Features'], columns=['Model', \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:19:31] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:19:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.79592 |  0:00:12s\n",
      "epoch 1  | loss: 0.49582 |  0:00:25s\n",
      "epoch 2  | loss: 0.44158 |  0:00:40s\n",
      "epoch 3  | loss: 0.40275 |  0:00:53s\n",
      "epoch 4  | loss: 0.40626 |  0:01:05s\n",
      "epoch 5  | loss: 0.4009  |  0:01:18s\n",
      "epoch 6  | loss: 0.39774 |  0:01:30s\n",
      "epoch 7  | loss: 0.39294 |  0:01:43s\n",
      "epoch 8  | loss: 0.3906  |  0:01:57s\n",
      "epoch 9  | loss: 0.39566 |  0:02:17s\n",
      "epoch 10 | loss: 0.3941  |  0:02:54s\n",
      "epoch 11 | loss: 0.39292 |  0:03:31s\n",
      "epoch 12 | loss: 0.39145 |  0:04:11s\n",
      "epoch 13 | loss: 0.3886  |  0:04:34s\n",
      "epoch 14 | loss: 0.39061 |  0:05:00s\n",
      "epoch 15 | loss: 0.39403 |  0:05:23s\n",
      "epoch 16 | loss: 0.38763 |  0:05:47s\n",
      "epoch 17 | loss: 0.38817 |  0:06:08s\n",
      "epoch 18 | loss: 0.39708 |  0:06:32s\n",
      "epoch 19 | loss: 0.39374 |  0:06:56s\n",
      "epoch 20 | loss: 0.3895  |  0:07:21s\n",
      "epoch 21 | loss: 0.38912 |  0:07:45s\n",
      "epoch 22 | loss: 0.38642 |  0:08:08s\n",
      "epoch 23 | loss: 0.3847  |  0:08:33s\n",
      "epoch 24 | loss: 0.38419 |  0:08:57s\n",
      "epoch 25 | loss: 0.38397 |  0:09:20s\n",
      "epoch 26 | loss: 0.38094 |  0:09:44s\n",
      "epoch 27 | loss: 0.38124 |  0:10:07s\n",
      "epoch 28 | loss: 0.38337 |  0:10:31s\n",
      "epoch 29 | loss: 0.38498 |  0:10:55s\n",
      "epoch 30 | loss: 0.38105 |  0:11:19s\n",
      "epoch 31 | loss: 0.38232 |  0:11:40s\n",
      "epoch 32 | loss: 0.3815  |  0:12:04s\n",
      "epoch 33 | loss: 0.38328 |  0:12:26s\n",
      "epoch 34 | loss: 0.37821 |  0:12:49s\n",
      "epoch 35 | loss: 0.3803  |  0:13:13s\n",
      "epoch 36 | loss: 0.37915 |  0:13:36s\n",
      "epoch 37 | loss: 0.37989 |  0:13:59s\n",
      "epoch 38 | loss: 0.39247 |  0:14:23s\n",
      "epoch 39 | loss: 0.3804  |  0:14:46s\n",
      "epoch 40 | loss: 0.37794 |  0:15:11s\n",
      "epoch 41 | loss: 0.37671 |  0:15:32s\n",
      "epoch 42 | loss: 0.37527 |  0:15:54s\n",
      "epoch 43 | loss: 0.37477 |  0:16:15s\n",
      "epoch 44 | loss: 0.38458 |  0:16:38s\n",
      "epoch 45 | loss: 0.38768 |  0:17:00s\n",
      "epoch 46 | loss: 0.37762 |  0:17:25s\n",
      "epoch 47 | loss: 0.37628 |  0:17:48s\n",
      "epoch 48 | loss: 0.37779 |  0:18:10s\n",
      "epoch 49 | loss: 0.37808 |  0:18:33s\n",
      "epoch 50 | loss: 0.37675 |  0:18:56s\n",
      "epoch 51 | loss: 0.37481 |  0:19:20s\n",
      "epoch 52 | loss: 0.37458 |  0:19:43s\n",
      "epoch 53 | loss: 0.37851 |  0:20:06s\n",
      "epoch 54 | loss: 0.37758 |  0:20:30s\n",
      "epoch 55 | loss: 0.38001 |  0:20:51s\n",
      "epoch 56 | loss: 0.38308 |  0:21:15s\n",
      "epoch 57 | loss: 0.38137 |  0:21:39s\n",
      "epoch 58 | loss: 0.37925 |  0:22:01s\n",
      "epoch 59 | loss: 0.38585 |  0:22:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:42:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:42:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.90432 |  0:00:23s\n",
      "epoch 1  | loss: 0.39276 |  0:00:49s\n",
      "epoch 2  | loss: 0.34943 |  0:01:11s\n",
      "epoch 3  | loss: 0.34461 |  0:01:34s\n",
      "epoch 4  | loss: 0.34024 |  0:01:57s\n",
      "epoch 5  | loss: 0.34186 |  0:02:20s\n",
      "epoch 6  | loss: 0.34383 |  0:02:42s\n",
      "epoch 7  | loss: 0.33729 |  0:03:08s\n",
      "epoch 8  | loss: 0.33883 |  0:03:30s\n",
      "epoch 9  | loss: 0.33975 |  0:03:54s\n",
      "epoch 10 | loss: 0.34485 |  0:04:17s\n",
      "epoch 11 | loss: 0.33227 |  0:04:40s\n",
      "epoch 12 | loss: 0.33144 |  0:05:06s\n",
      "epoch 13 | loss: 0.3314  |  0:05:29s\n",
      "epoch 14 | loss: 0.33928 |  0:05:50s\n",
      "epoch 15 | loss: 0.32989 |  0:06:12s\n",
      "epoch 16 | loss: 0.33309 |  0:06:33s\n",
      "epoch 17 | loss: 0.33441 |  0:06:56s\n",
      "epoch 18 | loss: 0.33631 |  0:07:21s\n",
      "epoch 19 | loss: 0.32965 |  0:07:44s\n",
      "epoch 20 | loss: 0.33031 |  0:08:08s\n",
      "epoch 21 | loss: 0.32679 |  0:08:31s\n",
      "epoch 22 | loss: 0.32387 |  0:08:53s\n",
      "epoch 23 | loss: 0.32129 |  0:09:18s\n",
      "epoch 24 | loss: 0.3256  |  0:09:40s\n",
      "epoch 25 | loss: 0.33124 |  0:10:03s\n",
      "epoch 26 | loss: 0.32599 |  0:10:26s\n",
      "epoch 27 | loss: 0.32683 |  0:10:49s\n",
      "epoch 28 | loss: 0.32928 |  0:11:12s\n",
      "epoch 29 | loss: 0.32668 |  0:11:35s\n",
      "epoch 30 | loss: 0.32345 |  0:11:58s\n",
      "epoch 31 | loss: 0.32416 |  0:12:21s\n",
      "epoch 32 | loss: 0.32507 |  0:12:43s\n",
      "epoch 33 | loss: 0.32404 |  0:13:05s\n",
      "epoch 34 | loss: 0.32054 |  0:13:29s\n",
      "epoch 35 | loss: 0.32212 |  0:13:53s\n",
      "epoch 36 | loss: 0.32217 |  0:14:14s\n",
      "epoch 37 | loss: 0.31913 |  0:14:37s\n",
      "epoch 38 | loss: 0.32162 |  0:15:00s\n",
      "epoch 39 | loss: 0.32277 |  0:15:23s\n",
      "epoch 40 | loss: 0.31707 |  0:15:46s\n",
      "epoch 41 | loss: 0.31975 |  0:16:09s\n",
      "epoch 42 | loss: 0.3171  |  0:16:31s\n",
      "epoch 43 | loss: 0.32048 |  0:16:55s\n",
      "epoch 44 | loss: 0.32208 |  0:17:17s\n",
      "epoch 45 | loss: 0.31896 |  0:17:41s\n",
      "epoch 46 | loss: 0.31771 |  0:18:03s\n",
      "epoch 47 | loss: 0.32248 |  0:18:25s\n",
      "epoch 48 | loss: 0.31958 |  0:18:48s\n",
      "epoch 49 | loss: 0.31565 |  0:19:12s\n",
      "epoch 50 | loss: 0.32475 |  0:19:35s\n",
      "epoch 51 | loss: 0.32692 |  0:20:00s\n",
      "epoch 52 | loss: 0.32314 |  0:20:21s\n",
      "epoch 53 | loss: 0.31824 |  0:20:44s\n",
      "epoch 54 | loss: 0.3168  |  0:21:06s\n",
      "epoch 55 | loss: 0.3186  |  0:21:29s\n",
      "epoch 56 | loss: 0.3175  |  0:21:51s\n",
      "epoch 57 | loss: 0.32138 |  0:22:14s\n",
      "epoch 58 | loss: 0.31531 |  0:22:37s\n",
      "epoch 59 | loss: 0.32162 |  0:22:59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:05:22] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:05:22] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 0.88531 |  0:00:22s\n",
      "epoch 1  | loss: 0.45036 |  0:00:46s\n",
      "epoch 2  | loss: 0.38011 |  0:01:11s\n",
      "epoch 3  | loss: 0.37047 |  0:01:34s\n",
      "epoch 4  | loss: 0.36544 |  0:01:56s\n",
      "epoch 5  | loss: 0.35333 |  0:02:18s\n",
      "epoch 6  | loss: 0.34735 |  0:02:40s\n",
      "epoch 7  | loss: 0.34333 |  0:03:04s\n",
      "epoch 8  | loss: 0.33326 |  0:03:26s\n",
      "epoch 9  | loss: 0.33396 |  0:03:48s\n",
      "epoch 10 | loss: 0.3291  |  0:04:10s\n",
      "epoch 11 | loss: 0.33335 |  0:04:33s\n",
      "epoch 12 | loss: 0.33609 |  0:04:56s\n",
      "epoch 13 | loss: 0.3272  |  0:05:22s\n",
      "epoch 14 | loss: 0.32744 |  0:05:46s\n",
      "epoch 15 | loss: 0.32778 |  0:06:08s\n",
      "epoch 16 | loss: 0.33217 |  0:06:31s\n",
      "epoch 17 | loss: 0.33119 |  0:06:55s\n",
      "epoch 18 | loss: 0.32972 |  0:07:19s\n",
      "epoch 19 | loss: 0.32382 |  0:07:44s\n",
      "epoch 20 | loss: 0.32044 |  0:08:07s\n",
      "epoch 21 | loss: 0.32228 |  0:08:31s\n",
      "epoch 22 | loss: 0.32318 |  0:08:53s\n",
      "epoch 23 | loss: 0.31931 |  0:09:17s\n",
      "epoch 24 | loss: 0.31832 |  0:09:43s\n",
      "epoch 25 | loss: 0.32146 |  0:10:08s\n",
      "epoch 26 | loss: 0.31939 |  0:10:31s\n",
      "epoch 27 | loss: 0.32324 |  0:10:54s\n",
      "epoch 28 | loss: 0.32076 |  0:11:16s\n",
      "epoch 29 | loss: 0.31843 |  0:11:41s\n",
      "epoch 30 | loss: 0.32007 |  0:12:03s\n",
      "epoch 31 | loss: 0.31646 |  0:12:26s\n",
      "epoch 32 | loss: 0.31788 |  0:12:49s\n",
      "epoch 33 | loss: 0.32226 |  0:13:11s\n",
      "epoch 34 | loss: 0.32327 |  0:13:36s\n",
      "epoch 35 | loss: 0.31879 |  0:14:00s\n",
      "epoch 36 | loss: 0.32053 |  0:14:22s\n",
      "epoch 37 | loss: 0.31749 |  0:14:44s\n",
      "epoch 38 | loss: 0.31663 |  0:15:06s\n",
      "epoch 39 | loss: 0.31701 |  0:15:29s\n",
      "epoch 40 | loss: 0.31846 |  0:15:54s\n",
      "epoch 41 | loss: 0.31558 |  0:16:17s\n",
      "epoch 42 | loss: 0.31806 |  0:16:39s\n",
      "epoch 43 | loss: 0.3132  |  0:17:01s\n",
      "epoch 44 | loss: 0.31541 |  0:17:24s\n",
      "epoch 45 | loss: 0.31546 |  0:17:46s\n",
      "epoch 46 | loss: 0.31311 |  0:18:12s\n",
      "epoch 47 | loss: 0.31327 |  0:18:35s\n",
      "epoch 48 | loss: 0.3125  |  0:18:59s\n",
      "epoch 49 | loss: 0.31129 |  0:19:23s\n",
      "epoch 50 | loss: 0.31289 |  0:19:46s\n",
      "epoch 51 | loss: 0.31092 |  0:20:12s\n",
      "epoch 52 | loss: 0.31229 |  0:20:36s\n",
      "epoch 53 | loss: 0.31126 |  0:20:58s\n",
      "epoch 54 | loss: 0.309   |  0:21:20s\n",
      "epoch 55 | loss: 0.31072 |  0:21:42s\n",
      "epoch 56 | loss: 0.31349 |  0:22:05s\n",
      "epoch 57 | loss: 0.3108  |  0:22:29s\n",
      "epoch 58 | loss: 0.31376 |  0:22:51s\n",
      "epoch 59 | loss: 0.32204 |  0:23:14s\n"
     ]
    }
   ],
   "source": [
    "top10_results = []\n",
    "for model, feats in top10_feats.items():\n",
    "    train_X_slice = train_X[feats]\n",
    "    test_X_slice = test_X[feats]\n",
    "    cat_feats = set(train_X_slice.columns) & set([column_names[x] for x in categorical_idx])\n",
    "    cat_idx = sorted([list(train_X_slice.columns).index(x) for x in cat_feats])\n",
    "    cat_dims = [len(set(list(train_X_slice.iloc[:, x].unique()) + list(test_X_slice.iloc[:, x].unique()))) for x in cat_idx]\n",
    "    \n",
    "    model_xgb = XGBClassifier(**{**xgb_tuned, **XGBOOST_PARAMS})\n",
    "    model_xgb.fit(train_X_slice, train_y, verbose=1)\n",
    "    top10_results.append([model, \"XGBoost\", \"acc\", round(accuracy_score(test_y, model_xgb.predict(test_X_slice)), 6)])\n",
    "    top10_results.append([model, \"XGBoost\", \"auc\", round(roc_auc_score(test_y, model_xgb.predict_proba(test_X_slice)[:, 1]), 6)])\n",
    "    result = []\n",
    "    model_tabnet = TabNetClassifier(**{**tabnet_tuned, **TABNET_PARAMS}, cat_idxs=cat_idx, cat_dims=cat_dims)\n",
    "    model_tabnet.fit(train_X_slice.values, train_y.values, max_epochs=60)\n",
    "    top10_results.append([model, \"TabNet\", \"acc\", round(accuracy_score(test_y, model_tabnet.predict(test_X_slice.values)), 6)])\n",
    "    top10_results.append([model, \"TabNet\", \"auc\", round(roc_auc_score(test_y, model_tabnet.predict_proba(test_X_slice.values)[:, 1]), 6)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_scores = pd.DataFrame(top10_results, columns=[\"Features\", \"Model\", \"Score\", \"Value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th colspan=\"2\" halign=\"left\">TabNet</th>\n",
       "      <th colspan=\"2\" halign=\"left\">XGBoost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Features</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shap</th>\n",
       "      <td>0.856704</td>\n",
       "      <td>0.909560</td>\n",
       "      <td>0.857318</td>\n",
       "      <td>0.913265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabnet</th>\n",
       "      <td>0.859570</td>\n",
       "      <td>0.911028</td>\n",
       "      <td>0.869396</td>\n",
       "      <td>0.916778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.826612</td>\n",
       "      <td>0.851441</td>\n",
       "      <td>0.835619</td>\n",
       "      <td>0.867916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Model       TabNet             XGBoost          \n",
       "Score          acc       auc       acc       auc\n",
       "Features                                        \n",
       "shap      0.856704  0.909560  0.857318  0.913265\n",
       "tabnet    0.859570  0.911028  0.869396  0.916778\n",
       "xgboost   0.826612  0.851441  0.835619  0.867916"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(top_10_scores, values='Value', index=['Features'], columns=['Model', \"Score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
