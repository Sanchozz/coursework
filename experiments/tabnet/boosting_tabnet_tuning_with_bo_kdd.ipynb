{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "from bo_parameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificators = [\n",
    "    (\"TABNETClassifier\", TabNetClassifier, SEARCH_SPACE_TABNET, TABNET_PARAMS),\n",
    "    (\"XGBClassifier\", XGBClassifier, SEARCH_SPACE_XGB, XGBOOST_PARAMS),\n",
    "    (\"LGBMClassifier\", LGBMClassifier, SEARCH_SPACE_LGBM, LIGHTGBM_PARAMS),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BayesianOptimization(clf, params, search_range, model_name):\n",
    "    def func_gb(values):\n",
    "        for i, param in enumerate(search_range):\n",
    "            params[param.name] = values[i]\n",
    "            if param.name == \"n_a\":\n",
    "                params[\"n_d\"] = values[i]\n",
    "        print('\\nTesting next set of paramaters...', params)\n",
    "\n",
    "        model = clf(**params)\n",
    "        model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], eval_metric=[\"auc\"], early_stopping_rounds=10, verbose=0)\n",
    "        neg_auc = round(-roc_auc_score(valid_y, model.predict_proba(valid_x)[:, 1]), 6)\n",
    "\n",
    "        print('AUC: ', -neg_auc, ' of boosting iteration ')\n",
    "        return neg_auc\n",
    "    \n",
    "    def func_tabnet(values):\n",
    "        for i, param in enumerate(search_range):\n",
    "            params[param.name] = values[i]\n",
    "            if param.name == \"n_a\":\n",
    "                params[\"n_d\"] = values[i]\n",
    "        print('\\nTesting next set of paramaters...', params)\n",
    "\n",
    "        model = clf(**params)\n",
    "        model.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], eval_metric=[\"auc\"], max_epochs=40)\n",
    "        neg_auc = round(-roc_auc_score(valid_y, model.predict_proba(valid_x)[:, 1]), 6)\n",
    "\n",
    "        print('AUC: ', -neg_auc, ' of boosting iteration ')\n",
    "        return neg_auc\n",
    "    \n",
    "    return func_gb if model_name != \"TABNETClassifier\" else func_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"appetency\", \"churn\", \"upselling\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2983401976418565, 'lambda_sparse': 0.036534007757701534, 'n_steps': 4, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.20132 | val_0_auc: 0.52781 |  0:00:14s\n",
      "epoch 1  | loss: 0.14292 | val_0_auc: 0.55656 |  0:00:28s\n",
      "epoch 2  | loss: 0.11334 | val_0_auc: 0.50093 |  0:00:42s\n",
      "epoch 3  | loss: 0.10378 | val_0_auc: 0.51211 |  0:00:56s\n",
      "epoch 4  | loss: 0.09904 | val_0_auc: 0.48887 |  0:01:10s\n",
      "epoch 5  | loss: 0.09495 | val_0_auc: 0.55913 |  0:01:23s\n",
      "epoch 6  | loss: 0.09475 | val_0_auc: 0.52865 |  0:01:37s\n",
      "epoch 7  | loss: 0.09408 | val_0_auc: 0.57013 |  0:01:50s\n",
      "epoch 8  | loss: 0.09513 | val_0_auc: 0.50221 |  0:02:04s\n",
      "epoch 9  | loss: 0.09279 | val_0_auc: 0.55148 |  0:02:17s\n",
      "epoch 10 | loss: 0.09323 | val_0_auc: 0.52589 |  0:02:30s\n",
      "epoch 11 | loss: 0.0925  | val_0_auc: 0.54843 |  0:02:44s\n",
      "epoch 12 | loss: 0.09296 | val_0_auc: 0.56788 |  0:02:57s\n",
      "epoch 13 | loss: 0.09238 | val_0_auc: 0.57482 |  0:03:11s\n",
      "epoch 14 | loss: 0.09219 | val_0_auc: 0.59453 |  0:03:24s\n",
      "epoch 15 | loss: 0.09104 | val_0_auc: 0.62916 |  0:03:38s\n",
      "epoch 16 | loss: 0.0912  | val_0_auc: 0.6516  |  0:03:51s\n",
      "epoch 17 | loss: 0.08991 | val_0_auc: 0.64015 |  0:04:04s\n",
      "epoch 18 | loss: 0.08989 | val_0_auc: 0.62201 |  0:04:17s\n",
      "epoch 19 | loss: 0.09002 | val_0_auc: 0.64539 |  0:04:30s\n",
      "epoch 20 | loss: 0.08913 | val_0_auc: 0.64899 |  0:04:43s\n",
      "epoch 21 | loss: 0.08884 | val_0_auc: 0.67634 |  0:04:56s\n",
      "epoch 22 | loss: 0.08807 | val_0_auc: 0.69387 |  0:05:10s\n",
      "epoch 23 | loss: 0.08796 | val_0_auc: 0.71132 |  0:05:23s\n",
      "epoch 24 | loss: 0.08707 | val_0_auc: 0.71085 |  0:05:36s\n",
      "epoch 25 | loss: 0.0866  | val_0_auc: 0.73034 |  0:05:50s\n",
      "epoch 26 | loss: 0.08593 | val_0_auc: 0.70988 |  0:06:03s\n",
      "epoch 27 | loss: 0.08546 | val_0_auc: 0.73318 |  0:06:16s\n",
      "epoch 28 | loss: 0.0855  | val_0_auc: 0.73467 |  0:06:30s\n",
      "epoch 29 | loss: 0.08524 | val_0_auc: 0.75548 |  0:06:43s\n",
      "epoch 30 | loss: 0.08403 | val_0_auc: 0.74488 |  0:06:57s\n",
      "epoch 31 | loss: 0.08458 | val_0_auc: 0.73828 |  0:07:10s\n",
      "epoch 32 | loss: 0.08455 | val_0_auc: 0.75001 |  0:07:24s\n",
      "epoch 33 | loss: 0.08414 | val_0_auc: 0.7535  |  0:07:37s\n",
      "epoch 34 | loss: 0.08392 | val_0_auc: 0.74384 |  0:07:51s\n",
      "epoch 35 | loss: 0.08421 | val_0_auc: 0.76269 |  0:08:04s\n",
      "epoch 36 | loss: 0.08358 | val_0_auc: 0.78084 |  0:08:18s\n",
      "epoch 37 | loss: 0.08454 | val_0_auc: 0.78198 |  0:08:31s\n",
      "epoch 38 | loss: 0.08457 | val_0_auc: 0.77924 |  0:08:45s\n",
      "epoch 39 | loss: 0.08564 | val_0_auc: 0.70533 |  0:08:59s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_val_0_auc = 0.78198\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.781977  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 547.4040\n",
      "Function value obtained: -0.7820\n",
      "Current minimum: -0.7820\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6380054718463248, 'lambda_sparse': 0.0985556331757418, 'n_steps': 5, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.33425 | val_0_auc: 0.51458 |  0:00:17s\n",
      "epoch 1  | loss: 0.18915 | val_0_auc: 0.50125 |  0:00:34s\n",
      "epoch 2  | loss: 0.13137 | val_0_auc: 0.47364 |  0:00:51s\n",
      "epoch 3  | loss: 0.11459 | val_0_auc: 0.5137  |  0:01:09s\n",
      "epoch 4  | loss: 0.10123 | val_0_auc: 0.5388  |  0:01:25s\n",
      "epoch 5  | loss: 0.09522 | val_0_auc: 0.55317 |  0:01:42s\n",
      "epoch 6  | loss: 0.09434 | val_0_auc: 0.58331 |  0:01:59s\n",
      "epoch 7  | loss: 0.09398 | val_0_auc: 0.52048 |  0:02:16s\n",
      "epoch 8  | loss: 0.09293 | val_0_auc: 0.5196  |  0:02:32s\n",
      "epoch 9  | loss: 0.0934  | val_0_auc: 0.5772  |  0:02:49s\n",
      "epoch 10 | loss: 0.09298 | val_0_auc: 0.57395 |  0:03:06s\n",
      "epoch 11 | loss: 0.09327 | val_0_auc: 0.54987 |  0:03:22s\n",
      "epoch 12 | loss: 0.09274 | val_0_auc: 0.58506 |  0:03:39s\n",
      "epoch 13 | loss: 0.09194 | val_0_auc: 0.57726 |  0:03:56s\n",
      "epoch 14 | loss: 0.09209 | val_0_auc: 0.55106 |  0:04:12s\n",
      "epoch 15 | loss: 0.09178 | val_0_auc: 0.58253 |  0:04:29s\n",
      "epoch 16 | loss: 0.0921  | val_0_auc: 0.55514 |  0:04:45s\n",
      "epoch 17 | loss: 0.09241 | val_0_auc: 0.56959 |  0:05:02s\n",
      "epoch 18 | loss: 0.09216 | val_0_auc: 0.55157 |  0:05:18s\n",
      "epoch 19 | loss: 0.09254 | val_0_auc: 0.57503 |  0:05:35s\n",
      "epoch 20 | loss: 0.09151 | val_0_auc: 0.58497 |  0:05:51s\n",
      "epoch 21 | loss: 0.0917  | val_0_auc: 0.56805 |  0:06:08s\n",
      "epoch 22 | loss: 0.0925  | val_0_auc: 0.57901 |  0:06:24s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.58506\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.58506  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 394.5840\n",
      "Function value obtained: -0.5851\n",
      "Current minimum: -0.7820\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.7748962817319678, 'lambda_sparse': 0.03940948213944514, 'n_steps': 3, 'n_a': 64, 'n_d': 64, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.18995 | val_0_auc: 0.52019 |  0:00:13s\n",
      "epoch 1  | loss: 0.13223 | val_0_auc: 0.53942 |  0:00:26s\n",
      "epoch 2  | loss: 0.10677 | val_0_auc: 0.50869 |  0:00:39s\n",
      "epoch 3  | loss: 0.09643 | val_0_auc: 0.50051 |  0:00:51s\n",
      "epoch 4  | loss: 0.09607 | val_0_auc: 0.50347 |  0:01:03s\n",
      "epoch 5  | loss: 0.09343 | val_0_auc: 0.50234 |  0:01:15s\n",
      "epoch 6  | loss: 0.09423 | val_0_auc: 0.49861 |  0:01:28s\n",
      "epoch 7  | loss: 0.09366 | val_0_auc: 0.49416 |  0:01:40s\n",
      "epoch 8  | loss: 0.09303 | val_0_auc: 0.49228 |  0:01:52s\n",
      "epoch 9  | loss: 0.092   | val_0_auc: 0.51055 |  0:02:04s\n",
      "epoch 10 | loss: 0.093   | val_0_auc: 0.50098 |  0:02:16s\n",
      "epoch 11 | loss: 0.09201 | val_0_auc: 0.54546 |  0:02:28s\n",
      "epoch 12 | loss: 0.09137 | val_0_auc: 0.50846 |  0:02:40s\n",
      "epoch 13 | loss: 0.09186 | val_0_auc: 0.49554 |  0:02:52s\n",
      "epoch 14 | loss: 0.09238 | val_0_auc: 0.52023 |  0:03:04s\n",
      "epoch 15 | loss: 0.09351 | val_0_auc: 0.52257 |  0:03:16s\n",
      "epoch 16 | loss: 0.0914  | val_0_auc: 0.52262 |  0:03:29s\n",
      "epoch 17 | loss: 0.09198 | val_0_auc: 0.52971 |  0:03:41s\n",
      "epoch 18 | loss: 0.09161 | val_0_auc: 0.51724 |  0:03:53s\n",
      "epoch 19 | loss: 0.09169 | val_0_auc: 0.52054 |  0:04:04s\n",
      "epoch 20 | loss: 0.09175 | val_0_auc: 0.52273 |  0:04:16s\n",
      "epoch 21 | loss: 0.09121 | val_0_auc: 0.50349 |  0:04:28s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.54546\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.54546  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 275.0577\n",
      "Function value obtained: -0.5455\n",
      "Current minimum: -0.7820\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.166406002658061, 'lambda_sparse': 0.02551617311266607, 'n_steps': 6, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.208   | val_0_auc: 0.42451 |  0:00:18s\n",
      "epoch 1  | loss: 0.13093 | val_0_auc: 0.50771 |  0:00:36s\n",
      "epoch 2  | loss: 0.11808 | val_0_auc: 0.53069 |  0:00:54s\n",
      "epoch 3  | loss: 0.11006 | val_0_auc: 0.5853  |  0:01:11s\n",
      "epoch 4  | loss: 0.10429 | val_0_auc: 0.54161 |  0:01:29s\n",
      "epoch 5  | loss: 0.09801 | val_0_auc: 0.63707 |  0:01:47s\n",
      "epoch 6  | loss: 0.09933 | val_0_auc: 0.63594 |  0:02:05s\n",
      "epoch 7  | loss: 0.09899 | val_0_auc: 0.60107 |  0:02:23s\n",
      "epoch 8  | loss: 0.09681 | val_0_auc: 0.68553 |  0:02:41s\n",
      "epoch 9  | loss: 0.09361 | val_0_auc: 0.68394 |  0:02:58s\n",
      "epoch 10 | loss: 0.09226 | val_0_auc: 0.6848  |  0:03:16s\n",
      "epoch 11 | loss: 0.09201 | val_0_auc: 0.72256 |  0:03:33s\n",
      "epoch 12 | loss: 0.09093 | val_0_auc: 0.70241 |  0:03:51s\n",
      "epoch 13 | loss: 0.09153 | val_0_auc: 0.71282 |  0:04:09s\n",
      "epoch 14 | loss: 0.09036 | val_0_auc: 0.68193 |  0:04:26s\n",
      "epoch 15 | loss: 0.08991 | val_0_auc: 0.6948  |  0:04:43s\n",
      "epoch 16 | loss: 0.0881  | val_0_auc: 0.70608 |  0:05:00s\n",
      "epoch 17 | loss: 0.08795 | val_0_auc: 0.7237  |  0:05:17s\n",
      "epoch 18 | loss: 0.08675 | val_0_auc: 0.74681 |  0:05:34s\n",
      "epoch 19 | loss: 0.08575 | val_0_auc: 0.75355 |  0:05:51s\n",
      "epoch 20 | loss: 0.0852  | val_0_auc: 0.75773 |  0:06:08s\n",
      "epoch 21 | loss: 0.08422 | val_0_auc: 0.77521 |  0:06:25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 0.08414 | val_0_auc: 0.7782  |  0:06:42s\n",
      "epoch 23 | loss: 0.08307 | val_0_auc: 0.77648 |  0:06:59s\n",
      "epoch 24 | loss: 0.08384 | val_0_auc: 0.78087 |  0:07:15s\n",
      "epoch 25 | loss: 0.08378 | val_0_auc: 0.77309 |  0:07:32s\n",
      "epoch 26 | loss: 0.08314 | val_0_auc: 0.78238 |  0:07:49s\n",
      "epoch 27 | loss: 0.08301 | val_0_auc: 0.78961 |  0:08:06s\n",
      "epoch 28 | loss: 0.08273 | val_0_auc: 0.79659 |  0:08:22s\n",
      "epoch 29 | loss: 0.08277 | val_0_auc: 0.80488 |  0:08:38s\n",
      "epoch 30 | loss: 0.08219 | val_0_auc: 0.80675 |  0:08:55s\n",
      "epoch 31 | loss: 0.08188 | val_0_auc: 0.80714 |  0:09:11s\n",
      "epoch 32 | loss: 0.08193 | val_0_auc: 0.80461 |  0:09:28s\n",
      "epoch 33 | loss: 0.08213 | val_0_auc: 0.80276 |  0:09:44s\n",
      "epoch 34 | loss: 0.08371 | val_0_auc: 0.80629 |  0:10:00s\n",
      "epoch 35 | loss: 0.08294 | val_0_auc: 0.79493 |  0:10:17s\n",
      "epoch 36 | loss: 0.08215 | val_0_auc: 0.80531 |  0:10:34s\n",
      "epoch 37 | loss: 0.08167 | val_0_auc: 0.81043 |  0:10:51s\n",
      "epoch 38 | loss: 0.08165 | val_0_auc: 0.79308 |  0:11:07s\n",
      "epoch 39 | loss: 0.08278 | val_0_auc: 0.81285 |  0:11:24s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.81285\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.812855  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 695.3082\n",
      "Function value obtained: -0.8129\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.5726213660607455, 'lambda_sparse': 0.04952101001065431, 'n_steps': 4, 'n_a': 8, 'n_d': 8, 'momentum': 0.7}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.27357 | val_0_auc: 0.46526 |  0:00:12s\n",
      "epoch 1  | loss: 0.13571 | val_0_auc: 0.55701 |  0:00:24s\n",
      "epoch 2  | loss: 0.11852 | val_0_auc: 0.56267 |  0:00:37s\n",
      "epoch 3  | loss: 0.10726 | val_0_auc: 0.57419 |  0:00:49s\n",
      "epoch 4  | loss: 0.104   | val_0_auc: 0.60298 |  0:01:01s\n",
      "epoch 5  | loss: 0.10107 | val_0_auc: 0.58594 |  0:01:13s\n",
      "epoch 6  | loss: 0.09809 | val_0_auc: 0.60108 |  0:01:25s\n",
      "epoch 7  | loss: 0.09503 | val_0_auc: 0.61693 |  0:01:37s\n",
      "epoch 8  | loss: 0.0924  | val_0_auc: 0.64778 |  0:01:49s\n",
      "epoch 9  | loss: 0.09111 | val_0_auc: 0.64229 |  0:02:01s\n",
      "epoch 10 | loss: 0.09054 | val_0_auc: 0.65151 |  0:02:12s\n",
      "epoch 11 | loss: 0.09053 | val_0_auc: 0.63678 |  0:02:24s\n",
      "epoch 12 | loss: 0.0903  | val_0_auc: 0.65355 |  0:02:36s\n",
      "epoch 13 | loss: 0.09001 | val_0_auc: 0.68153 |  0:02:47s\n",
      "epoch 14 | loss: 0.08977 | val_0_auc: 0.67068 |  0:02:59s\n",
      "epoch 15 | loss: 0.08977 | val_0_auc: 0.69876 |  0:03:11s\n",
      "epoch 16 | loss: 0.08974 | val_0_auc: 0.69012 |  0:03:22s\n",
      "epoch 17 | loss: 0.08886 | val_0_auc: 0.68735 |  0:03:34s\n",
      "epoch 18 | loss: 0.08838 | val_0_auc: 0.67791 |  0:03:46s\n",
      "epoch 19 | loss: 0.08887 | val_0_auc: 0.69208 |  0:03:57s\n",
      "epoch 20 | loss: 0.08782 | val_0_auc: 0.69407 |  0:04:09s\n",
      "epoch 21 | loss: 0.08737 | val_0_auc: 0.69968 |  0:04:20s\n",
      "epoch 22 | loss: 0.08809 | val_0_auc: 0.71067 |  0:04:32s\n",
      "epoch 23 | loss: 0.0885  | val_0_auc: 0.7096  |  0:04:44s\n",
      "epoch 24 | loss: 0.09054 | val_0_auc: 0.68315 |  0:04:55s\n",
      "epoch 25 | loss: 0.09159 | val_0_auc: 0.67197 |  0:05:06s\n",
      "epoch 26 | loss: 0.0898  | val_0_auc: 0.66099 |  0:05:18s\n",
      "epoch 27 | loss: 0.08953 | val_0_auc: 0.66564 |  0:05:29s\n",
      "epoch 28 | loss: 0.08937 | val_0_auc: 0.6475  |  0:05:39s\n",
      "epoch 29 | loss: 0.08983 | val_0_auc: 0.6405  |  0:05:50s\n",
      "epoch 30 | loss: 0.08964 | val_0_auc: 0.66512 |  0:06:01s\n",
      "epoch 31 | loss: 0.08864 | val_0_auc: 0.67084 |  0:06:12s\n",
      "epoch 32 | loss: 0.08815 | val_0_auc: 0.6675  |  0:06:23s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.71067\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.710672  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 391.2269\n",
      "Function value obtained: -0.7107\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2562921330956742, 'lambda_sparse': 0.038171448681909664, 'n_steps': 9, 'n_a': 24, 'n_d': 24, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.19599 | val_0_auc: 0.5071  |  0:00:30s\n",
      "epoch 1  | loss: 0.14653 | val_0_auc: 0.54111 |  0:01:00s\n",
      "epoch 2  | loss: 0.13498 | val_0_auc: 0.65268 |  0:01:30s\n",
      "epoch 3  | loss: 0.12148 | val_0_auc: 0.7069  |  0:02:01s\n",
      "epoch 4  | loss: 0.10235 | val_0_auc: 0.63615 |  0:02:31s\n",
      "epoch 5  | loss: 0.10276 | val_0_auc: 0.63895 |  0:03:01s\n",
      "epoch 6  | loss: 0.10178 | val_0_auc: 0.6142  |  0:03:31s\n",
      "epoch 7  | loss: 0.09603 | val_0_auc: 0.64296 |  0:04:00s\n",
      "epoch 8  | loss: 0.09268 | val_0_auc: 0.65684 |  0:04:29s\n",
      "epoch 9  | loss: 0.09339 | val_0_auc: 0.67419 |  0:04:58s\n",
      "epoch 10 | loss: 0.09209 | val_0_auc: 0.67617 |  0:05:27s\n",
      "epoch 11 | loss: 0.09074 | val_0_auc: 0.64319 |  0:05:56s\n",
      "epoch 12 | loss: 0.09209 | val_0_auc: 0.61935 |  0:06:25s\n",
      "epoch 13 | loss: 0.09199 | val_0_auc: 0.68547 |  0:06:54s\n",
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_val_0_auc = 0.7069\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.706903  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 433.6111\n",
      "Function value obtained: -0.7069\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3576003338159097, 'lambda_sparse': 0.09820303838141516, 'n_steps': 8, 'n_a': 16, 'n_d': 16, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.32223 | val_0_auc: 0.48461 |  0:00:25s\n",
      "epoch 1  | loss: 0.20783 | val_0_auc: 0.48701 |  0:00:51s\n",
      "epoch 2  | loss: 0.18854 | val_0_auc: 0.52414 |  0:01:17s\n",
      "epoch 3  | loss: 0.16466 | val_0_auc: 0.51079 |  0:01:43s\n",
      "epoch 4  | loss: 0.14533 | val_0_auc: 0.52788 |  0:02:08s\n",
      "epoch 5  | loss: 0.13144 | val_0_auc: 0.5005  |  0:02:34s\n",
      "epoch 6  | loss: 0.11955 | val_0_auc: 0.568   |  0:02:59s\n",
      "epoch 7  | loss: 0.10853 | val_0_auc: 0.48122 |  0:03:24s\n",
      "epoch 8  | loss: 0.10603 | val_0_auc: 0.53689 |  0:03:48s\n",
      "epoch 9  | loss: 0.10589 | val_0_auc: 0.48783 |  0:04:12s\n",
      "epoch 10 | loss: 0.10341 | val_0_auc: 0.49723 |  0:04:36s\n",
      "epoch 11 | loss: 0.10121 | val_0_auc: 0.48148 |  0:05:01s\n",
      "epoch 12 | loss: 0.10195 | val_0_auc: 0.50845 |  0:05:26s\n",
      "epoch 13 | loss: 0.1008  | val_0_auc: 0.52713 |  0:05:51s\n",
      "epoch 14 | loss: 0.09851 | val_0_auc: 0.49202 |  0:06:16s\n",
      "epoch 15 | loss: 0.09739 | val_0_auc: 0.51347 |  0:06:40s\n",
      "epoch 16 | loss: 0.09535 | val_0_auc: 0.4813  |  0:07:04s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.568\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.567998  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 440.0433\n",
      "Function value obtained: -0.5680\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1547307748121, 'lambda_sparse': 0.052991509817684024, 'n_steps': 4, 'n_a': 8, 'n_d': 8, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.29924 | val_0_auc: 0.51172 |  0:00:12s\n",
      "epoch 1  | loss: 0.16395 | val_0_auc: 0.64127 |  0:00:25s\n",
      "epoch 2  | loss: 0.12617 | val_0_auc: 0.66312 |  0:00:37s\n",
      "epoch 3  | loss: 0.10836 | val_0_auc: 0.63007 |  0:00:49s\n",
      "epoch 4  | loss: 0.09368 | val_0_auc: 0.54629 |  0:01:01s\n",
      "epoch 5  | loss: 0.08457 | val_0_auc: 0.62918 |  0:01:12s\n",
      "epoch 6  | loss: 0.08241 | val_0_auc: 0.70523 |  0:01:23s\n",
      "epoch 7  | loss: 0.08183 | val_0_auc: 0.78341 |  0:01:34s\n",
      "epoch 8  | loss: 0.08044 | val_0_auc: 0.78342 |  0:01:45s\n",
      "epoch 9  | loss: 0.08038 | val_0_auc: 0.77684 |  0:01:56s\n",
      "epoch 10 | loss: 0.08111 | val_0_auc: 0.77937 |  0:02:06s\n",
      "epoch 11 | loss: 0.08057 | val_0_auc: 0.77299 |  0:02:17s\n",
      "epoch 12 | loss: 0.08029 | val_0_auc: 0.78859 |  0:02:28s\n",
      "epoch 13 | loss: 0.07984 | val_0_auc: 0.77834 |  0:02:38s\n",
      "epoch 14 | loss: 0.07988 | val_0_auc: 0.78409 |  0:02:49s\n",
      "epoch 15 | loss: 0.07959 | val_0_auc: 0.77863 |  0:03:00s\n",
      "epoch 16 | loss: 0.07998 | val_0_auc: 0.78109 |  0:03:11s\n",
      "epoch 17 | loss: 0.08057 | val_0_auc: 0.77771 |  0:03:21s\n",
      "epoch 18 | loss: 0.07946 | val_0_auc: 0.77702 |  0:03:32s\n",
      "epoch 19 | loss: 0.08066 | val_0_auc: 0.78558 |  0:03:43s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20 | loss: 0.07961 | val_0_auc: 0.79162 |  0:03:53s\n",
      "epoch 21 | loss: 0.07955 | val_0_auc: 0.79325 |  0:04:04s\n",
      "epoch 22 | loss: 0.07963 | val_0_auc: 0.7921  |  0:04:14s\n",
      "epoch 23 | loss: 0.07974 | val_0_auc: 0.79449 |  0:04:25s\n",
      "epoch 24 | loss: 0.0803  | val_0_auc: 0.78576 |  0:04:36s\n",
      "epoch 25 | loss: 0.07987 | val_0_auc: 0.80037 |  0:04:46s\n",
      "epoch 26 | loss: 0.07925 | val_0_auc: 0.79681 |  0:04:57s\n",
      "epoch 27 | loss: 0.07918 | val_0_auc: 0.78039 |  0:05:07s\n",
      "epoch 28 | loss: 0.07917 | val_0_auc: 0.79226 |  0:05:18s\n",
      "epoch 29 | loss: 0.07916 | val_0_auc: 0.79688 |  0:05:29s\n",
      "epoch 30 | loss: 0.079   | val_0_auc: 0.7981  |  0:05:39s\n",
      "epoch 31 | loss: 0.07904 | val_0_auc: 0.79385 |  0:05:50s\n",
      "epoch 32 | loss: 0.07893 | val_0_auc: 0.7841  |  0:06:00s\n",
      "epoch 33 | loss: 0.07922 | val_0_auc: 0.79851 |  0:06:11s\n",
      "epoch 34 | loss: 0.07919 | val_0_auc: 0.79874 |  0:06:22s\n",
      "epoch 35 | loss: 0.07893 | val_0_auc: 0.79128 |  0:06:32s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.80037\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.800321  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 399.2590\n",
      "Function value obtained: -0.8003\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.6288251372245577, 'lambda_sparse': 0.050229856457073084, 'n_steps': 4, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.35309 | val_0_auc: 0.51184 |  0:00:13s\n",
      "epoch 1  | loss: 0.14962 | val_0_auc: 0.481   |  0:00:27s\n",
      "epoch 2  | loss: 0.12332 | val_0_auc: 0.5485  |  0:00:40s\n",
      "epoch 3  | loss: 0.11322 | val_0_auc: 0.5534  |  0:00:54s\n",
      "epoch 4  | loss: 0.10606 | val_0_auc: 0.5158  |  0:01:07s\n",
      "epoch 5  | loss: 0.09714 | val_0_auc: 0.63364 |  0:01:20s\n",
      "epoch 6  | loss: 0.09543 | val_0_auc: 0.63687 |  0:01:33s\n",
      "epoch 7  | loss: 0.09253 | val_0_auc: 0.52256 |  0:01:45s\n",
      "epoch 8  | loss: 0.09159 | val_0_auc: 0.59405 |  0:01:58s\n",
      "epoch 9  | loss: 0.09237 | val_0_auc: 0.64757 |  0:02:10s\n",
      "epoch 10 | loss: 0.09131 | val_0_auc: 0.64086 |  0:02:23s\n",
      "epoch 11 | loss: 0.09261 | val_0_auc: 0.64848 |  0:02:35s\n",
      "epoch 12 | loss: 0.09118 | val_0_auc: 0.61976 |  0:02:47s\n",
      "epoch 13 | loss: 0.09133 | val_0_auc: 0.64619 |  0:03:00s\n",
      "epoch 14 | loss: 0.09304 | val_0_auc: 0.62839 |  0:03:12s\n",
      "epoch 15 | loss: 0.09178 | val_0_auc: 0.64555 |  0:03:25s\n",
      "epoch 16 | loss: 0.0907  | val_0_auc: 0.61915 |  0:03:37s\n",
      "epoch 17 | loss: 0.0903  | val_0_auc: 0.65412 |  0:03:49s\n",
      "epoch 18 | loss: 0.09085 | val_0_auc: 0.62497 |  0:04:02s\n",
      "epoch 19 | loss: 0.09196 | val_0_auc: 0.6345  |  0:04:14s\n",
      "epoch 20 | loss: 0.09143 | val_0_auc: 0.63605 |  0:04:26s\n",
      "epoch 21 | loss: 0.09143 | val_0_auc: 0.63853 |  0:04:39s\n",
      "epoch 22 | loss: 0.09175 | val_0_auc: 0.62626 |  0:04:52s\n",
      "epoch 23 | loss: 0.09045 | val_0_auc: 0.60676 |  0:05:04s\n",
      "epoch 24 | loss: 0.09091 | val_0_auc: 0.62994 |  0:05:17s\n",
      "epoch 25 | loss: 0.09112 | val_0_auc: 0.61874 |  0:05:29s\n",
      "epoch 26 | loss: 0.09151 | val_0_auc: 0.62847 |  0:05:42s\n",
      "epoch 27 | loss: 0.09096 | val_0_auc: 0.63127 |  0:05:54s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.65412\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.654121  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 361.7508\n",
      "Function value obtained: -0.6541\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.8050693934300004, 'lambda_sparse': 0.06544476082885765, 'n_steps': 9, 'n_a': 16, 'n_d': 16, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.53785 | val_0_auc: 0.52109 |  0:00:28s\n",
      "epoch 1  | loss: 0.15519 | val_0_auc: 0.51131 |  0:00:57s\n",
      "epoch 2  | loss: 0.14208 | val_0_auc: 0.56717 |  0:01:25s\n",
      "epoch 3  | loss: 0.16069 | val_0_auc: 0.48116 |  0:01:53s\n",
      "epoch 4  | loss: 0.11563 | val_0_auc: 0.48133 |  0:02:21s\n",
      "epoch 5  | loss: 0.10564 | val_0_auc: 0.54347 |  0:02:48s\n",
      "epoch 6  | loss: 0.09933 | val_0_auc: 0.50661 |  0:03:15s\n",
      "epoch 7  | loss: 0.09925 | val_0_auc: 0.56493 |  0:03:43s\n",
      "epoch 8  | loss: 0.09929 | val_0_auc: 0.54512 |  0:04:10s\n",
      "epoch 9  | loss: 0.09915 | val_0_auc: 0.54277 |  0:04:37s\n",
      "epoch 10 | loss: 0.09704 | val_0_auc: 0.55675 |  0:05:05s\n",
      "epoch 11 | loss: 0.09728 | val_0_auc: 0.55412 |  0:05:32s\n",
      "epoch 12 | loss: 0.0989  | val_0_auc: 0.60684 |  0:06:00s\n",
      "epoch 13 | loss: 0.09743 | val_0_auc: 0.56376 |  0:06:28s\n",
      "epoch 14 | loss: 0.09885 | val_0_auc: 0.54711 |  0:06:55s\n",
      "epoch 15 | loss: 0.09631 | val_0_auc: 0.53035 |  0:07:23s\n",
      "epoch 16 | loss: 0.0961  | val_0_auc: 0.58633 |  0:07:51s\n",
      "epoch 17 | loss: 0.09598 | val_0_auc: 0.58839 |  0:08:18s\n",
      "epoch 18 | loss: 0.09625 | val_0_auc: 0.46492 |  0:08:46s\n",
      "epoch 19 | loss: 0.09563 | val_0_auc: 0.50582 |  0:09:13s\n",
      "epoch 20 | loss: 0.09515 | val_0_auc: 0.51877 |  0:09:40s\n",
      "epoch 21 | loss: 0.09587 | val_0_auc: 0.56603 |  0:10:08s\n",
      "epoch 22 | loss: 0.09544 | val_0_auc: 0.51754 |  0:10:35s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.60684\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.606842  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 656.0687\n",
      "Function value obtained: -0.6068\n",
      "Current minimum: -0.8129\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0, 'lambda_sparse': 0.0001, 'n_steps': 3, 'n_a': 8, 'n_d': 8, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.31931 | val_0_auc: 0.5508  |  0:00:09s\n",
      "epoch 1  | loss: 0.10513 | val_0_auc: 0.50339 |  0:00:19s\n",
      "epoch 2  | loss: 0.09491 | val_0_auc: 0.58056 |  0:00:29s\n",
      "epoch 3  | loss: 0.09164 | val_0_auc: 0.55691 |  0:00:39s\n",
      "epoch 4  | loss: 0.09167 | val_0_auc: 0.5888  |  0:00:49s\n",
      "epoch 5  | loss: 0.09165 | val_0_auc: 0.65931 |  0:00:58s\n",
      "epoch 6  | loss: 0.09022 | val_0_auc: 0.69007 |  0:01:08s\n",
      "epoch 7  | loss: 0.08759 | val_0_auc: 0.73168 |  0:01:18s\n",
      "epoch 8  | loss: 0.08476 | val_0_auc: 0.78501 |  0:01:27s\n",
      "epoch 9  | loss: 0.08297 | val_0_auc: 0.80932 |  0:01:37s\n",
      "epoch 10 | loss: 0.08089 | val_0_auc: 0.80859 |  0:01:47s\n",
      "epoch 11 | loss: 0.08048 | val_0_auc: 0.81765 |  0:01:57s\n",
      "epoch 12 | loss: 0.0798  | val_0_auc: 0.80841 |  0:02:07s\n",
      "epoch 13 | loss: 0.08197 | val_0_auc: 0.82717 |  0:02:16s\n",
      "epoch 14 | loss: 0.07992 | val_0_auc: 0.83065 |  0:02:26s\n",
      "epoch 15 | loss: 0.07881 | val_0_auc: 0.83549 |  0:02:35s\n",
      "epoch 16 | loss: 0.0789  | val_0_auc: 0.83508 |  0:02:45s\n",
      "epoch 17 | loss: 0.0796  | val_0_auc: 0.82445 |  0:02:55s\n",
      "epoch 18 | loss: 0.0782  | val_0_auc: 0.83069 |  0:03:05s\n",
      "epoch 19 | loss: 0.07826 | val_0_auc: 0.82466 |  0:03:14s\n",
      "epoch 20 | loss: 0.07816 | val_0_auc: 0.82935 |  0:03:24s\n",
      "epoch 21 | loss: 0.07855 | val_0_auc: 0.82579 |  0:03:34s\n",
      "epoch 22 | loss: 0.07788 | val_0_auc: 0.83025 |  0:03:44s\n",
      "epoch 23 | loss: 0.07762 | val_0_auc: 0.83166 |  0:03:53s\n",
      "epoch 24 | loss: 0.07809 | val_0_auc: 0.82761 |  0:04:03s\n",
      "epoch 25 | loss: 0.07725 | val_0_auc: 0.81814 |  0:04:13s\n",
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_val_0_auc = 0.83549\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.835493  of boosting iteration \n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 260.9852\n",
      "Function value obtained: -0.8355\n",
      "Current minimum: -0.8355\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0, 'lambda_sparse': 0.0001, 'n_steps': 3, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.14505 | val_0_auc: 0.45731 |  0:00:13s\n",
      "epoch 1  | loss: 0.10458 | val_0_auc: 0.49699 |  0:00:26s\n",
      "epoch 2  | loss: 0.09049 | val_0_auc: 0.74643 |  0:00:40s\n",
      "epoch 3  | loss: 0.08493 | val_0_auc: 0.77902 |  0:00:53s\n",
      "epoch 4  | loss: 0.08484 | val_0_auc: 0.7824  |  0:01:07s\n",
      "epoch 5  | loss: 0.08238 | val_0_auc: 0.82115 |  0:01:20s\n",
      "epoch 6  | loss: 0.08371 | val_0_auc: 0.82557 |  0:01:34s\n",
      "epoch 7  | loss: 0.08101 | val_0_auc: 0.82238 |  0:01:47s\n",
      "epoch 8  | loss: 0.0816  | val_0_auc: 0.82024 |  0:02:01s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.08056 | val_0_auc: 0.82436 |  0:02:14s\n",
      "epoch 10 | loss: 0.08036 | val_0_auc: 0.81959 |  0:02:28s\n",
      "epoch 11 | loss: 0.08005 | val_0_auc: 0.80545 |  0:02:41s\n",
      "epoch 12 | loss: 0.07864 | val_0_auc: 0.81474 |  0:02:55s\n",
      "epoch 13 | loss: 0.07901 | val_0_auc: 0.81939 |  0:03:08s\n",
      "epoch 14 | loss: 0.0793  | val_0_auc: 0.81291 |  0:03:21s\n",
      "epoch 15 | loss: 0.07971 | val_0_auc: 0.81812 |  0:03:35s\n",
      "epoch 16 | loss: 0.07831 | val_0_auc: 0.82741 |  0:03:48s\n",
      "epoch 17 | loss: 0.07848 | val_0_auc: 0.82301 |  0:04:02s\n",
      "epoch 18 | loss: 0.07779 | val_0_auc: 0.82373 |  0:04:15s\n",
      "epoch 19 | loss: 0.07836 | val_0_auc: 0.82674 |  0:04:28s\n",
      "epoch 20 | loss: 0.07756 | val_0_auc: 0.81751 |  0:04:42s\n",
      "epoch 21 | loss: 0.07748 | val_0_auc: 0.81965 |  0:04:55s\n",
      "epoch 22 | loss: 0.07782 | val_0_auc: 0.80211 |  0:05:10s\n",
      "epoch 23 | loss: 0.07768 | val_0_auc: 0.81191 |  0:05:23s\n",
      "epoch 24 | loss: 0.07729 | val_0_auc: 0.81454 |  0:05:37s\n",
      "epoch 25 | loss: 0.07697 | val_0_auc: 0.81324 |  0:05:50s\n",
      "epoch 26 | loss: 0.07659 | val_0_auc: 0.78011 |  0:06:04s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.82741\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.82741  of boosting iteration \n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 374.5274\n",
      "Function value obtained: -0.8274\n",
      "Current minimum: -0.8355\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2917828325804677, 'lambda_sparse': 0.0001, 'n_steps': 3, 'n_a': 8, 'n_d': 8, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.31368 | val_0_auc: 0.51796 |  0:00:09s\n",
      "epoch 1  | loss: 0.09917 | val_0_auc: 0.5704  |  0:00:19s\n",
      "epoch 2  | loss: 0.09203 | val_0_auc: 0.58039 |  0:00:29s\n",
      "epoch 3  | loss: 0.09081 | val_0_auc: 0.60419 |  0:00:39s\n",
      "epoch 4  | loss: 0.09019 | val_0_auc: 0.66132 |  0:00:49s\n",
      "epoch 5  | loss: 0.08854 | val_0_auc: 0.65617 |  0:00:58s\n",
      "epoch 6  | loss: 0.08852 | val_0_auc: 0.71255 |  0:01:08s\n",
      "epoch 7  | loss: 0.08758 | val_0_auc: 0.77365 |  0:01:18s\n",
      "epoch 8  | loss: 0.08386 | val_0_auc: 0.79603 |  0:01:28s\n",
      "epoch 9  | loss: 0.08332 | val_0_auc: 0.81113 |  0:01:38s\n",
      "epoch 10 | loss: 0.08143 | val_0_auc: 0.80044 |  0:01:48s\n",
      "epoch 11 | loss: 0.08173 | val_0_auc: 0.82074 |  0:01:58s\n",
      "epoch 12 | loss: 0.08155 | val_0_auc: 0.8098  |  0:02:07s\n",
      "epoch 13 | loss: 0.0806  | val_0_auc: 0.80568 |  0:02:17s\n",
      "epoch 14 | loss: 0.08046 | val_0_auc: 0.82946 |  0:02:27s\n",
      "epoch 15 | loss: 0.08042 | val_0_auc: 0.83385 |  0:02:37s\n",
      "epoch 16 | loss: 0.08002 | val_0_auc: 0.82947 |  0:02:46s\n",
      "epoch 17 | loss: 0.07992 | val_0_auc: 0.82149 |  0:02:56s\n",
      "epoch 18 | loss: 0.07881 | val_0_auc: 0.83549 |  0:03:06s\n",
      "epoch 19 | loss: 0.07938 | val_0_auc: 0.8308  |  0:03:16s\n",
      "epoch 20 | loss: 0.07846 | val_0_auc: 0.83109 |  0:03:26s\n",
      "epoch 21 | loss: 0.07944 | val_0_auc: 0.82242 |  0:03:36s\n",
      "epoch 22 | loss: 0.07849 | val_0_auc: 0.814   |  0:03:46s\n",
      "epoch 23 | loss: 0.07866 | val_0_auc: 0.83632 |  0:03:55s\n",
      "epoch 24 | loss: 0.07878 | val_0_auc: 0.82396 |  0:04:05s\n",
      "epoch 25 | loss: 0.07816 | val_0_auc: 0.82754 |  0:04:15s\n",
      "epoch 26 | loss: 0.07803 | val_0_auc: 0.83131 |  0:04:25s\n",
      "epoch 27 | loss: 0.0777  | val_0_auc: 0.82308 |  0:04:35s\n",
      "epoch 28 | loss: 0.07784 | val_0_auc: 0.82154 |  0:04:45s\n",
      "epoch 29 | loss: 0.07768 | val_0_auc: 0.83501 |  0:04:55s\n",
      "epoch 30 | loss: 0.07761 | val_0_auc: 0.83466 |  0:05:05s\n",
      "epoch 31 | loss: 0.07777 | val_0_auc: 0.84292 |  0:05:15s\n",
      "epoch 32 | loss: 0.07738 | val_0_auc: 0.82118 |  0:05:25s\n",
      "epoch 33 | loss: 0.07745 | val_0_auc: 0.84127 |  0:05:35s\n",
      "epoch 34 | loss: 0.07681 | val_0_auc: 0.82749 |  0:05:45s\n",
      "epoch 35 | loss: 0.07665 | val_0_auc: 0.84176 |  0:05:55s\n",
      "epoch 36 | loss: 0.07678 | val_0_auc: 0.83435 |  0:06:05s\n",
      "epoch 37 | loss: 0.07618 | val_0_auc: 0.83644 |  0:06:15s\n",
      "epoch 38 | loss: 0.07576 | val_0_auc: 0.83348 |  0:06:25s\n",
      "epoch 39 | loss: 0.07608 | val_0_auc: 0.81577 |  0:06:35s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 31 and best_val_0_auc = 0.84292\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.842918  of boosting iteration \n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 404.4888\n",
      "Function value obtained: -0.8429\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1813928671251266, 'lambda_sparse': 0.012010224070240877, 'n_steps': 3, 'n_a': 8, 'n_d': 8, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.3077  | val_0_auc: 0.53049 |  0:00:10s\n",
      "epoch 1  | loss: 0.11544 | val_0_auc: 0.47716 |  0:00:20s\n",
      "epoch 2  | loss: 0.10627 | val_0_auc: 0.50335 |  0:00:29s\n",
      "epoch 3  | loss: 0.09787 | val_0_auc: 0.51187 |  0:00:39s\n",
      "epoch 4  | loss: 0.09563 | val_0_auc: 0.48384 |  0:00:49s\n",
      "epoch 5  | loss: 0.09416 | val_0_auc: 0.53363 |  0:00:58s\n",
      "epoch 6  | loss: 0.09298 | val_0_auc: 0.53173 |  0:01:07s\n",
      "epoch 7  | loss: 0.09124 | val_0_auc: 0.50728 |  0:01:16s\n",
      "epoch 8  | loss: 0.09146 | val_0_auc: 0.51177 |  0:01:24s\n",
      "epoch 9  | loss: 0.09141 | val_0_auc: 0.53857 |  0:01:33s\n",
      "epoch 10 | loss: 0.09121 | val_0_auc: 0.51422 |  0:01:41s\n",
      "epoch 11 | loss: 0.0923  | val_0_auc: 0.50496 |  0:01:50s\n",
      "epoch 12 | loss: 0.09195 | val_0_auc: 0.47752 |  0:01:58s\n",
      "epoch 13 | loss: 0.09114 | val_0_auc: 0.5469  |  0:02:07s\n",
      "epoch 14 | loss: 0.0909  | val_0_auc: 0.51418 |  0:02:15s\n",
      "epoch 15 | loss: 0.09132 | val_0_auc: 0.51296 |  0:02:24s\n",
      "epoch 16 | loss: 0.09106 | val_0_auc: 0.55018 |  0:02:32s\n",
      "epoch 17 | loss: 0.0919  | val_0_auc: 0.5146  |  0:02:41s\n",
      "epoch 18 | loss: 0.0912  | val_0_auc: 0.46579 |  0:02:49s\n",
      "epoch 19 | loss: 0.09096 | val_0_auc: 0.53613 |  0:02:58s\n",
      "epoch 20 | loss: 0.0908  | val_0_auc: 0.53466 |  0:03:06s\n",
      "epoch 21 | loss: 0.09101 | val_0_auc: 0.47441 |  0:03:14s\n",
      "epoch 22 | loss: 0.09084 | val_0_auc: 0.47431 |  0:03:23s\n",
      "epoch 23 | loss: 0.09097 | val_0_auc: 0.5467  |  0:03:31s\n",
      "epoch 24 | loss: 0.09173 | val_0_auc: 0.52844 |  0:03:40s\n",
      "epoch 25 | loss: 0.09081 | val_0_auc: 0.46242 |  0:03:48s\n",
      "epoch 26 | loss: 0.09095 | val_0_auc: 0.46226 |  0:03:57s\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_val_0_auc = 0.55018\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.550289  of boosting iteration \n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 243.1676\n",
      "Function value obtained: -0.5503\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2814536792078353, 'lambda_sparse': 0.00335813420549488, 'n_steps': 8, 'n_a': 64, 'n_d': 64, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.19098 | val_0_auc: 0.49202 |  0:00:31s\n",
      "epoch 1  | loss: 0.14645 | val_0_auc: 0.5215  |  0:01:03s\n",
      "epoch 2  | loss: 0.20135 | val_0_auc: 0.52059 |  0:01:35s\n",
      "epoch 3  | loss: 0.11531 | val_0_auc: 0.54304 |  0:02:07s\n",
      "epoch 4  | loss: 0.09262 | val_0_auc: 0.6775  |  0:02:39s\n",
      "epoch 5  | loss: 0.0927  | val_0_auc: 0.72536 |  0:03:10s\n",
      "epoch 6  | loss: 0.09049 | val_0_auc: 0.75087 |  0:03:42s\n",
      "epoch 7  | loss: 0.08929 | val_0_auc: 0.71263 |  0:04:13s\n",
      "epoch 8  | loss: 0.08864 | val_0_auc: 0.68396 |  0:04:45s\n",
      "epoch 9  | loss: 0.08581 | val_0_auc: 0.75589 |  0:05:17s\n",
      "epoch 10 | loss: 0.08563 | val_0_auc: 0.77183 |  0:05:48s\n",
      "epoch 11 | loss: 0.08474 | val_0_auc: 0.77754 |  0:06:19s\n",
      "epoch 12 | loss: 0.08522 | val_0_auc: 0.7409  |  0:06:50s\n",
      "epoch 13 | loss: 0.08448 | val_0_auc: 0.76411 |  0:07:22s\n",
      "epoch 14 | loss: 0.08478 | val_0_auc: 0.77564 |  0:07:53s\n",
      "epoch 15 | loss: 0.08429 | val_0_auc: 0.76252 |  0:08:25s\n",
      "epoch 16 | loss: 0.08458 | val_0_auc: 0.76904 |  0:08:56s\n",
      "epoch 17 | loss: 0.0851  | val_0_auc: 0.77679 |  0:09:28s\n",
      "epoch 18 | loss: 0.08473 | val_0_auc: 0.7749  |  0:09:59s\n",
      "epoch 19 | loss: 0.08465 | val_0_auc: 0.77121 |  0:10:30s\n",
      "epoch 20 | loss: 0.08419 | val_0_auc: 0.75904 |  0:11:01s\n",
      "epoch 21 | loss: 0.08401 | val_0_auc: 0.75762 |  0:11:32s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.77754\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.777551  of boosting iteration \n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 712.6332\n",
      "Function value obtained: -0.7776\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.2894231788136723, 'lambda_sparse': 0.08307822563602811, 'n_steps': 5, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.29926 | val_0_auc: 0.47195 |  0:00:19s\n",
      "epoch 1  | loss: 0.19936 | val_0_auc: 0.51221 |  0:00:38s\n",
      "epoch 2  | loss: 0.15886 | val_0_auc: 0.48714 |  0:00:57s\n",
      "epoch 3  | loss: 0.1319  | val_0_auc: 0.4909  |  0:01:15s\n",
      "epoch 4  | loss: 0.10452 | val_0_auc: 0.55639 |  0:01:33s\n",
      "epoch 5  | loss: 0.0975  | val_0_auc: 0.4834  |  0:01:51s\n",
      "epoch 6  | loss: 0.09826 | val_0_auc: 0.60486 |  0:02:09s\n",
      "epoch 7  | loss: 0.09567 | val_0_auc: 0.54333 |  0:02:26s\n",
      "epoch 8  | loss: 0.09654 | val_0_auc: 0.56078 |  0:02:44s\n",
      "epoch 9  | loss: 0.09535 | val_0_auc: 0.58827 |  0:03:02s\n",
      "epoch 10 | loss: 0.09508 | val_0_auc: 0.59333 |  0:03:19s\n",
      "epoch 11 | loss: 0.0949  | val_0_auc: 0.5505  |  0:03:37s\n",
      "epoch 12 | loss: 0.09305 | val_0_auc: 0.62585 |  0:03:54s\n",
      "epoch 13 | loss: 0.09231 | val_0_auc: 0.62604 |  0:04:12s\n",
      "epoch 14 | loss: 0.09071 | val_0_auc: 0.61616 |  0:04:29s\n",
      "epoch 15 | loss: 0.09147 | val_0_auc: 0.60645 |  0:04:47s\n",
      "epoch 16 | loss: 0.09195 | val_0_auc: 0.61167 |  0:05:05s\n",
      "epoch 17 | loss: 0.09139 | val_0_auc: 0.62085 |  0:05:23s\n",
      "epoch 18 | loss: 0.09056 | val_0_auc: 0.6277  |  0:05:40s\n",
      "epoch 19 | loss: 0.09025 | val_0_auc: 0.62103 |  0:05:58s\n",
      "epoch 20 | loss: 0.09108 | val_0_auc: 0.62706 |  0:06:15s\n",
      "epoch 21 | loss: 0.08998 | val_0_auc: 0.6155  |  0:06:33s\n",
      "epoch 22 | loss: 0.09037 | val_0_auc: 0.6175  |  0:06:50s\n",
      "epoch 23 | loss: 0.09063 | val_0_auc: 0.61436 |  0:07:08s\n",
      "epoch 24 | loss: 0.09016 | val_0_auc: 0.62263 |  0:07:25s\n",
      "epoch 25 | loss: 0.09074 | val_0_auc: 0.62346 |  0:07:43s\n",
      "epoch 26 | loss: 0.09548 | val_0_auc: 0.61147 |  0:08:00s\n",
      "epoch 27 | loss: 0.0918  | val_0_auc: 0.64457 |  0:08:17s\n",
      "epoch 28 | loss: 0.09092 | val_0_auc: 0.6312  |  0:08:35s\n",
      "epoch 29 | loss: 0.09088 | val_0_auc: 0.59449 |  0:08:52s\n",
      "epoch 30 | loss: 0.09255 | val_0_auc: 0.57252 |  0:09:09s\n",
      "epoch 31 | loss: 0.09157 | val_0_auc: 0.5907  |  0:09:26s\n",
      "epoch 32 | loss: 0.09122 | val_0_auc: 0.60404 |  0:09:43s\n",
      "epoch 33 | loss: 0.09108 | val_0_auc: 0.63712 |  0:10:01s\n",
      "epoch 34 | loss: 0.09168 | val_0_auc: 0.59745 |  0:10:18s\n",
      "epoch 35 | loss: 0.09173 | val_0_auc: 0.51098 |  0:10:35s\n",
      "epoch 36 | loss: 0.0915  | val_0_auc: 0.59693 |  0:10:53s\n",
      "epoch 37 | loss: 0.0917  | val_0_auc: 0.59188 |  0:11:10s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_auc = 0.64457\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.644222  of boosting iteration \n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 682.5481\n",
      "Function value obtained: -0.6442\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1568598988890153, 'lambda_sparse': 0.0001, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.15399 | val_0_auc: 0.48587 |  0:00:21s\n",
      "epoch 1  | loss: 0.11    | val_0_auc: 0.46408 |  0:00:43s\n",
      "epoch 2  | loss: 0.10261 | val_0_auc: 0.5083  |  0:01:04s\n",
      "epoch 3  | loss: 0.11154 | val_0_auc: 0.47374 |  0:01:25s\n",
      "epoch 4  | loss: 0.09616 | val_0_auc: 0.51386 |  0:01:47s\n",
      "epoch 5  | loss: 0.09388 | val_0_auc: 0.50382 |  0:02:08s\n",
      "epoch 6  | loss: 0.09456 | val_0_auc: 0.47349 |  0:02:29s\n",
      "epoch 7  | loss: 0.09339 | val_0_auc: 0.49204 |  0:02:49s\n",
      "epoch 8  | loss: 0.09208 | val_0_auc: 0.5051  |  0:03:10s\n",
      "epoch 9  | loss: 0.0919  | val_0_auc: 0.46918 |  0:03:31s\n",
      "epoch 10 | loss: 0.09278 | val_0_auc: 0.55443 |  0:03:53s\n",
      "epoch 11 | loss: 0.09331 | val_0_auc: 0.47937 |  0:04:14s\n",
      "epoch 12 | loss: 0.09273 | val_0_auc: 0.49676 |  0:04:34s\n",
      "epoch 13 | loss: 0.09428 | val_0_auc: 0.45657 |  0:04:54s\n",
      "epoch 14 | loss: 0.09095 | val_0_auc: 0.47077 |  0:05:15s\n",
      "epoch 15 | loss: 0.09096 | val_0_auc: 0.55185 |  0:05:36s\n",
      "epoch 16 | loss: 0.09091 | val_0_auc: 0.58569 |  0:05:56s\n",
      "epoch 17 | loss: 0.09021 | val_0_auc: 0.62747 |  0:06:17s\n",
      "epoch 18 | loss: 0.08996 | val_0_auc: 0.62692 |  0:06:38s\n",
      "epoch 19 | loss: 0.09066 | val_0_auc: 0.60331 |  0:06:58s\n",
      "epoch 20 | loss: 0.0911  | val_0_auc: 0.56121 |  0:07:19s\n",
      "epoch 21 | loss: 0.09131 | val_0_auc: 0.55338 |  0:07:39s\n",
      "epoch 22 | loss: 0.09075 | val_0_auc: 0.56792 |  0:08:00s\n",
      "epoch 23 | loss: 0.09076 | val_0_auc: 0.55221 |  0:08:20s\n",
      "epoch 24 | loss: 0.09077 | val_0_auc: 0.59338 |  0:08:40s\n",
      "epoch 25 | loss: 0.09004 | val_0_auc: 0.57605 |  0:09:00s\n",
      "epoch 26 | loss: 0.09107 | val_0_auc: 0.5934  |  0:09:19s\n",
      "epoch 27 | loss: 0.09087 | val_0_auc: 0.57765 |  0:09:39s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_val_0_auc = 0.62747\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.627469  of boosting iteration \n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 594.6643\n",
      "Function value obtained: -0.6275\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3256485914891876, 'lambda_sparse': 0.0001, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.95}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.14803 | val_0_auc: 0.48048 |  0:00:21s\n",
      "epoch 1  | loss: 0.10178 | val_0_auc: 0.5882  |  0:00:42s\n",
      "epoch 2  | loss: 0.09342 | val_0_auc: 0.64469 |  0:01:03s\n",
      "epoch 3  | loss: 0.09204 | val_0_auc: 0.75774 |  0:01:25s\n",
      "epoch 4  | loss: 0.09851 | val_0_auc: 0.74766 |  0:01:46s\n",
      "epoch 5  | loss: 0.08717 | val_0_auc: 0.7611  |  0:02:07s\n",
      "epoch 6  | loss: 0.08632 | val_0_auc: 0.73051 |  0:02:28s\n",
      "epoch 7  | loss: 0.08485 | val_0_auc: 0.71319 |  0:02:48s\n",
      "epoch 8  | loss: 0.08507 | val_0_auc: 0.7012  |  0:03:09s\n",
      "epoch 9  | loss: 0.08631 | val_0_auc: 0.71389 |  0:03:30s\n",
      "epoch 10 | loss: 0.08518 | val_0_auc: 0.75285 |  0:03:51s\n",
      "epoch 11 | loss: 0.08582 | val_0_auc: 0.78881 |  0:04:12s\n",
      "epoch 12 | loss: 0.08355 | val_0_auc: 0.77065 |  0:04:32s\n",
      "epoch 13 | loss: 0.08436 | val_0_auc: 0.79232 |  0:04:53s\n",
      "epoch 14 | loss: 0.0828  | val_0_auc: 0.78856 |  0:05:15s\n",
      "epoch 15 | loss: 0.08324 | val_0_auc: 0.78964 |  0:05:36s\n",
      "epoch 16 | loss: 0.08293 | val_0_auc: 0.78766 |  0:05:56s\n",
      "epoch 17 | loss: 0.08261 | val_0_auc: 0.79237 |  0:06:17s\n",
      "epoch 18 | loss: 0.08235 | val_0_auc: 0.80604 |  0:06:38s\n",
      "epoch 19 | loss: 0.0823  | val_0_auc: 0.78882 |  0:06:59s\n",
      "epoch 20 | loss: 0.08181 | val_0_auc: 0.80696 |  0:07:19s\n",
      "epoch 21 | loss: 0.08252 | val_0_auc: 0.80375 |  0:07:40s\n",
      "epoch 22 | loss: 0.08168 | val_0_auc: 0.80881 |  0:08:01s\n",
      "epoch 23 | loss: 0.08105 | val_0_auc: 0.81273 |  0:08:21s\n",
      "epoch 24 | loss: 0.0811  | val_0_auc: 0.81177 |  0:08:42s\n",
      "epoch 25 | loss: 0.08046 | val_0_auc: 0.82125 |  0:09:03s\n",
      "epoch 26 | loss: 0.08153 | val_0_auc: 0.80059 |  0:09:23s\n",
      "epoch 27 | loss: 0.08196 | val_0_auc: 0.79499 |  0:09:44s\n",
      "epoch 28 | loss: 0.08272 | val_0_auc: 0.78174 |  0:10:05s\n",
      "epoch 29 | loss: 0.08222 | val_0_auc: 0.77487 |  0:10:25s\n",
      "epoch 30 | loss: 0.08262 | val_0_auc: 0.73937 |  0:10:45s\n",
      "epoch 31 | loss: 0.08274 | val_0_auc: 0.7935  |  0:11:06s\n",
      "epoch 32 | loss: 0.08261 | val_0_auc: 0.79089 |  0:11:26s\n",
      "epoch 33 | loss: 0.08254 | val_0_auc: 0.79397 |  0:11:47s\n",
      "epoch 34 | loss: 0.08271 | val_0_auc: 0.78508 |  0:12:08s\n",
      "epoch 35 | loss: 0.08318 | val_0_auc: 0.76698 |  0:12:28s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.82125\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.821248  of boosting iteration \n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 763.7520\n",
      "Function value obtained: -0.8212\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.263126899388804, 'lambda_sparse': 0.027653328481288868, 'n_steps': 6, 'n_a': 24, 'n_d': 24, 'momentum': 0.9}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.18219 | val_0_auc: 0.49162 |  0:00:21s\n",
      "epoch 1  | loss: 0.13476 | val_0_auc: 0.54295 |  0:00:42s\n",
      "epoch 2  | loss: 0.12053 | val_0_auc: 0.61386 |  0:01:03s\n",
      "epoch 3  | loss: 0.10924 | val_0_auc: 0.70369 |  0:01:24s\n",
      "epoch 4  | loss: 0.1052  | val_0_auc: 0.66369 |  0:01:45s\n",
      "epoch 5  | loss: 0.10211 | val_0_auc: 0.70928 |  0:02:06s\n",
      "epoch 6  | loss: 0.0981  | val_0_auc: 0.65726 |  0:02:27s\n",
      "epoch 7  | loss: 0.09498 | val_0_auc: 0.65094 |  0:02:47s\n",
      "epoch 8  | loss: 0.09245 | val_0_auc: 0.68732 |  0:03:08s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9  | loss: 0.09042 | val_0_auc: 0.66852 |  0:03:29s\n",
      "epoch 10 | loss: 0.09203 | val_0_auc: 0.67662 |  0:03:49s\n",
      "epoch 11 | loss: 0.0955  | val_0_auc: 0.68604 |  0:04:10s\n",
      "epoch 12 | loss: 0.09071 | val_0_auc: 0.71687 |  0:04:30s\n",
      "epoch 13 | loss: 0.09026 | val_0_auc: 0.7105  |  0:04:50s\n",
      "epoch 14 | loss: 0.08824 | val_0_auc: 0.73679 |  0:05:12s\n",
      "epoch 15 | loss: 0.08862 | val_0_auc: 0.72489 |  0:05:32s\n",
      "epoch 16 | loss: 0.08891 | val_0_auc: 0.69861 |  0:05:51s\n",
      "epoch 17 | loss: 0.08823 | val_0_auc: 0.69575 |  0:06:11s\n",
      "epoch 18 | loss: 0.08748 | val_0_auc: 0.69407 |  0:06:31s\n",
      "epoch 19 | loss: 0.08706 | val_0_auc: 0.73726 |  0:06:51s\n",
      "epoch 20 | loss: 0.08684 | val_0_auc: 0.68763 |  0:07:11s\n",
      "epoch 21 | loss: 0.08781 | val_0_auc: 0.69055 |  0:07:31s\n",
      "epoch 22 | loss: 0.08641 | val_0_auc: 0.69627 |  0:07:51s\n",
      "epoch 23 | loss: 0.08631 | val_0_auc: 0.70564 |  0:08:11s\n",
      "epoch 24 | loss: 0.08677 | val_0_auc: 0.7062  |  0:08:31s\n",
      "epoch 25 | loss: 0.08623 | val_0_auc: 0.69034 |  0:08:51s\n",
      "epoch 26 | loss: 0.08725 | val_0_auc: 0.69372 |  0:09:11s\n",
      "epoch 27 | loss: 0.0873  | val_0_auc: 0.69474 |  0:09:31s\n",
      "epoch 28 | loss: 0.08693 | val_0_auc: 0.68761 |  0:09:51s\n",
      "epoch 29 | loss: 0.08636 | val_0_auc: 0.65243 |  0:10:11s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_auc = 0.73726\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.737263  of boosting iteration \n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 626.3893\n",
      "Function value obtained: -0.7373\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.104381586220289, 'lambda_sparse': 0.029797113857238062, 'n_steps': 4, 'n_a': 8, 'n_d': 8, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.25845 | val_0_auc: 0.48215 |  0:00:12s\n",
      "epoch 1  | loss: 0.13856 | val_0_auc: 0.55261 |  0:00:25s\n",
      "epoch 2  | loss: 0.11745 | val_0_auc: 0.49879 |  0:00:38s\n",
      "epoch 3  | loss: 0.10341 | val_0_auc: 0.52776 |  0:00:51s\n",
      "epoch 4  | loss: 0.0965  | val_0_auc: 0.59381 |  0:01:03s\n",
      "epoch 5  | loss: 0.09369 | val_0_auc: 0.54181 |  0:01:15s\n",
      "epoch 6  | loss: 0.09241 | val_0_auc: 0.58126 |  0:01:27s\n",
      "epoch 7  | loss: 0.0921  | val_0_auc: 0.5973  |  0:01:39s\n",
      "epoch 8  | loss: 0.091   | val_0_auc: 0.65146 |  0:01:51s\n",
      "epoch 9  | loss: 0.0895  | val_0_auc: 0.72252 |  0:02:03s\n",
      "epoch 10 | loss: 0.08715 | val_0_auc: 0.75199 |  0:02:15s\n",
      "epoch 11 | loss: 0.08735 | val_0_auc: 0.77611 |  0:02:26s\n",
      "epoch 12 | loss: 0.08459 | val_0_auc: 0.79507 |  0:02:38s\n",
      "epoch 13 | loss: 0.08516 | val_0_auc: 0.77978 |  0:02:50s\n",
      "epoch 14 | loss: 0.08432 | val_0_auc: 0.80273 |  0:03:02s\n",
      "epoch 15 | loss: 0.08305 | val_0_auc: 0.80694 |  0:03:14s\n",
      "epoch 16 | loss: 0.08293 | val_0_auc: 0.80222 |  0:03:26s\n",
      "epoch 17 | loss: 0.0827  | val_0_auc: 0.81093 |  0:03:38s\n",
      "epoch 18 | loss: 0.08227 | val_0_auc: 0.81749 |  0:03:50s\n",
      "epoch 19 | loss: 0.08259 | val_0_auc: 0.8128  |  0:04:03s\n",
      "epoch 20 | loss: 0.08185 | val_0_auc: 0.8112  |  0:04:15s\n",
      "epoch 21 | loss: 0.08159 | val_0_auc: 0.81296 |  0:04:27s\n",
      "epoch 22 | loss: 0.08146 | val_0_auc: 0.81884 |  0:04:39s\n",
      "epoch 23 | loss: 0.08046 | val_0_auc: 0.81199 |  0:04:51s\n",
      "epoch 24 | loss: 0.08202 | val_0_auc: 0.80695 |  0:05:03s\n",
      "epoch 25 | loss: 0.08144 | val_0_auc: 0.81344 |  0:05:15s\n",
      "epoch 26 | loss: 0.08133 | val_0_auc: 0.80211 |  0:05:27s\n",
      "epoch 27 | loss: 0.08096 | val_0_auc: 0.81159 |  0:05:39s\n",
      "epoch 28 | loss: 0.08053 | val_0_auc: 0.81249 |  0:05:51s\n",
      "epoch 29 | loss: 0.08084 | val_0_auc: 0.79049 |  0:06:03s\n",
      "epoch 30 | loss: 0.08061 | val_0_auc: 0.81681 |  0:06:15s\n",
      "epoch 31 | loss: 0.08027 | val_0_auc: 0.81364 |  0:06:27s\n",
      "epoch 32 | loss: 0.08143 | val_0_auc: 0.80894 |  0:06:39s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.81884\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.818843  of boosting iteration \n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 410.3489\n",
      "Function value obtained: -0.8188\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1438343836888305, 'lambda_sparse': 0.02919618487880152, 'n_steps': 10, 'n_a': 8, 'n_d': 8, 'momentum': 0.98}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.22275 | val_0_auc: 0.53916 |  0:00:29s\n",
      "epoch 1  | loss: 0.15666 | val_0_auc: 0.4598  |  0:00:59s\n",
      "epoch 2  | loss: 0.12969 | val_0_auc: 0.53818 |  0:01:29s\n",
      "epoch 3  | loss: 0.11701 | val_0_auc: 0.54133 |  0:01:58s\n",
      "epoch 4  | loss: 0.11444 | val_0_auc: 0.68128 |  0:02:28s\n",
      "epoch 5  | loss: 0.11695 | val_0_auc: 0.71571 |  0:02:57s\n",
      "epoch 6  | loss: 0.10513 | val_0_auc: 0.70698 |  0:03:27s\n",
      "epoch 7  | loss: 0.10019 | val_0_auc: 0.56388 |  0:03:56s\n",
      "epoch 8  | loss: 0.09591 | val_0_auc: 0.73917 |  0:04:25s\n",
      "epoch 9  | loss: 0.09096 | val_0_auc: 0.70215 |  0:04:54s\n",
      "epoch 10 | loss: 0.09258 | val_0_auc: 0.62251 |  0:05:22s\n",
      "epoch 11 | loss: 0.09184 | val_0_auc: 0.77005 |  0:05:50s\n",
      "epoch 12 | loss: 0.09126 | val_0_auc: 0.67018 |  0:06:18s\n",
      "epoch 13 | loss: 0.09068 | val_0_auc: 0.66048 |  0:06:45s\n",
      "epoch 14 | loss: 0.09004 | val_0_auc: 0.64798 |  0:07:12s\n",
      "epoch 15 | loss: 0.09015 | val_0_auc: 0.65066 |  0:07:40s\n",
      "epoch 16 | loss: 0.08989 | val_0_auc: 0.64421 |  0:08:07s\n",
      "epoch 17 | loss: 0.0891  | val_0_auc: 0.6518  |  0:08:35s\n",
      "epoch 18 | loss: 0.08868 | val_0_auc: 0.7195  |  0:09:04s\n",
      "epoch 19 | loss: 0.08915 | val_0_auc: 0.6792  |  0:09:33s\n",
      "epoch 20 | loss: 0.08984 | val_0_auc: 0.75888 |  0:10:02s\n",
      "epoch 21 | loss: 0.08922 | val_0_auc: 0.67151 |  0:10:31s\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_val_0_auc = 0.77005\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.769952  of boosting iteration \n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 652.1553\n",
      "Function value obtained: -0.7700\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.016930464853784, 'lambda_sparse': 0.02442784747683791, 'n_steps': 3, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.28074 | val_0_auc: 0.47617 |  0:00:10s\n",
      "epoch 1  | loss: 0.12987 | val_0_auc: 0.48333 |  0:00:21s\n",
      "epoch 2  | loss: 0.11419 | val_0_auc: 0.48162 |  0:00:33s\n",
      "epoch 3  | loss: 0.10485 | val_0_auc: 0.52941 |  0:00:44s\n",
      "epoch 4  | loss: 0.09917 | val_0_auc: 0.53946 |  0:00:55s\n",
      "epoch 5  | loss: 0.09372 | val_0_auc: 0.52275 |  0:01:05s\n",
      "epoch 6  | loss: 0.09268 | val_0_auc: 0.55204 |  0:01:14s\n",
      "epoch 7  | loss: 0.09168 | val_0_auc: 0.53306 |  0:01:24s\n",
      "epoch 8  | loss: 0.09121 | val_0_auc: 0.52936 |  0:01:33s\n",
      "epoch 9  | loss: 0.09076 | val_0_auc: 0.52093 |  0:01:43s\n",
      "epoch 10 | loss: 0.09094 | val_0_auc: 0.53184 |  0:01:53s\n",
      "epoch 11 | loss: 0.09134 | val_0_auc: 0.51792 |  0:02:02s\n",
      "epoch 12 | loss: 0.09151 | val_0_auc: 0.53223 |  0:02:12s\n",
      "epoch 13 | loss: 0.09083 | val_0_auc: 0.52385 |  0:02:21s\n",
      "epoch 14 | loss: 0.0905  | val_0_auc: 0.52609 |  0:02:31s\n",
      "epoch 15 | loss: 0.09076 | val_0_auc: 0.5199  |  0:02:40s\n",
      "epoch 16 | loss: 0.09108 | val_0_auc: 0.52944 |  0:02:50s\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_val_0_auc = 0.55204\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.552043  of boosting iteration \n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 176.9174\n",
      "Function value obtained: -0.5520\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0, 'lambda_sparse': 0.0023585654232854947, 'n_steps': 9, 'n_a': 8, 'n_d': 8, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.26316 | val_0_auc: 0.51708 |  0:00:27s\n",
      "epoch 1  | loss: 0.10731 | val_0_auc: 0.56558 |  0:00:53s\n",
      "epoch 2  | loss: 0.10562 | val_0_auc: 0.56503 |  0:01:20s\n",
      "epoch 3  | loss: 0.10727 | val_0_auc: 0.60647 |  0:01:47s\n",
      "epoch 4  | loss: 0.09839 | val_0_auc: 0.7329  |  0:02:14s\n",
      "epoch 5  | loss: 0.09405 | val_0_auc: 0.70762 |  0:02:41s\n",
      "epoch 6  | loss: 0.09621 | val_0_auc: 0.6923  |  0:03:08s\n",
      "epoch 7  | loss: 0.09496 | val_0_auc: 0.6507  |  0:03:34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8  | loss: 0.08951 | val_0_auc: 0.69458 |  0:03:59s\n",
      "epoch 9  | loss: 0.08864 | val_0_auc: 0.73523 |  0:04:25s\n",
      "epoch 10 | loss: 0.08705 | val_0_auc: 0.72185 |  0:04:51s\n",
      "epoch 11 | loss: 0.08592 | val_0_auc: 0.7352  |  0:05:19s\n",
      "epoch 12 | loss: 0.08611 | val_0_auc: 0.72879 |  0:05:45s\n",
      "epoch 13 | loss: 0.08644 | val_0_auc: 0.72705 |  0:06:11s\n",
      "epoch 14 | loss: 0.08622 | val_0_auc: 0.69933 |  0:06:38s\n",
      "epoch 15 | loss: 0.08733 | val_0_auc: 0.737   |  0:07:04s\n",
      "epoch 16 | loss: 0.0867  | val_0_auc: 0.7293  |  0:07:31s\n",
      "epoch 17 | loss: 0.08664 | val_0_auc: 0.72131 |  0:07:57s\n",
      "epoch 18 | loss: 0.08663 | val_0_auc: 0.72655 |  0:08:24s\n",
      "epoch 19 | loss: 0.08541 | val_0_auc: 0.78095 |  0:08:49s\n",
      "epoch 20 | loss: 0.08585 | val_0_auc: 0.69798 |  0:09:15s\n",
      "epoch 21 | loss: 0.08575 | val_0_auc: 0.77956 |  0:09:42s\n",
      "epoch 22 | loss: 0.08478 | val_0_auc: 0.78973 |  0:10:07s\n",
      "epoch 23 | loss: 0.08234 | val_0_auc: 0.79084 |  0:10:33s\n",
      "epoch 24 | loss: 0.08039 | val_0_auc: 0.79796 |  0:10:59s\n",
      "epoch 25 | loss: 0.0801  | val_0_auc: 0.8191  |  0:11:25s\n",
      "epoch 26 | loss: 0.07957 | val_0_auc: 0.80629 |  0:11:51s\n",
      "epoch 27 | loss: 0.07959 | val_0_auc: 0.81575 |  0:12:16s\n",
      "epoch 28 | loss: 0.07992 | val_0_auc: 0.82634 |  0:12:42s\n",
      "epoch 29 | loss: 0.07881 | val_0_auc: 0.81839 |  0:13:08s\n",
      "epoch 30 | loss: 0.08004 | val_0_auc: 0.83033 |  0:13:33s\n",
      "epoch 31 | loss: 0.0796  | val_0_auc: 0.82142 |  0:13:59s\n",
      "epoch 32 | loss: 0.07963 | val_0_auc: 0.824   |  0:14:25s\n",
      "epoch 33 | loss: 0.0802  | val_0_auc: 0.82603 |  0:14:51s\n",
      "epoch 34 | loss: 0.07911 | val_0_auc: 0.81424 |  0:15:17s\n",
      "epoch 35 | loss: 0.07858 | val_0_auc: 0.82157 |  0:15:43s\n",
      "epoch 36 | loss: 0.07922 | val_0_auc: 0.82197 |  0:16:08s\n",
      "epoch 37 | loss: 0.07886 | val_0_auc: 0.8253  |  0:16:34s\n",
      "epoch 38 | loss: 0.07929 | val_0_auc: 0.82455 |  0:17:00s\n",
      "epoch 39 | loss: 0.07853 | val_0_auc: 0.83038 |  0:17:26s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 39 and best_val_0_auc = 0.83038\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.830385  of boosting iteration \n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 1066.7723\n",
      "Function value obtained: -0.8304\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.034523693335514, 'lambda_sparse': 0.00010381014304610934, 'n_steps': 7, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.30293 | val_0_auc: 0.48988 |  0:00:26s\n",
      "epoch 1  | loss: 0.10665 | val_0_auc: 0.49071 |  0:00:51s\n",
      "epoch 2  | loss: 0.11249 | val_0_auc: 0.50828 |  0:01:17s\n",
      "epoch 3  | loss: 0.1147  | val_0_auc: 0.65943 |  0:01:43s\n",
      "epoch 4  | loss: 0.0905  | val_0_auc: 0.75431 |  0:02:09s\n",
      "epoch 5  | loss: 0.0865  | val_0_auc: 0.7798  |  0:02:34s\n",
      "epoch 6  | loss: 0.08479 | val_0_auc: 0.77492 |  0:03:00s\n",
      "epoch 7  | loss: 0.08482 | val_0_auc: 0.77693 |  0:03:25s\n",
      "epoch 8  | loss: 0.08384 | val_0_auc: 0.77998 |  0:03:50s\n",
      "epoch 9  | loss: 0.0837  | val_0_auc: 0.80196 |  0:04:16s\n",
      "epoch 10 | loss: 0.08518 | val_0_auc: 0.76787 |  0:04:42s\n",
      "epoch 11 | loss: 0.08382 | val_0_auc: 0.78171 |  0:05:08s\n",
      "epoch 12 | loss: 0.08397 | val_0_auc: 0.78547 |  0:05:33s\n",
      "epoch 13 | loss: 0.08332 | val_0_auc: 0.78642 |  0:05:58s\n",
      "epoch 14 | loss: 0.08389 | val_0_auc: 0.7867  |  0:06:24s\n",
      "epoch 15 | loss: 0.08373 | val_0_auc: 0.79712 |  0:06:49s\n",
      "epoch 16 | loss: 0.08552 | val_0_auc: 0.78346 |  0:07:15s\n",
      "epoch 17 | loss: 0.08268 | val_0_auc: 0.79019 |  0:07:40s\n",
      "epoch 18 | loss: 0.08266 | val_0_auc: 0.79144 |  0:08:06s\n",
      "epoch 19 | loss: 0.08368 | val_0_auc: 0.79783 |  0:08:32s\n",
      "\n",
      "Early stopping occurred at epoch 19 with best_epoch = 9 and best_val_0_auc = 0.80196\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.80196  of boosting iteration \n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 529.9936\n",
      "Function value obtained: -0.8020\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0888117918122742, 'lambda_sparse': 0.03692763990599441, 'n_steps': 3, 'n_a': 32, 'n_d': 32, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.21595 | val_0_auc: 0.47233 |  0:00:12s\n",
      "epoch 1  | loss: 0.13087 | val_0_auc: 0.44948 |  0:00:24s\n",
      "epoch 2  | loss: 0.10655 | val_0_auc: 0.60593 |  0:00:36s\n",
      "epoch 3  | loss: 0.09787 | val_0_auc: 0.62944 |  0:00:48s\n",
      "epoch 4  | loss: 0.09622 | val_0_auc: 0.64437 |  0:01:00s\n",
      "epoch 5  | loss: 0.09458 | val_0_auc: 0.62708 |  0:01:12s\n",
      "epoch 6  | loss: 0.09196 | val_0_auc: 0.6737  |  0:01:23s\n",
      "epoch 7  | loss: 0.09151 | val_0_auc: 0.67694 |  0:01:35s\n",
      "epoch 8  | loss: 0.09179 | val_0_auc: 0.71167 |  0:01:47s\n",
      "epoch 9  | loss: 0.08936 | val_0_auc: 0.73499 |  0:01:59s\n",
      "epoch 10 | loss: 0.08758 | val_0_auc: 0.71472 |  0:02:11s\n",
      "epoch 11 | loss: 0.08824 | val_0_auc: 0.73578 |  0:02:22s\n",
      "epoch 12 | loss: 0.08627 | val_0_auc: 0.75127 |  0:02:34s\n",
      "epoch 13 | loss: 0.08661 | val_0_auc: 0.71679 |  0:02:46s\n",
      "epoch 14 | loss: 0.08605 | val_0_auc: 0.71156 |  0:02:57s\n",
      "epoch 15 | loss: 0.08635 | val_0_auc: 0.74986 |  0:03:09s\n",
      "epoch 16 | loss: 0.08488 | val_0_auc: 0.7527  |  0:03:21s\n",
      "epoch 17 | loss: 0.0853  | val_0_auc: 0.74489 |  0:03:32s\n",
      "epoch 18 | loss: 0.08497 | val_0_auc: 0.75816 |  0:03:44s\n",
      "epoch 19 | loss: 0.0843  | val_0_auc: 0.78004 |  0:03:56s\n",
      "epoch 20 | loss: 0.08445 | val_0_auc: 0.77647 |  0:04:07s\n",
      "epoch 21 | loss: 0.08541 | val_0_auc: 0.77355 |  0:04:19s\n",
      "epoch 22 | loss: 0.08518 | val_0_auc: 0.77565 |  0:04:30s\n",
      "epoch 23 | loss: 0.08453 | val_0_auc: 0.79725 |  0:04:42s\n",
      "epoch 24 | loss: 0.08395 | val_0_auc: 0.79101 |  0:04:53s\n",
      "epoch 25 | loss: 0.08294 | val_0_auc: 0.80975 |  0:05:05s\n",
      "epoch 26 | loss: 0.08252 | val_0_auc: 0.78448 |  0:05:17s\n",
      "epoch 27 | loss: 0.0833  | val_0_auc: 0.79615 |  0:05:28s\n",
      "epoch 28 | loss: 0.08246 | val_0_auc: 0.80222 |  0:05:40s\n",
      "epoch 29 | loss: 0.08259 | val_0_auc: 0.7969  |  0:05:51s\n",
      "epoch 30 | loss: 0.08455 | val_0_auc: 0.76667 |  0:06:02s\n",
      "epoch 31 | loss: 0.08502 | val_0_auc: 0.76948 |  0:06:14s\n",
      "epoch 32 | loss: 0.08389 | val_0_auc: 0.77254 |  0:06:25s\n",
      "epoch 33 | loss: 0.0825  | val_0_auc: 0.78007 |  0:06:37s\n",
      "epoch 34 | loss: 0.08249 | val_0_auc: 0.78463 |  0:06:48s\n",
      "epoch 35 | loss: 0.08245 | val_0_auc: 0.78453 |  0:06:59s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_auc = 0.80975\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.809573  of boosting iteration \n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 428.6555\n",
      "Function value obtained: -0.8096\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0963459942002887, 'lambda_sparse': 0.0483189112469174, 'n_steps': 5, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.32469 | val_0_auc: 0.47944 |  0:00:16s\n",
      "epoch 1  | loss: 0.17149 | val_0_auc: 0.51232 |  0:00:33s\n",
      "epoch 2  | loss: 0.13777 | val_0_auc: 0.48781 |  0:00:50s\n",
      "epoch 3  | loss: 0.11691 | val_0_auc: 0.52969 |  0:01:07s\n",
      "epoch 4  | loss: 0.1033  | val_0_auc: 0.47985 |  0:01:23s\n",
      "epoch 5  | loss: 0.09624 | val_0_auc: 0.48571 |  0:01:39s\n",
      "epoch 6  | loss: 0.09636 | val_0_auc: 0.52388 |  0:01:54s\n",
      "epoch 7  | loss: 0.09447 | val_0_auc: 0.52095 |  0:02:10s\n",
      "epoch 8  | loss: 0.09232 | val_0_auc: 0.5149  |  0:02:25s\n",
      "epoch 9  | loss: 0.09152 | val_0_auc: 0.52927 |  0:02:39s\n",
      "epoch 10 | loss: 0.09179 | val_0_auc: 0.51817 |  0:02:54s\n",
      "epoch 11 | loss: 0.09208 | val_0_auc: 0.53161 |  0:03:09s\n",
      "epoch 12 | loss: 0.09186 | val_0_auc: 0.52731 |  0:03:24s\n",
      "epoch 13 | loss: 0.09249 | val_0_auc: 0.55143 |  0:03:39s\n",
      "epoch 14 | loss: 0.09109 | val_0_auc: 0.51472 |  0:03:54s\n",
      "epoch 15 | loss: 0.09138 | val_0_auc: 0.5391  |  0:04:09s\n",
      "epoch 16 | loss: 0.09095 | val_0_auc: 0.52561 |  0:04:24s\n",
      "epoch 17 | loss: 0.09082 | val_0_auc: 0.57833 |  0:04:39s\n",
      "epoch 18 | loss: 0.09087 | val_0_auc: 0.5377  |  0:04:54s\n",
      "epoch 19 | loss: 0.09091 | val_0_auc: 0.5228  |  0:05:10s\n",
      "epoch 20 | loss: 0.09118 | val_0_auc: 0.53904 |  0:05:25s\n",
      "epoch 21 | loss: 0.09129 | val_0_auc: 0.53244 |  0:05:40s\n",
      "epoch 22 | loss: 0.09177 | val_0_auc: 0.53156 |  0:05:55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23 | loss: 0.09173 | val_0_auc: 0.52577 |  0:06:10s\n",
      "epoch 24 | loss: 0.09102 | val_0_auc: 0.47925 |  0:06:24s\n",
      "epoch 25 | loss: 0.09013 | val_0_auc: 0.60778 |  0:06:40s\n",
      "epoch 26 | loss: 0.08837 | val_0_auc: 0.62205 |  0:06:55s\n",
      "epoch 27 | loss: 0.08825 | val_0_auc: 0.60964 |  0:07:10s\n",
      "epoch 28 | loss: 0.08829 | val_0_auc: 0.6284  |  0:07:26s\n",
      "epoch 29 | loss: 0.08859 | val_0_auc: 0.61878 |  0:07:41s\n",
      "epoch 30 | loss: 0.08821 | val_0_auc: 0.61985 |  0:07:56s\n",
      "epoch 31 | loss: 0.08814 | val_0_auc: 0.62361 |  0:08:11s\n",
      "epoch 32 | loss: 0.08747 | val_0_auc: 0.61149 |  0:08:27s\n",
      "epoch 33 | loss: 0.08789 | val_0_auc: 0.62047 |  0:08:42s\n",
      "epoch 34 | loss: 0.08754 | val_0_auc: 0.62428 |  0:08:57s\n",
      "epoch 35 | loss: 0.08813 | val_0_auc: 0.64528 |  0:09:12s\n",
      "epoch 36 | loss: 0.08826 | val_0_auc: 0.62887 |  0:09:28s\n",
      "epoch 37 | loss: 0.08775 | val_0_auc: 0.6257  |  0:09:43s\n",
      "epoch 38 | loss: 0.08779 | val_0_auc: 0.61481 |  0:09:58s\n",
      "epoch 39 | loss: 0.08828 | val_0_auc: 0.62751 |  0:10:14s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 35 and best_val_0_auc = 0.64528\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.645034  of boosting iteration \n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 626.2119\n",
      "Function value obtained: -0.6450\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.3027474945270145, 'lambda_sparse': 0.0001, 'n_steps': 7, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.21856 | val_0_auc: 0.41308 |  0:00:22s\n",
      "epoch 1  | loss: 0.12278 | val_0_auc: 0.5178  |  0:00:45s\n",
      "epoch 2  | loss: 0.09845 | val_0_auc: 0.55828 |  0:01:08s\n",
      "epoch 3  | loss: 0.09466 | val_0_auc: 0.64861 |  0:01:31s\n",
      "epoch 4  | loss: 0.09433 | val_0_auc: 0.68469 |  0:01:54s\n",
      "epoch 5  | loss: 0.0913  | val_0_auc: 0.70328 |  0:02:17s\n",
      "epoch 6  | loss: 0.08677 | val_0_auc: 0.68267 |  0:02:39s\n",
      "epoch 7  | loss: 0.08632 | val_0_auc: 0.69365 |  0:03:01s\n",
      "epoch 8  | loss: 0.08894 | val_0_auc: 0.69798 |  0:03:24s\n",
      "epoch 9  | loss: 0.08974 | val_0_auc: 0.69434 |  0:03:46s\n",
      "epoch 10 | loss: 0.08938 | val_0_auc: 0.63013 |  0:04:08s\n",
      "epoch 11 | loss: 0.09003 | val_0_auc: 0.59484 |  0:04:30s\n",
      "epoch 12 | loss: 0.08779 | val_0_auc: 0.635   |  0:04:53s\n",
      "epoch 13 | loss: 0.08921 | val_0_auc: 0.67272 |  0:05:16s\n",
      "epoch 14 | loss: 0.08953 | val_0_auc: 0.62068 |  0:05:39s\n",
      "epoch 15 | loss: 0.08922 | val_0_auc: 0.61917 |  0:06:01s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_val_0_auc = 0.70328\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.703281  of boosting iteration \n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 378.0068\n",
      "Function value obtained: -0.7033\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.1352777175984987, 'lambda_sparse': 0.0311924353011143, 'n_steps': 5, 'n_a': 16, 'n_d': 16, 'momentum': 0.6}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.29338 | val_0_auc: 0.50307 |  0:00:17s\n",
      "epoch 1  | loss: 0.14006 | val_0_auc: 0.57605 |  0:00:34s\n",
      "epoch 2  | loss: 0.12109 | val_0_auc: 0.50207 |  0:00:51s\n",
      "epoch 3  | loss: 0.1055  | val_0_auc: 0.52219 |  0:01:08s\n",
      "epoch 4  | loss: 0.09922 | val_0_auc: 0.52235 |  0:01:25s\n",
      "epoch 5  | loss: 0.09613 | val_0_auc: 0.5408  |  0:01:41s\n",
      "epoch 6  | loss: 0.09658 | val_0_auc: 0.567   |  0:01:58s\n",
      "epoch 7  | loss: 0.09441 | val_0_auc: 0.5978  |  0:02:14s\n",
      "epoch 8  | loss: 0.09243 | val_0_auc: 0.57972 |  0:02:30s\n",
      "epoch 9  | loss: 0.0922  | val_0_auc: 0.56726 |  0:02:46s\n",
      "epoch 10 | loss: 0.0921  | val_0_auc: 0.55766 |  0:03:02s\n",
      "epoch 11 | loss: 0.09241 | val_0_auc: 0.58382 |  0:03:19s\n",
      "epoch 12 | loss: 0.09235 | val_0_auc: 0.60775 |  0:03:35s\n",
      "epoch 13 | loss: 0.09267 | val_0_auc: 0.57368 |  0:03:51s\n",
      "epoch 14 | loss: 0.09163 | val_0_auc: 0.57909 |  0:04:07s\n",
      "epoch 15 | loss: 0.09143 | val_0_auc: 0.59301 |  0:04:23s\n",
      "epoch 16 | loss: 0.09098 | val_0_auc: 0.5934  |  0:04:39s\n",
      "epoch 17 | loss: 0.09049 | val_0_auc: 0.5885  |  0:04:55s\n",
      "epoch 18 | loss: 0.0908  | val_0_auc: 0.58471 |  0:05:12s\n",
      "epoch 19 | loss: 0.09055 | val_0_auc: 0.59356 |  0:05:28s\n",
      "epoch 20 | loss: 0.09069 | val_0_auc: 0.57325 |  0:05:44s\n",
      "epoch 21 | loss: 0.09043 | val_0_auc: 0.60184 |  0:06:00s\n",
      "epoch 22 | loss: 0.09113 | val_0_auc: 0.58596 |  0:06:16s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.60775\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.607755  of boosting iteration \n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 388.3953\n",
      "Function value obtained: -0.6078\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.011199569821702, 'lambda_sparse': 0.0001, 'n_steps': 8, 'n_a': 32, 'n_d': 32, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.18736 | val_0_auc: 0.50961 |  0:00:29s\n",
      "epoch 1  | loss: 0.11883 | val_0_auc: 0.56909 |  0:00:58s\n",
      "epoch 2  | loss: 0.15723 | val_0_auc: 0.46813 |  0:01:28s\n",
      "epoch 3  | loss: 0.1455  | val_0_auc: 0.65328 |  0:01:57s\n",
      "epoch 4  | loss: 0.09577 | val_0_auc: 0.63735 |  0:02:26s\n",
      "epoch 5  | loss: 0.09284 | val_0_auc: 0.65205 |  0:02:55s\n",
      "epoch 6  | loss: 0.09012 | val_0_auc: 0.6787  |  0:03:24s\n",
      "epoch 7  | loss: 0.09028 | val_0_auc: 0.67064 |  0:03:53s\n",
      "epoch 8  | loss: 0.08858 | val_0_auc: 0.6819  |  0:04:21s\n",
      "epoch 9  | loss: 0.08809 | val_0_auc: 0.65525 |  0:04:50s\n",
      "epoch 10 | loss: 0.08921 | val_0_auc: 0.64251 |  0:05:20s\n",
      "epoch 11 | loss: 0.08838 | val_0_auc: 0.6638  |  0:05:48s\n",
      "epoch 12 | loss: 0.08773 | val_0_auc: 0.71183 |  0:06:16s\n",
      "epoch 13 | loss: 0.08768 | val_0_auc: 0.69047 |  0:06:45s\n",
      "epoch 14 | loss: 0.08735 | val_0_auc: 0.6638  |  0:07:14s\n",
      "epoch 15 | loss: 0.08691 | val_0_auc: 0.66032 |  0:07:43s\n",
      "epoch 16 | loss: 0.08722 | val_0_auc: 0.67696 |  0:08:10s\n",
      "epoch 17 | loss: 0.0869  | val_0_auc: 0.66923 |  0:08:38s\n",
      "epoch 18 | loss: 0.08613 | val_0_auc: 0.66886 |  0:09:06s\n",
      "epoch 19 | loss: 0.08741 | val_0_auc: 0.66864 |  0:09:35s\n",
      "epoch 20 | loss: 0.08739 | val_0_auc: 0.65672 |  0:10:04s\n",
      "epoch 21 | loss: 0.0874  | val_0_auc: 0.71062 |  0:10:33s\n",
      "epoch 22 | loss: 0.08723 | val_0_auc: 0.65105 |  0:11:01s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_val_0_auc = 0.71183\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.71183  of boosting iteration \n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 681.7737\n",
      "Function value obtained: -0.7118\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'seed': 42, 'verbose': True, 'gamma': 1.0, 'lambda_sparse': 0.0051183788729922235, 'n_steps': 6, 'n_a': 64, 'n_d': 64, 'momentum': 0.8}\n",
      "Device used : cpu\n",
      "epoch 0  | loss: 0.33475 | val_0_auc: 0.50766 |  0:00:24s\n",
      "epoch 1  | loss: 0.13295 | val_0_auc: 0.50211 |  0:00:49s\n",
      "epoch 2  | loss: 0.10542 | val_0_auc: 0.60537 |  0:01:14s\n",
      "epoch 3  | loss: 0.09671 | val_0_auc: 0.78311 |  0:01:39s\n",
      "epoch 4  | loss: 0.08676 | val_0_auc: 0.76856 |  0:02:03s\n",
      "epoch 5  | loss: 0.08746 | val_0_auc: 0.77362 |  0:02:28s\n",
      "epoch 6  | loss: 0.08414 | val_0_auc: 0.7387  |  0:02:53s\n",
      "epoch 7  | loss: 0.08341 | val_0_auc: 0.79025 |  0:03:18s\n",
      "epoch 8  | loss: 0.08348 | val_0_auc: 0.78973 |  0:03:43s\n",
      "epoch 9  | loss: 0.08295 | val_0_auc: 0.75558 |  0:04:07s\n",
      "epoch 10 | loss: 0.08369 | val_0_auc: 0.78681 |  0:04:32s\n",
      "epoch 11 | loss: 0.08211 | val_0_auc: 0.77731 |  0:04:57s\n",
      "epoch 12 | loss: 0.08183 | val_0_auc: 0.78895 |  0:05:23s\n",
      "epoch 13 | loss: 0.08084 | val_0_auc: 0.7797  |  0:05:47s\n",
      "epoch 14 | loss: 0.08114 | val_0_auc: 0.78686 |  0:06:12s\n",
      "epoch 15 | loss: 0.08078 | val_0_auc: 0.77594 |  0:06:36s\n",
      "epoch 16 | loss: 0.08196 | val_0_auc: 0.80095 |  0:07:01s\n",
      "epoch 17 | loss: 0.08082 | val_0_auc: 0.79086 |  0:07:25s\n",
      "epoch 18 | loss: 0.08098 | val_0_auc: 0.79401 |  0:07:50s\n",
      "epoch 19 | loss: 0.08081 | val_0_auc: 0.78903 |  0:08:14s\n",
      "epoch 20 | loss: 0.08025 | val_0_auc: 0.77652 |  0:08:38s\n",
      "epoch 21 | loss: 0.08049 | val_0_auc: 0.79454 |  0:09:03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22 | loss: 0.08075 | val_0_auc: 0.80136 |  0:09:27s\n",
      "epoch 23 | loss: 0.07988 | val_0_auc: 0.78813 |  0:09:52s\n",
      "epoch 24 | loss: 0.07976 | val_0_auc: 0.80002 |  0:10:16s\n",
      "epoch 25 | loss: 0.07987 | val_0_auc: 0.78384 |  0:10:41s\n",
      "epoch 26 | loss: 0.07978 | val_0_auc: 0.79068 |  0:11:05s\n",
      "epoch 27 | loss: 0.07988 | val_0_auc: 0.79075 |  0:11:30s\n",
      "epoch 28 | loss: 0.0803  | val_0_auc: 0.7919  |  0:11:54s\n",
      "epoch 29 | loss: 0.08076 | val_0_auc: 0.79277 |  0:12:19s\n",
      "epoch 30 | loss: 0.07936 | val_0_auc: 0.79562 |  0:12:43s\n",
      "epoch 31 | loss: 0.07921 | val_0_auc: 0.78753 |  0:13:07s\n",
      "epoch 32 | loss: 0.0796  | val_0_auc: 0.79947 |  0:13:32s\n",
      "\n",
      "Early stopping occurred at epoch 32 with best_epoch = 22 and best_val_0_auc = 0.80136\n",
      "Best weights from best epoch are automatically used!\n",
      "AUC:  0.801358  of boosting iteration \n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 828.3928\n",
      "Function value obtained: -0.8014\n",
      "Current minimum: -0.8429\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9115919517809082, 'max_depth': 7, 'n_estimators': 144}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.835855  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 1.3568\n",
      "Function value obtained: -0.8359\n",
      "Current minimum: -0.8359\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6406746302524544, 'max_depth': 4, 'n_estimators': 775}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.850239  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 1.2226\n",
      "Function value obtained: -0.8502\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43493507074037885, 'max_depth': 14, 'n_estimators': 346}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:09] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.828321  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 3.3734\n",
      "Function value obtained: -0.8283\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.5803025792095959, 'max_depth': 6, 'n_estimators': 374}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.850169  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 1.5647\n",
      "Function value obtained: -0.8502\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7986776480109528, 'max_depth': 4, 'n_estimators': 271}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:14] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.849362  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 1.1152\n",
      "Function value obtained: -0.8494\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.8711397534666082, 'max_depth': 11, 'n_estimators': 481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.83574  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 2.0025\n",
      "Function value obtained: -0.8357\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4230006612423386, 'max_depth': 6, 'n_estimators': 836}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:17] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.850149  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 1.7896\n",
      "Function value obtained: -0.8501\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9825771685560655, 'max_depth': 9, 'n_estimators': 760}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:19] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.829305  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 1.6966\n",
      "Function value obtained: -0.8293\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.4451344632234332, 'max_depth': 14, 'n_estimators': 385}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:20] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.82534  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 2.5357\n",
      "Function value obtained: -0.8253\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.26051186699738493, 'max_depth': 7, 'n_estimators': 970}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.846654  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 3.0303\n",
      "Function value obtained: -0.8467\n",
      "Current minimum: -0.8502\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 315}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:26] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.853213  of boosting iteration \n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.2961\n",
      "Function value obtained: -0.8532\n",
      "Current minimum: -0.8532\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:29] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.854788  of boosting iteration \n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.3220\n",
      "Function value obtained: -0.8548\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 183}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:32] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.846496  of boosting iteration \n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.4702\n",
      "Function value obtained: -0.8465\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 795}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:35] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.854788  of boosting iteration \n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.3480\n",
      "Function value obtained: -0.8548\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10186000058897077, 'max_depth': 10, 'n_estimators': 685}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:38] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.843389  of boosting iteration \n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 5.3744\n",
      "Function value obtained: -0.8434\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9987613472955277, 'max_depth': 3, 'n_estimators': 541}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:44] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.83951  of boosting iteration \n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.2090\n",
      "Function value obtained: -0.8395\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10024305903331414, 'max_depth': 12, 'n_estimators': 264}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:45] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.843592  of boosting iteration \n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.3380\n",
      "Function value obtained: -0.8436\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9972848130630817, 'max_depth': 5, 'n_estimators': 372}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:51] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.841974  of boosting iteration \n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.6172\n",
      "Function value obtained: -0.8420\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9993524174904217, 'max_depth': 15, 'n_estimators': 983}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:53] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.799359  of boosting iteration \n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.6957\n",
      "Function value obtained: -0.7994\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10274680533431585, 'max_depth': 15, 'n_estimators': 778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:45:56] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.832256  of boosting iteration \n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 5.7401\n",
      "Function value obtained: -0.8323\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10342167415775995, 'max_depth': 6, 'n_estimators': 164}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:01] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.852185  of boosting iteration \n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 4.3559\n",
      "Function value obtained: -0.8522\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.31662208106372236, 'max_depth': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:06] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.849596  of boosting iteration \n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.2197\n",
      "Function value obtained: -0.8496\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49253301287854145, 'max_depth': 10, 'n_estimators': 247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:08] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.827073  of boosting iteration \n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.2904\n",
      "Function value obtained: -0.8271\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.47733572147192493, 'max_depth': 3, 'n_estimators': 161}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:10] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.841074  of boosting iteration \n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.4101\n",
      "Function value obtained: -0.8411\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:12] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.854788  of boosting iteration \n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.3756\n",
      "Function value obtained: -0.8548\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10146816775858006, 'max_depth': 8, 'n_estimators': 750}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:15] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.854454  of boosting iteration \n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 6.4897\n",
      "Function value obtained: -0.8545\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.6475566411069852, 'max_depth': 5, 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:21] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.853748  of boosting iteration \n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.6280\n",
      "Function value obtained: -0.8537\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.9999103885253068, 'max_depth': 12, 'n_estimators': 249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:23] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.811313  of boosting iteration \n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 2.4512\n",
      "Function value obtained: -0.8113\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.7178786193852936, 'max_depth': 8, 'n_estimators': 215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:25] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.832592  of boosting iteration \n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.9013\n",
      "Function value obtained: -0.8326\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'booster': 'gbtree', 'objective': 'binary:logistic', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.3370675014731547, 'max_depth': 12, 'n_estimators': 936}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:46:27] WARNING: ../src/learner.cc:541: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "AUC:  0.839254  of boosting iteration \n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 3.4167\n",
      "Function value obtained: -0.8393\n",
      "Current minimum: -0.8548\n",
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.29470302886859817, 'max_depth': 4, 'n_estimators': 689}\n",
      "AUC:  0.836335  of boosting iteration \n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 0.6439\n",
      "Function value obtained: -0.8363\n",
      "Current minimum: -0.8363\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.43451759820126923, 'max_depth': 13, 'n_estimators': 556}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.807012  of boosting iteration \n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 0.6954\n",
      "Function value obtained: -0.8070\n",
      "Current minimum: -0.8363\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.23838343507237225, 'max_depth': 9, 'n_estimators': 787}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.819  of boosting iteration \n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 0.7029\n",
      "Function value obtained: -0.8190\n",
      "Current minimum: -0.8363\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.38453815845086226, 'max_depth': 4, 'n_estimators': 264}\n",
      "AUC:  0.83643  of boosting iteration \n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 0.6761\n",
      "Function value obtained: -0.8364\n",
      "Current minimum: -0.8364\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.22806125729006957, 'max_depth': 13, 'n_estimators': 473}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.828264  of boosting iteration \n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 0.7009\n",
      "Function value obtained: -0.8283\n",
      "Current minimum: -0.8364\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2355558623665136, 'max_depth': 12, 'n_estimators': 746}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.820687  of boosting iteration \n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 0.6957\n",
      "Function value obtained: -0.8207\n",
      "Current minimum: -0.8364\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2707934113750825, 'max_depth': 5, 'n_estimators': 307}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.846167  of boosting iteration \n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 0.6798\n",
      "Function value obtained: -0.8462\n",
      "Current minimum: -0.8462\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2613202611935387, 'max_depth': 12, 'n_estimators': 361}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.814081  of boosting iteration \n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 0.6935\n",
      "Function value obtained: -0.8141\n",
      "Current minimum: -0.8462\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.44064507309696654, 'max_depth': 7, 'n_estimators': 364}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.798953  of boosting iteration \n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 0.6895\n",
      "Function value obtained: -0.7990\n",
      "Current minimum: -0.8462\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2826540570060721, 'max_depth': 3, 'n_estimators': 797}\n",
      "AUC:  0.844129  of boosting iteration \n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 0.9970\n",
      "Function value obtained: -0.8441\n",
      "Current minimum: -0.8462\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 361}\n",
      "AUC:  0.854977  of boosting iteration \n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9764\n",
      "Function value obtained: -0.8550\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "AUC:  0.846399  of boosting iteration \n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0082\n",
      "Function value obtained: -0.8464\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.853088  of boosting iteration \n",
      "Iteration No: 13 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0732\n",
      "Function value obtained: -0.8531\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 14 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 15, 'n_estimators': 713}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.837777  of boosting iteration \n",
      "Iteration No: 14 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0230\n",
      "Function value obtained: -0.8378\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 15 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.851165  of boosting iteration \n",
      "Iteration No: 15 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9832\n",
      "Function value obtained: -0.8512\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 16 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 109}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.843622  of boosting iteration \n",
      "Iteration No: 16 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0019\n",
      "Function value obtained: -0.8436\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 17 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49929575044886143, 'max_depth': 3, 'n_estimators': 348}\n",
      "AUC:  0.835184  of boosting iteration \n",
      "Iteration No: 17 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9154\n",
      "Function value obtained: -0.8352\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 18 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.851165  of boosting iteration \n",
      "Iteration No: 18 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0527\n",
      "Function value obtained: -0.8512\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 19 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 1000}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.851165  of boosting iteration \n",
      "Iteration No: 19 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0295\n",
      "Function value obtained: -0.8512\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 20 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1880617274662131, 'max_depth': 6, 'n_estimators': 109}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.843158  of boosting iteration \n",
      "Iteration No: 20 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0198\n",
      "Function value obtained: -0.8432\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 21 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.16772354027914116, 'max_depth': 4, 'n_estimators': 346}\n",
      "AUC:  0.852419  of boosting iteration \n",
      "Iteration No: 21 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9575\n",
      "Function value obtained: -0.8524\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 22 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 13, 'n_estimators': 985}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.83885  of boosting iteration \n",
      "Iteration No: 22 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0027\n",
      "Function value obtained: -0.8388\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 23 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.49989118493369655, 'max_depth': 15, 'n_estimators': 789}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.807012  of boosting iteration \n",
      "Iteration No: 23 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9388\n",
      "Function value obtained: -0.8070\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 24 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10238089389494337, 'max_depth': 3, 'n_estimators': 828}\n",
      "AUC:  0.847099  of boosting iteration \n",
      "Iteration No: 24 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9959\n",
      "Function value obtained: -0.8471\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 25 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10020906157035664, 'max_depth': 10, 'n_estimators': 888}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.844105  of boosting iteration \n",
      "Iteration No: 25 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9882\n",
      "Function value obtained: -0.8441\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 26 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.2548212110619307, 'max_depth': 15, 'n_estimators': 850}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.819237  of boosting iteration \n",
      "Iteration No: 26 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9747\n",
      "Function value obtained: -0.8192\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 27 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1996708661471766, 'max_depth': 3, 'n_estimators': 134}\n",
      "AUC:  0.845395  of boosting iteration \n",
      "Iteration No: 27 ended. Search finished for the next optimal point.\n",
      "Time taken: 0.9529\n",
      "Function value obtained: -0.8454\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 28 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marinamandarina_07/anaconda3/lib/python3.8/site-packages/skopt/optimizer/optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.851165  of boosting iteration \n",
      "Iteration No: 28 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0433\n",
      "Function value obtained: -0.8512\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 29 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 417}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.853088  of boosting iteration \n",
      "Iteration No: 29 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.1472\n",
      "Function value obtained: -0.8531\n",
      "Current minimum: -0.8550\n",
      "Iteration No: 30 started. Searching for the next optimal point.\n",
      "\n",
      "Testing next set of paramaters... {'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', 'n_jobs': -1, 'random_state': 42, 'silent': True, 'learning_rate': 0.10020322399897706, 'max_depth': 7, 'n_estimators': 990}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "AUC:  0.849935  of boosting iteration \n",
      "Iteration No: 30 ended. Search finished for the next optimal point.\n",
      "Time taken: 1.0405\n",
      "Function value obtained: -0.8499\n",
      "Current minimum: -0.8550\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets[:1]:\n",
    "    train = pd.read_csv(\"data/kdd/small/{}/train.csv\".format(dataset))\n",
    "    valid = pd.read_csv(\"data/kdd/small/{}/val.csv\".format(dataset))\n",
    "    \n",
    "    train_x = train.drop([\"TARGET\"], axis=1).values\n",
    "    train_y = train[\"TARGET\"].values\n",
    "    \n",
    "    valid_x = valid.drop([\"TARGET\"], axis=1).values\n",
    "    valid_y = valid[\"TARGET\"].values\n",
    "\n",
    "    results = dict()\n",
    "    for clf_name, clf, search_range, params in classificators:\n",
    "        res_gp = gp_minimize(BayesianOptimization(clf, params, search_range, clf_name), search_range, n_jobs=-1, verbose=True, n_random_starts=10, n_calls=30)\n",
    "        results[clf_name] = res_gp\n",
    "        with open('{}_hp_kdd_{}.pickle'.format(clf_name, dataset), 'wb') as f:\n",
    "            pickle.dump(res_gp.x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(results[\"LGBMClassifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Convergence plot'}, xlabel='Number of calls $n$', ylabel='$\\\\min f(x)$ after $n$ calls'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEYCAYAAABlfjCwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAoXklEQVR4nO3de5wcVZ338c83t0lCZiAEEwKEm0bkIiBElH0CDleBZZeLy7rKrsEbUUHB26Mu+qzrisqurK7rBbKiGxVxWRHBxdUAMiAqahAIIQGCSjASEgLBZJKQZDK/54+qnvQM3TPdM9NT1TXf9+s1r+mqOnX6d9JJ/3JOnTqliMDMzKxRxmQdgJmZFZsTjZmZNZQTjZmZNZQTjZmZNZQTjZmZNZQTjZmZNZQTjZkNmaQLJN2ddRyWT040VniS3ihpsaROSasl/a+kuVnHNVpJ6pD0tqzjsJHjRGOFJul9wOeBTwEzgH2BLwNnZRhWL5LGZR2DWSM50VhhSdoV+ARwUUR8LyI2RcT2iPhBRHwwLdMi6fOSnkx/Pi+pJT3WLmmVpPdLWpv2ht6cHnu1pKckjS17v3MkLUlfj5H0YUm/lfSMpOsl7Z4e219SSHqrpCeAn0gaK+lKSesk/V7SxWmZcaW2SLomjeGPkj5Zeu/SsJWkz0pan55/ellcu0v6etq+9ZK+X3bsTEn3S3pO0s8lHd7Pn2dIeo+k36Vx/oukit8hkv5M0q8l/Sn9/Wfp/suB44Avpj3ML9b/yVqzcaKxIjsWmAjc2E+Zy4BXA0cCRwDHAB8tO74nsCuwN/BW4EuSpkbEPcAm4MSysm8Evp2+fg9wNvAaYC9gPfClPu/9GuBg4LXA24HT0ziOSs8ttxDoAl4CvAI4FSgffnoV8AiwB/DPwDWSlB77JjAZOBSYDnwOQNJRwNeA+cA04Grg5lKireIcYE4a41nAW/oWSBPqLcAX0nr/FbhF0rSIuAz4KXBxREyJiIv7eS8riojwj38K+QOcDzw1QJnfAmeUbb8WeDx93Q5sAcaVHV8LvDp9/Unga+nrVpLEs1+6vRw4qey8mcB2YBywPxDAgWXHfwLML9s+OS0zjmTIbyswqez4G4A70tcXAI+VHZucnrtn+r7dwNQKbf8K8E999j0CvKbKn1UAp5Vtvwu4vSyGu9PXfwf8qs+5vwAuSF93AG/L+u+Hf0bux2PDVmTPAHtIGhcRXVXK7AWsLNteme7rqaPPuZuBKenrbwM/l/RO4FzgNxFRqms/4EZJ3WXn7iBJGiV/6BPHH6oc2w8YD6ze2UlhTJ8yT5VeRMTmtNwUYHfg2YhYzwvtB8yT9O6yfRPo3f6+yt+z759VeVtW9tm3kqRXaKOQh86syH4BPM8Lh6HKPUnyhVuyb7pvQBGxjOQL9HR6D5tB8oV8ekTsVvYzMSL+WF5F2evVwD5l27P61LUV2KOsrraIOLSGMP8A7C5ptyrHLu8T4+SIuK6f+srjqvZn1ffPtFS21HYvGT/KONFYYUXEn4D/R3Jd5WxJkyWNl3S6pH9Oi10HfFTSiyTtkZb/Vh1v822S6zHHA/9dtv8q4HJJ+wGk9fc30+164BJJe6dJ4UNl7VgNLAKulNSWTjR4saTXDBRceu7/Al+WNDVt//Hp4f8A3iHpVUrsIunPJbX2U+UH03pmAZcA/1WhzA+Bl6bTysdJej1wCPA/6fE1wIEDxW7F4URjhRYR/wq8j+QC/9Mk/4u/GPh+WuSTwGJgCfAg8Jt0X62uI7mW85OIWFe2/9+Am4FFkjYC95BcsK/mP0iSyRLgPpIv6y6S4TaAN5EMay0jmVjwXZLrL7X4O5LrQw+TXGO6FCAiFpNMQvhiWudjJNda+nMTcC9wP8kF/2v6FoiIZ4AzgfeTDF/+X+DMsj+ffwP+Kp0B94Ua22BNTBHuxZrlTTo9+aqI6DsElRlJAcyOiMeyjsWai3s0ZjkgaZKkM9Khpr2Bf6D/adlmTSPzRJPeTHarpBXp76lVyl0iaamkhyRdOtD56Vj0QkkPSlou6SMj1CSzwRDwjyRDWPeRTI/+f5lGZDZMMk80wIdJ5uLPBm5Pt3uRdBjJWPIxJDfVnSlp9gDnnwe0RMTLgaOB+ZL2b2RDzAYrIjZHxCsjojUipkfEmyNiQ9ZxlYsIedjMBiMPieYskrueSX+fXaHMwcA96T/GLuBOkjuU+zs/gF3SJTwmAduAXP3DNTMbDfJww+aMdAomEbFa0vQKZZaSTBWdRnKn9hkkM4X6O/+7JEloNcmd0u+NiGcrBSDpQuBCgEmTJh09a9asXse7u7sZMyYPOXn4FK1Nbk/+Fa1NRWsPDK1Njz766LqIeFGlYyOSaCTdRrIcRl+X1XJ+RCyXdAVwK9AJPEAy9bM/x5BMDd0LmAr8VNJtEfG7CvUvABYAzJkzJxYvXtzreEdHB+3t7bWE2jSK1ia3J/+K1qaitQeG1iZJfVeD6DEiiSYiTq52TNIaSTPT3shMknn+leq4hnTOvqRPAavSQ9XOfyPwo4jYDqyV9DOSxQBfkGjMzKxx8tDvuxmYl76eR3JD2AuUhsQk7UuyrlRpmYxq5z8BnFi645lkhd6Hhz16MzPrVx4SzWeAUyStAE5Jt5G0l6QflpW7QdIy4AckzxdZ39/5JEuyTyG5vvNr4OsRsaThrTEzs14ynwyQLldxUoX9T5Jc9C9tH1fn+Z0kU5zNzCxDeejRmJlZgWXeoymKRXct4+pr72btMxuYPq2N+efP5dTjDxl0uUbVaWY20pxohsGiu5ZxxVWL2Lo1mXG9Zt0GrrhqEUCvL/xayzWqTjOzLDjRDIOrr72754u+ZOvWLj79pR9z06Kd8w+WrVjN9q4dA5arp2y1cldfe7cTjZnlghPNMFj7TOWVbbZ37eCB5asqHhtMuXrKVovJzGykOdEMg+nT2liz7oVf7FN3ncw/vf8verY/duUPWP+nzQOWq6dstXLTp7XV1QYzs0bxrLNhMP/8ubS09M7ZLS3jePcF7Rx56Kyen3df0F5TuXrKVis3//y5jWmsmVmd3KMZBqVrIQPN/Kq13GDq/OLCDp59bjNjx47hQ+841ddnzCw3nGiGyanHH1LTl3ut5eqt84RjD+K0N/07W7d18coj9q+pfjOzkeChs4IYP34shx20FwBLHv5jxtGYme3kRFMghx+8NwBLltU2g83MbCQ40RTIkYckD2y7v8ap0mZmI8GJpkAOfelMxo4dw4rfr2Xzlm1Zh2NmBjjRFMrElvEcdOAMuruDB32dxsxywommYI48ZB8AHljuRGNm+eBEUzCHH5wmGk8IMLOccKIpmMMP3hsJlj+2mq3bugY+wcyswZxoCqZtykQOnLUH27bv4OHHnso6HDMzJ5oiOrznOo2Hz8wse040BXSkE42Z5YgTTQEdkU4IePDhJ9mxozvjaMxstHOiKaA9dp/C3nvuxuYt23hs5dNZh2Nmo5wTTUEdka575mnOZpY1J5qCOuIQ309jZvngRFNQpes0DyxfRURkHI2ZjWZONAW19567MW23XXhuwxae+OOzWYdjZqOYE01BSeoZPrvfw2dmliEnmgIrTQhY4gU2zSxDTjQFdkT6IDTfuGlmWXKiKbAD992DKbu08NTTG3jq6Q1Zh2Nmo5QTTYGNGSMOf1lp+My9GjPLhhNNwXlCgJllzYmm4Er307hHY2ZZcaIpuIMOnEHLhHE8vupZ1v9pc9bhmNko5ERTcOPHj+XQl84EYMnDnuZsZiMv80QjaXdJt0pakf6eWqXcJZKWSnpI0qUDnS9pgqSvS3pQ0gOS2kekQTnkdc/MLEuZJxrgw8DtETEbuD3d7kXSYcDbgWOAI4AzJc0e4Py3A0TEy4FTgCsl5aG9I6583TMzs5GWhy/es4CF6euFwNkVyhwM3BMRmyOiC7gTOGeA8w8hSTxExFrgOWDO8IbeHA596UzGjh3Dit+vZfOWbVmHY2ajjLJe2VfScxGxW9n2+oiY2qfMwcBNwLHAFpIEsjgi3l3tfEkXkvRk3gDMAu4D3hoRN1SI4ULgQoAZM2Yc/Z3vfKfX8c7OTqZMmTIczc3MVdcvZ9WaTcz7y9nM3m/XQrSpnNuTf0VrU9HaA0Nr0wknnHBvRFT8z/y4IUVVI0m3AXtWOHRZLedHxHJJVwC3Ap3AA0DXAKd9jaQntBhYCfy82jkRsQBYADBnzpxob2/vdbyjo4O++5rNQyvhupsXE+On0d4+txBtKuf25F/R2lS09kDj2jQiiSYiTq52TNIaSTMjYrWkmcDaKnVcA1yTnvMpoHTBoeL56RDbe8ve5+fAimFpUBM64pBZXHfzYk8IMLMRl4drNDcD89LX80iGyF5A0vT0977AucB1/Z0vabKkXdLXpwBdEbGsEQ1oBoe/bC8Alj+2mq3bBuoMmpkNnxHp0QzgM8D1kt4KPAGcByBpL+CrEXFGWu4GSdOA7cBFEbG+v/OB6cCPJXUDfwT+bkRak1NtrZM4cN89+N0T63j4saeyDsfMRpHME01EPAOcVGH/k8AZZdvH1Xn+48BBwxZoARxxyD787ol1PLB8FftOyzoaMxst8jB0ZiOkdD+NF9g0s5GUeY/GRk7piZu/fuBxfnX/48y47lHmnz+XU48/5AVlF921jKuvvZu1z2xg+rS2quXqKdvIOtes29Bve8wsO040o8h9D/0BAaVbp9as28AVX1nEjh3dnDz34J5yt929nM8uuK1n0kC1cvWUHbE6r1oE4GRjliOZ37CZN3PmzInFixf32leU+fKvm7+ANeuK/6TNGXu0ccPVF2YdxpAU5e9cuaK1qWjtgaG1SVK2N2xaPqx9pnqSGTdu5+W6rq7umsrVU3Yk6+yvnWY28pxoRpHp09oq9mj69gCq9Xwq9RRqLTuSdU6f1vaCfWaWHc86G0Xmnz+Xlpbe/7doaRnH/PPnDqpcM9VpZtlxj2YUKV0g75mltUfl2Vzl5Qaa9VVr2UbW+ekv/ZjtXTuYuutk3n1BuycCmOWME80oc+rxh3Dq8YcMeNGvVK6eOoerXL113v6zR/jZ4t/ywfmncPyrZg94jpmNLA+dWdNrndICwMZNz2cciZlV4kRjTa9tyiQANnQ60ZjlkRONNb3WXdIeTefWjCMxs0qcaKzptU2ZCMCGzi0ZR2JmldScaCSdJ6k1ff1RSd+TdFTjQjOrTWuaaNyjMcuneno0H4uIjZLmAq8FFgJfaUxYZrVr3aWUaHyNxiyP6kk0O9Lffw58JSJuAiYMf0hm9WltTRONZ52Z5VI9ieaPkhYArwd+KKmlzvPNGqJtl9I1GicaszyqJ1GcB/wvcGpEPAdMBT7QiKDM6rHzGo0TjVkeDbgygKSNQOlZAgJCUs9rwCsYWqZK05s7N2+luzsYM0YZR2Rm5QZMNBHROhKBmA3WuHFjaRk/hq3bu9m0ZWvP5AAzywdfY7FCmDgx+T+Th8/M8qeeobNK4xERER46s8xNbhnLnzYmEwL2mpF1NGZWzkNnVgg7ezS+adMsb+p6TICkqcBsoGcQPCLuGu6gzOo1OX0AmpehMcufmhONpLcBlwD7APcDrwZ+AZzYkMjM6jCxZSwAGze5R2OWN/VMBrgEeCWwMiJOAF4BPN2QqMzqNGlimmg8GcAsd+pJNM9HxPMAkloi4mHgoMaEZVafSS2edWaWV/Vco1klaTfg+8CtktYDTzYiKLN6TSpNBvB6Z2a5U3OiiYhz0pcfl3QHsCvwo4ZEZVanyek1mg0bnWjM8qauWWclEXHncAdiNhQTW9yjMcureh58tjAdOittT5X0tYZEZVanyelkAK/gbJY/9UwGODxdtRmAiFhPMvPMLHMTPRnALLfqSTRj0hs2AZC0O4McejMbbu7RmOVXPYniSuDnkr5LsvbZXwOXNyQqszpNmDCWMWPElue309W1g3HjxmYdkpmlau7RRMQ3gNcBa0hu1Dw3Ir7ZqMDM6jFG6nk8gCcEmOVLXY8JiIhlEfHFiPj3iFg2HAFI2l3SrZJWpL+nVil3iaSlkh6SdGnZ/vPSfd2S5vQ55yOSHpP0iKTXDke8ll+lB6B5YU2zfMnD82g+DNweEbOB29PtXiQdBrwdOAY4AjhT0uz08FLgXOCuPuccAvwNcChwGvBlSR5PKbDW1qRH4+s0ZvmSh0RzFrAwfb0QOLtCmYOBeyJic0R0AXcC5wBExPKIeKRKvd+JiK0R8XvgMZJEZQXVM3TmRGOWK/Ws3nwicD7wHEkvYgmwNCKGOk4xIyJWA0TEaknTK5RZClwuaRqwBTgDWDxAvXsD95Rtr0r3vYCkC4ELAWbMmEFHR0ev452dnS/Y1+yK1qbOzk62bNoAwC8X38fWjU9kHNHQFO3zgeK1qWjtgca1qZ5ZZ98CLkrPOZyk53Eo8JKBTpR0G7BnhUOX1fLGEbFc0hXArUAn8ADQNdDbVqqqSv0LgAUAc+bMifb29l7HOzo66Luv2RWtTR0dHbzkxRN5cMWz7DPrANrbj8o6pCEp2ucDxWtT0doDjWtTPYnmsYi4MX393/W8SUScXO2YpDWSZqa9mZnA2ip1XANck57zKZIeSn9WAbPKtvfBi4AWmofOzPKpnms0d0p6r6RKPYWhuBmYl76eB9xUqVBpSE3SviQX/6+rod6/kdQi6QCSJ4P+algitlxqm+LJAGZ5VE+iORR4J7Ba0i2SLpd03jDE8BngFEkrgFPSbSTtJemHZeVukLQM+AFwUboEDpLOkbQKOBa4RdKPASLiIeB6YBnJKtMXRcSOYYjXcqp1Sjq92ffRmOVKPY8JOBdA0iSSpHMY8CrqHEarUO8zwEkV9j9JctG/tH1clfNvBG6scuxyvHrBqNE2ZRLgRwWY5U3da5VFxBaSGV8DzfoyG1E9N2xu8g2bZnmSh/tozIZFW2tpMsCWjCMxs3JONFYYO2eduUdjlic1JRolZg1c0iw7pVlnGzc9T0TFW6bMLAM1JZpI/tV+v7GhmA1NS8t4Jowfy7btO9i6baD7ec1spNQzdHaPpFc2LBKzYeCbNs3yp55EcwJJsvmtpCWSHpS0pFGBmQ1Gq2/aNMudeqY3n96wKMyGSSnRuEdjlh/19GieAI4D5kXESpIFKmc0JCqzQfIyNGb5U0+i+TLJMi9vSLc3Al8a9ojMhsDL0JjlTz1DZ6+KiKMk3QcQEeslTWhQXGaD0rMMjXs0ZrlRT49me/oo5ACQ9CKguyFRmQ1SzzI0vmnTLDfqSTRfIFm8crqky4G7gU83JCqzQdp5jcbL0JjlRT2rN18r6V6SlZYFnB0RyxsWmdkg7Jx15h6NWV7UnGgkXRERHwIerrDPLBday5ahMbN8qGfo7JQK+3xvjeWK76Mxy58BezSS3gm8Cziwz0oArcDPGhWY2WC0eQkas9ypZejsDOBM4BHgL8r2b4yIZxsSldkgeQkas/ypZejsxenvR4ANJDdqbgSQtHuD4jIblNL05s7NW+nu9qMCzPKglh7NVcCPgAOAe0lmnJUEcGAD4jIblHHjxjJ50gQ2b9nGpi1be1ZzNrPsDNijiYgvRMTBwNcj4sCIOKDsx0nGcmfnTZsePjPLg3ruo3mnpKnAbGBi2f67GhGY2WC1tU5izbqNbOh8nr287KtZ5uq5j+ZtwCXAPsD9wKuBXwAnNiQys0HyMjRm+VLPfTSXAK8EVkbECcArgKcbEpXZELT5pk2zXKkn0TwfEc8DSGqJiIeBgxoTltngeYqzWb7U85iAVZJ2A74P3CppPfBkI4IyGwqvDmCWL/VMBjgnfflxSXcAu5JMezbLlVavDmCWK/X0aHpExJ3DHYjZcPE1GrN8qecajVlT6LlGs9GJxiwPnGiscHqGztyjMcuFuhONpF3SRzqb5VJbq2edmeXJgIlG0hhJb5R0i6S1JA8+Wy3pIUn/Iml248M0q52XoDHLl1p6NHeQrOD8EWDPiJgVEdOB44B7gM9I+tsGxmhWl7YpkwD3aMzyopZZZydHxPa+O9Nn0dwA3CBp/LBHZjZIkydNYMwYseX57XR17WDcOI/0mmWpltWbtwNI+rwk9VfGLA/GjFHZhACvd2aWtXomA3QCN0vaBUDSqZKG/ChnSbtLulXSivT31CrlLpG0NL02dGnZ/vPSfd2S5pTtnybpDkmdkr441Ditufg6jVl+1JxoIuKjwHVAh6S7gfcDHx6GGD4M3B4Rs4HbK9Up6TDg7cAxwBHAmWWTEJYC5wJ9H1fwPPAx4APDEKM1mdbSzDNPcTbLXM2JRtJJJF/2m4AXAe+JiJ8OQwxnAQvT1wuBsyuUORi4JyI2R0QXcCdwDkBELI+IR/qeEBGbIuJukoRjo0zP0Jlv2jTLXD1L0FwGfCwi7pb0cuC/JL0vIn4yxBhmRMRqgIhYLWl6hTJLgcslTQO2AGcAi4f4vj0kXQhcCDBjxgw6Ojp6He/s7HzBvmZXtDb1bc/zmzcA8MvF97F14xMZRTV4Rft8oHhtKlp7oHFtqmdRzRPLXj8o6XSSWWd/NtC5km4D9qxw6LIa33u5pCuAW0muFT0AdNVybo31LwAWAMyZMyfa29t7He/o6KDvvmZXtDb1bc+9K7pY8uiz7DPrANrbj8ousEEq2ucDxWtT0doDjWvTgIlGkiIi+u5Pex8n9VemrOzJ/dS/RtLMtL6ZwNoqdVwDXJOe8ylg1UCx2+jlFZzN8qOmGzYlvVvSvuU7JU0AjpW0EJg3hBhuLjt/HnBTpUKlIbU0jnNJJiaYVdTmh5+Z5UYtQ2enAW8BrpN0APAcMBEYCywCPhcR9w8hhs8A10t6K/AEcB6ApL2Ar0bEGWm5G9JrNNuBiyJifVruHODfSSYo3CLp/oh4bXrscaANmCDpbODUiFg2hFitSbROSac3e9aZWeZqSTRXRMQlkv6T5Et+D2BLRDw3HAFExDPASRX2P0ly0b+0fVyV828EbqxybP/hiNGaT2kZmo2dvmHTLGu1DJ2VksBPI2J7RKweriRj1iilGzY9dGaWvVoSzY8k/QLYU9JbJB0taWKjAzMbitKjAjwZwCx7Aw6dRcQHJB0IdAAHAH8JHCppG7A0Il7f2BDN6udZZ2b5UdN9NBHxO0knR8SjpX2SpgCHNSwysyEozTrbuOl5IoIq68Ga2QioZ2WAlZLeCOzf57x7hjUis2HQ0jKeCePHsm37DrZu62Jii59kYZaVelZvvolkXbIukvXOSj9mudQ6xcNnZnlQT49mn4g4rWGRmA2z1l0m8sz6TWzofJ4XTWvNOhyzUaueHs3P08U0zZqCezRm+VBPj2YucIGk3wNbAQEREYc3JDKzIfIyNGb5UE+iOb1hUZg1gJehMcuHeh4TsLKRgZgNNy9DY5YPA16jSR/bjKSNkjakv0s/GxofotngeBkas3yoZWWAuelvT9uxplJ+06aZZafmoTNJc4C/p88Nm54MYHlVmnW2YaMTjVmW6pkMcC3wQeBBoLsx4ZgNn1b3aMxyoZ5E83RE3NywSMyGme+jMcuHehLNP0j6KnA7yX00AETE94Y9KrNh0OZEY5YL9SSaNwMvA8azc+gsACcay6XSowI868wsW/UkmiMiwkvQWNMoTW/u3LyV7u5gzBg/KsAsC/WsdXaPpEMaFonZMBs3biyTJ02guzvYvGVb1uGYjVr1JJq5wP2SHpG0RNKDkpY0KjCz4bDzps0tGUdiNnrVM3TmRwRY02lrncSadRvZuMnL0JhlxWudWaH19Gh806ZZZuoZOjNrOl6Gxix7TjRWaK1+Jo1Z5pxorNC8OoBZ9pxorNBKN2060Zhlx4nGCs3XaMyy50RjheahM7PsOdFYobV5MoBZ5pxorNB2PpPGN2yaZcWJxgpt5w2bXoLGLCtONFZobVMmAe7RmGXJicYKbfKkCYwZIzZv2UZX146swzEblZxorNDGjNHOe2ncqzHLROaJRtLukm6VtCL9PbVKuUskLZX0kKRLy/afl+7rljSnbP8pku5NH2dwr6QTR6A5lkOl6zSe4myWjcwTDfBh4PaImA3cnm73Iukw4O3AMcARwJmSZqeHlwLnAnf1OW0d8BfpU0HnAd9sTPiWd62t6RRn37Rplok8JJqzgIXp64XA2RXKHAzcExGbI6ILuBM4ByAilkfEI31PiIj7IuLJdPMhYKKkluEO3vKvZ+jMjwowy0Q9Dz5rlBkRsRogIlZLml6hzFLgcknTgC3AGcDiOt7jdcB9EVFxkF7ShcCFADNmzKCjo6PX8c7Ozhfsa3ZFa1N/7Xl+8wYAfrn4PrZufGIEoxq8on0+ULw2Fa090Lg2jUiikXQbsGeFQ5fVcn5ELJd0BXAr0Ak8AHTV+N6HAlcAp/ZT/wJgAcCcOXOivb291/GOjg767mt2RWtTf+25d0UXSx59llmzDqC9/aiRDWyQivb5QPHaVLT2QOPaNCKJJiJOrnZM0hpJM9PezExgbZU6rgGuSc/5FLBqoPeVtA9wI/CmiPjtoIK3pudlaMyylYdrNDeTXKwn/X1TpUKlITVJ+5Jc/L+uv0ol7QbcAnwkIn42XMFa8/H0ZrNs5SHRfAY4RdIK4JR0G0l7SfphWbkbJC0DfgBcFBHr03LnSFoFHAvcIunHafmLgZcAH5N0f/pT6fqPFVzrlHQZmk4vQ2OWhcwnA0TEM8BJFfY/SXLRv7R9XJXzbyQZHuu7/5PAJ4cvUmtWPcvQdLpHY5aFPPRozBqqZ2FNX6Mxy4QTjRVeW6sffmaWJScaK7yeyQBONGaZcKKxwmvrefiZE41ZFpxorPBaWsYzYfxYtm3fwdat27MOx2zUcaKxUaHVN22aZcaJxkaFnTdtOtGYjTQnGhsVSjPPNngFZ7MR50Rjo4KXoTHLjhONjQpehsYsO040Nip4GRqz7DjR2KjgZWjMsuNEY6OCb9o0y44TjY0KPffReNaZ2YhzorFRodU9GrPMONHYqOBEY5YdJxobFXqu0XjozGzEOdHYqOAbNs2y40Rjo0L50Fl3d2Qcjdno4kRjo8K4sWOYPGkC3d3B5i3bsg7HbFRxorFRY+dNm16GxmwkOdHYqNHWmi5D4+s0ZiPKicZGjZ4ejWeemY2ocVkHYDYSFt21jIceXQ3Axz/3P1zylhM49fhDqpa9+tq7WfvMBqZPa2P++XMrlq213GDqXLNuAzOue3RY62xEnPXUOVCb8hKnP6P+2zQYivAMnHJz5syJxYsX99rX0dFBe3t7NgE1SNHa1F97Ft21jCuuWsTWrV09+1omjOPiee20H/vS3vX84lG+uLCDrdv6L1trOdfpOpuyzpZxfOgdp9aVbCTdGxFzKh5zounNiaY59dee181fwJp1G0Y2ILMmN2OPNm64+sKay/eXaDx0ZoW39pnqSWa3tkm9tp/bUH1GWnnZWsu5TtfZrHX29++mXk40VnjTp7VV7NFU+h9btd5P37K1lnOdrrNZ65w+re0F+wbLs86s8OafP5eWlt7/p2ppGcf88+cOuqzrdJ2jrc6hcI/GCq90QbOWGTi1lm10nWvWbWDGHvmPs546+2tTnuL0Z9T/DLVBiQj/lP0cffTR0dcdd9zxgn3Nrmhtcnvyr2htKlp7IobWJmBxVPle9dCZmZk1lBONmZk1lBONmZk1lBONmZk1lBONmZk1lJeg6UPS08DKPrv3ANZlEE4jFa1Nbk/+Fa1NRWsPDK1N+0XEiyodcKKpgaTFUWUNn2ZVtDa5PflXtDYVrT3QuDZ56MzMzBrKicbMzBrKiaY2C7IOoAGK1ia3J/+K1qaitQca1CZfozEzs4Zyj8bMzBrKicbMzBrKiaYfkk6T9IikxyR9OOt4hoOkxyU9KOl+SYsHPiN/JH1N0lpJS8v27S7pVkkr0t9Ts4yxHlXa83FJf0w/p/slnZFljPWQNEvSHZKWS3pI0iXp/mb+jKq1qSk/J0kTJf1K0gNpe/4x3d+Qz8jXaKqQNBZ4FDgFWAX8GnhDRCzLNLAhkvQ4MCcimvZGM0nHA53ANyLisHTfPwPPRsRn0v8UTI2ID2UZZ62qtOfjQGdEfDbL2AZD0kxgZkT8RlIrcC9wNnABzfsZVWvTX9OEn5MkAbtERKek8cDdwCXAuTTgM3KPprpjgMci4ncRsQ34DnBWxjEZEBF3Ac/22X0WsDB9vZDkS6ApVGlP04qI1RHxm/T1RmA5sDfN/RlVa1NTSh8h05lujk9/ggZ9Rk401e0N/KFsexVN/BerTACLJN0r6cIBSzePGRGxGpIvBWB6xvEMh4slLUmH1ppmmKmcpP2BVwC/pCCfUZ82QZN+TpLGSrofWAvcGhEN+4ycaKpThX1FGGf8PxFxFHA6cFE6bGP58xXgxcCRwGrgykyjGQRJU4AbgEsjYkPW8QyHCm1q2s8pInZExJHAPsAxkg5r1Hs50VS3CphVtr0P8GRGsQybiHgy/b0WuJFkiLAI1qTj6KXx9LUZxzMkEbEm/SLoBv6DJvuc0nH/G4BrI+J76e6m/owqtanZPyeAiHgO6ABOo0GfkRNNdb8GZks6QNIE4G+AmzOOaUgk7ZJeyETSLsCpwNL+z2oaNwPz0tfzgJsyjGXISv/YU+fQRJ9TeqH5GmB5RPxr2aGm/YyqtalZPydJL5K0W/p6EnAy8DAN+ow866wf6VTFzwNjga9FxOXZRjQ0kg4k6cUAjAO+3YxtknQd0E6ypPka4B+A7wPXA/sCTwDnRURTXGCv0p52kuGYAB4H5pfGzvNO0lzgp8CDQHe6++9Jrmk062dUrU1voAk/J0mHk1zsH0vS4bg+Ij4haRoN+IycaMzMrKE8dGZmZg3lRGNmZg3lRGNmZg3lRGNmZg3lRGNmZg3lRGNmZg3lRGNmZg3lRGOjnqSQdGXZ9gfSZfqHWu/+5c+YaSRJ70mflXLtEOvprPTabCicaMxgK3CupD2yDqScErX+G30XcEZEnN/ImMwGw4nGDLqABcB7y3f27ZGUejrp/oclfVXSUknXSjpZ0s/SJxOWL6w4TtLCdBn570qanNb1t+kTDu+XdHX6oL3Sey6X9GXgN/Re2BVJ70vfc6mkS9N9VwEHAjdL6tWG9Pib0vd/QNI3033fTx8V8dBAj4tI18i7JT1/qaTXVyhzo6RPSvqppKckndxfnTa6ONGYJb4EnC9p1xrLvwT4N+Bw4GXAG4G5wAdI1sAqOQhYEBGHAxuAd0k6GHg9ySMbjgR2AOf3OecbEfGKiFhZ2inpaODNwKuAVwNvl/SKiHgHycriJ0TE58qDlHQocBlwYkQcQfIURYC3RMTRwBzgPekaV9WcBjwZEUekTwD9UYUyhwHPRcRxJL0r96yshxONGZA+W+QbwHtqPOX3EfFgujz8Q8DtkSwc+CCwf1m5P0TEz9LX3yJJRicBRwO/Th88dRJJj6RkZUTcU+E95wI3RsSm9OmI3wOOGyDOE4Hvlh7dXbZA4nskPQDcQ9Jrmt1PHQ8CJ0u6QtJxEfGn8oNpL21XoJTkxgHPDRCXjSLjsg7ALEc+TzJc9fV0u4ve/xmbWPZ6a9nr7rLtbnr/u+q7am2QPFRvYUR8pEocm6rsr/QwvoGobwyS2kmWhT82IjZL6qB323qJiEfT3tQZwKclLYqIT5QVORS4NyJ2pNuH0yTL5dvIcI/GLJX+b/964K3prjXAdEnTJLUAZw6i2n0lHZu+fgNwN3A78FeSpgNI2l3SfjXUdRdwtqTJ6fOEziFZur4/twN/XRoak7Q7Se9jfZpkXkYyDFeVpL2AzRHxLeCzwFF9ihwG3F+2fTiwpIb22CjhHo1Zb1cCFwNExHZJnyB5jsrvSR4MVa/lwDxJVwMrgK+kX/AfBRals8q2AxcBK/uph4j4jaT/BH6V7vpqRNw3wDkPSbocuFPSDuA+YD7wDklLgEdIhs/683LgXyR1p7G+s8LxX5ZtH4Z7NFbGz6MxM7OG8tCZmZk1lBONmZk1lBONmZk1lBONmZk1lBONmZk1lBONmZk1lBONmZk11P8HTk4qgAFrwZoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_convergence(results[\"TABNETClassifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10284216487315759, 4, 955]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"LGBMClassifier\"].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3317433223693123, 4, 772]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"XGBClassifier\"].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0131059206061017, 0.0051151725754103195, 10, 54]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"TABNETClassifier\"].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: appetency\n",
      "\tTABNETClassifier [1.2917828325804677, 0.0001, 3, 8, 0.98]\n",
      "\tXGBClassifier [0.1, 5, 183]\n",
      "\tLGBMClassifier [0.1, 4, 361]\n",
      "Dataset: churn\n",
      "\tTABNETClassifier [1.0, 0.001, 3, 8]\n",
      "\tXGBClassifier [0.10258423586932192, 3, 212]\n",
      "\tLGBMClassifier [0.10090558417844547, 3, 289]\n",
      "Dataset: upselling\n",
      "\tTABNETClassifier [1.0278804398968666, 0.060872939878532255, 3, 54]\n",
      "\tXGBClassifier [0.12505580158355423, 3, 978]\n",
      "\tLGBMClassifier [0.10056036392812367, 9, 574]\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"Dataset:\", dataset)\n",
    "    for classifier in classificators:\n",
    "        with open(\"{}_hp_kdd_{}.pickle\".format(classifier[0], dataset), \"rb\") as f:\n",
    "            params = pickle.load(f)\n",
    "            print('\\t' + classifier[0], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
